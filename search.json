[
  {
    "objectID": "blog/2024/05/11/optimal_threshold/index.html",
    "href": "blog/2024/05/11/optimal_threshold/index.html",
    "title": "最优分类阈值",
    "section": "",
    "text": "这里我们借助scikit-learn来探讨分类问题中阈值的选择。\n\n数据准备和参数选择\n首先是数据准备：\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nnp.set_printoptions(suppress=True, precision=8, linewidth=1000)\npd.options.mode.chained_assignment = None\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\n\ndata = load_breast_cancer()\nX = data[\"data\"]\ny = data[\"target\"]\n\nXtrain, Xvalid, ytrain, yvalid = train_test_split(X, y, test_size=.20, random_state=516)\n\nprint(f\"Xtrain.shape: {Xtrain.shape}\")\nprint(f\"Xvalid.shape: {Xvalid.shape}\")\n\nXtrain.shape: (455, 30)\nXvalid.shape: (114, 30)\n\n\n模型我们这里选择随机森林。超参的选择，基于GridSearchCV，这里也不赘述。有一个点需要说明，由于使用的是肿瘤数据集，在这种情况下，我们更关注的是recall，即尽量减少假阴性的情况。因而，我们在训练模型时，也是将recall作为评价指标。\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    \"n_estimators\": [100, 150, 250],\n    \"min_samples_leaf\": [2, 3, 4],\n    \"ccp_alpha\": [0, .1, .2, .3]\n    }\n\nmdl = GridSearchCV(\n    RandomForestClassifier(random_state=516), \n    param_grid, \n    scoring=\"recall\", \n    cv=5\n    )\n\nmdl.fit(Xtrain, ytrain)\n\nprint(f\"best parameters: {mdl.best_params_}\")\n\nbest parameters: {'ccp_alpha': 0, 'min_samples_leaf': 4, 'n_estimators': 100}\n\n\n\n\n模型预测\n拿到模型后，自然我们可以开始预测：\n\nypred = mdl.predict_proba(Xvalid)[:,1]\nypred\n\narray([0.005     , 0.82743637, 0.97088095, 0.        , 0.        , 1.        , 0.98020202, 0.67380556, 0.        , 0.99333333, 0.9975    , 0.30048576, 0.9528113 , 0.99666667, 0.04102381, 0.99444444, 1.        , 0.828226  , 0.        , 0.        , 0.97916667, 1.        , 0.99607143, 0.90425163, 0.        , 0.02844156, 0.99333333, 0.98183333, 0.9975    , 0.08869769, 0.97369841, 0.        , 1.        , 0.71100866, 0.96022727, 0.        , 0.71200885, 0.06103175, 0.005     , 0.99490476, 0.1644127 , 0.        , 0.23646934, 1.        , 0.57680164, 0.64901715, 0.9975    , 0.61790818, 0.95509668, 0.99383333, 0.04570455, 0.97575758, 1.        , 0.47115815, 0.92422619, 0.77371415, 0.        , 1.        , 0.26198657, 0.        , 0.28206638, 0.95216162, 0.98761905, 0.99464286, 0.98704762, 0.85579351, 0.10036905, 0.00222222, 0.98011905, 0.99857143, 0.92285967, 0.95180556, 0.97546947, 0.84433189, 0.005     , 0.99833333, 0.83616339, 1.        , 0.9955    , 1.        , 0.99833333, 1.        ,\n       0.86399315, 0.9807381 , 0.        , 0.99833333, 0.9975    , 0.        , 0.98733333, 0.96822727, 0.23980827, 0.7914127 , 0.        , 0.98133333, 1.        , 1.        , 0.89251019, 0.9498226 , 0.18943254, 0.83494391, 0.9975    , 1.        , 0.77079113, 0.99722222, 0.30208297, 1.        , 0.92111977, 0.99428571, 0.91936508, 0.47118074, 0.98467172, 0.006     , 0.05750305, 0.96954978])\n\n\n这个时候，我们要讲的东西就来了。一般地，我们会选择0.50作为分类阈值，即大于0.50的为正类，小于0.50的为负类。\n\nypred = mdl.predict_proba(Xvalid)[:,1].reshape(-1, 1)\nyhat = mdl.predict(Xvalid).reshape(-1, 1)\npreds = np.concatenate([ypred, yhat], axis=1)\nprint(preds)\nprint(confusion_matrix(yvalid, yhat))\n\n[[0.005      0.        ]\n [0.82743637 1.        ]\n [0.97088095 1.        ]\n [0.         0.        ]\n [0.         0.        ]\n [1.         1.        ]\n [0.98020202 1.        ]\n [0.67380556 1.        ]\n [0.         0.        ]\n [0.99333333 1.        ]\n [0.9975     1.        ]\n [0.30048576 0.        ]\n [0.9528113  1.        ]\n [0.99666667 1.        ]\n [0.04102381 0.        ]\n [0.99444444 1.        ]\n [1.         1.        ]\n [0.828226   1.        ]\n [0.         0.        ]\n [0.         0.        ]\n [0.97916667 1.        ]\n [1.         1.        ]\n [0.99607143 1.        ]\n [0.90425163 1.        ]\n [0.         0.        ]\n [0.02844156 0.        ]\n [0.99333333 1.        ]\n [0.98183333 1.        ]\n [0.9975     1.        ]\n [0.08869769 0.        ]\n [0.97369841 1.        ]\n [0.         0.        ]\n [1.         1.        ]\n [0.71100866 1.        ]\n [0.96022727 1.        ]\n [0.         0.        ]\n [0.71200885 1.        ]\n [0.06103175 0.        ]\n [0.005      0.        ]\n [0.99490476 1.        ]\n [0.1644127  0.        ]\n [0.         0.        ]\n [0.23646934 0.        ]\n [1.         1.        ]\n [0.57680164 1.        ]\n [0.64901715 1.        ]\n [0.9975     1.        ]\n [0.61790818 1.        ]\n [0.95509668 1.        ]\n [0.99383333 1.        ]\n [0.04570455 0.        ]\n [0.97575758 1.        ]\n [1.         1.        ]\n [0.47115815 0.        ]\n [0.92422619 1.        ]\n [0.77371415 1.        ]\n [0.         0.        ]\n [1.         1.        ]\n [0.26198657 0.        ]\n [0.         0.        ]\n [0.28206638 0.        ]\n [0.95216162 1.        ]\n [0.98761905 1.        ]\n [0.99464286 1.        ]\n [0.98704762 1.        ]\n [0.85579351 1.        ]\n [0.10036905 0.        ]\n [0.00222222 0.        ]\n [0.98011905 1.        ]\n [0.99857143 1.        ]\n [0.92285967 1.        ]\n [0.95180556 1.        ]\n [0.97546947 1.        ]\n [0.84433189 1.        ]\n [0.005      0.        ]\n [0.99833333 1.        ]\n [0.83616339 1.        ]\n [1.         1.        ]\n [0.9955     1.        ]\n [1.         1.        ]\n [0.99833333 1.        ]\n [1.         1.        ]\n [0.86399315 1.        ]\n [0.9807381  1.        ]\n [0.         0.        ]\n [0.99833333 1.        ]\n [0.9975     1.        ]\n [0.         0.        ]\n [0.98733333 1.        ]\n [0.96822727 1.        ]\n [0.23980827 0.        ]\n [0.7914127  1.        ]\n [0.         0.        ]\n [0.98133333 1.        ]\n [1.         1.        ]\n [1.         1.        ]\n [0.89251019 1.        ]\n [0.9498226  1.        ]\n [0.18943254 0.        ]\n [0.83494391 1.        ]\n [0.9975     1.        ]\n [1.         1.        ]\n [0.77079113 1.        ]\n [0.99722222 1.        ]\n [0.30208297 0.        ]\n [1.         1.        ]\n [0.92111977 1.        ]\n [0.99428571 1.        ]\n [0.91936508 1.        ]\n [0.47118074 0.        ]\n [0.98467172 1.        ]\n [0.006      0.        ]\n [0.05750305 0.        ]\n [0.96954978 1.        ]]\n[[35  3]\n [ 1 75]]\n\n\n但是，这个阈值是可以调整的。我们可以通过调整阈值来达到不同的目的。比如，我们可以通过调整阈值来减少假阴性的情况，这在类别不平衡时尤为重要。\n\n\n阈值的选择\n我们介绍几种常用的方法。\n\n1. 阳性类别prevalance\n我们看下这个数据集中阳性类别的比例：\n\nprint(f\"Proportion of positives in training set: {ytrain.sum() / ytrain.shape[0]:.2f}\")\n\nProportion of positives in training set: 0.62\n\n\n这个toy数据集很夸张哈，达到了0.62。在实际应用中，这个比例可能只有10%或者1%。这里我们只是拿它示例哈，用这个prevalance来作为阈值。\n\nthresh = 1- ytrain.sum() / ytrain.shape[0]\nyhat = np.where(ypred &lt;= thresh, 0, 1)\nprint(confusion_matrix(yvalid, yhat))\n\n[[34  4]\n [ 0 76]]\n\n\n考虑prevalance的方法，可以在类别不平衡的情况下，减少假阴性的情况。\n\n\n2. 最优F1指数\nF1指数是precision和recall的调和平均数。我们可以通过最大F1指数来选择最优的阈值。\n\n\nThreshold using optimal f1-score: 0.471.\n\n\nF1最高为0.471，我们采用它来进行预测：\n\nthresh = .471\nyhat = np.where(ypred &lt;= thresh, 0, 1)\nprint(confusion_matrix(yvalid, yhat))\n\n[[34  4]\n [ 0 76]]\n\n\n\n\n3. ROC曲线\n我们可以通过ROC曲线来选择最优的阈值。ROC曲线下的面积AUC越大，说明模型越好。我们可以选择ROC曲线最靠近左上角的点作为最优阈值。\n\n\n\n\n\n\n\n\n\n\n\n4. PRC曲线\nPRC曲线是precision-recall曲线。相比于ROC曲线，PRC曲线更适合类别不平衡的情况。我们主要选择PRC曲线最靠近右上角的点作为最优阈值。\n\n\n\n\n\n\n\n\n\nSelected threshold using precision-recall curve: 0.674.\n\n\n\n\n5. 分别关注precision和recall\n我们可以通过调整阈值来分别关注precision和recall。比如，我们可以通过调整阈值来提高recall，减少假阴性的情况。\n\n\n\n\n\n\n\n\n\n\n代码已经放进了星球里。"
  },
  {
    "objectID": "blog/2020/02/06/index.html",
    "href": "blog/2020/02/06/index.html",
    "title": "R语言数据处理分析实例",
    "section": "",
    "text": "前言\nR是一种块状结构程序语言，“块”由大括号划分，当“块”只包含一条语句时大括号可以省略。程序语句由换行符或分号分隔。和许多脚本语言一样，R语言不需要“声明”变量。\n接下来我们一起来处理分析一个简单的数据任务。\n\n\n实例\n一组学生参加了数学、科学和英语三科考试。为了给所有学生确定一个综合的成绩衡量指标，需要将这些科目的成绩组合起来，将排名在前20%的学生评定为A，接下来20%的学生评定为B，依此类推，最后，按字母顺序对学生进行排序。请试处理分析。\n如下，我们首先创建原始数据框：\n\nstudentname= c(\"John Davis\", \"Angela Williams\", \"Bullwinkle Moose\", \n               \"David Jones\", \"Janice Markhammer\", \"Cheryl Cushing\", \n               \"Reuven Ytzrhak\", \"Greg Knox\", \"Joel England\", \"Mary Rayburn\")\nmath= c(502, 600, 412, 358, 495, 512, 410, 625, 573, 522)\nscience= c(95, 99, 80, 82, 75, 85, 80, 95, 89, 86)\nEnglish= c(25, 22, 18, 15, 20, 28, 15, 30, 27,18)\nscoredata= data.frame(studentname, math, science, English, stringsAsFactors=FALSE)\nscoredata\n\n         studentname math science English\n1         John Davis  502      95      25\n2    Angela Williams  600      99      22\n3   Bullwinkle Moose  412      80      18\n4        David Jones  358      82      15\n5  Janice Markhammer  495      75      20\n6     Cheryl Cushing  512      85      28\n7     Reuven Ytzrhak  410      80      15\n8          Greg Knox  625      95      30\n9       Joel England  573      89      27\n10      Mary Rayburn  522      86      18\n\n\n观察此数据集，马上可以发现一些明显的挑战：首先，三科考试成绩是无法比较的。由于它们的均值和标准差相差很大，对它们求三科的平均成绩是没有意义的。因此，在组合这些考试成绩之前，必须将其转换为可比较的成绩。\n因而，我们做如下处理，将三科成绩进行标准化，这样每科考试成绩都是用单位标准差来表示，而非原始的分值尺度表示：\n\nz= scale(scoredata[2:4])\nz\n\n             math     science     English\n [1,]  0.01269128  1.07806562  0.58685145\n [2,]  1.14336936  1.59143020  0.03667822\n [3,] -1.02568654 -0.84705156 -0.69688609\n [4,] -1.64871324 -0.59036927 -1.24705932\n [5,] -0.06807144 -1.48875728 -0.33010394\n [6,]  0.12806660 -0.20534583  1.13702468\n [7,] -1.04876160 -0.84705156 -1.24705932\n [8,]  1.43180765  1.07806562  1.50380683\n [9,]  0.83185601  0.30801875  0.95363360\n[10,]  0.24344191 -0.07700469 -0.69688609\nattr(,\"scaled:center\")\n   math science English \n  500.9    86.6    21.8 \nattr(,\"scaled:scale\")\n     math   science   English \n86.673654  7.791734  5.452828 \n\n\nscale函数为数据对象按列进行中心化（center=TRUE）或标准化（center=TRUE, scale=TRUE），默认情况下，函数scale对矩阵或数据框的指定列进行均值为0、标准差为1的标准化，给出各科成绩标准化后的结果以及各科成绩的均值和标准差。\n然后，我们通过mean函数来计算各行的均值以获得三科成绩的综合评价得分，并使用cbind函数将其添加到学生成绩的原始数据框中：\n\nScore= apply(z, 1, mean)\nScore\n\n [1]  0.5592028  0.9238259 -0.8565414 -1.1620473 -0.6289776  0.3532485\n [7] -1.0476242  1.3378934  0.6978361 -0.1768163\n\nscoredata2= cbind(scoredata, Score)\nscoredata2\n\n         studentname math science English      Score\n1         John Davis  502      95      25  0.5592028\n2    Angela Williams  600      99      22  0.9238259\n3   Bullwinkle Moose  412      80      18 -0.8565414\n4        David Jones  358      82      15 -1.1620473\n5  Janice Markhammer  495      75      20 -0.6289776\n6     Cheryl Cushing  512      85      28  0.3532485\n7     Reuven Ytzrhak  410      80      15 -1.0476242\n8          Greg Knox  625      95      30  1.3378934\n9       Joel England  573      89      27  0.6978361\n10      Mary Rayburn  522      86      18 -0.1768163\n\n\napply函数可将一个任意函数应用到矩阵、数组、数据框的任何维度上，在矩阵或数据框中，1表示行，2表示列。cbind函数进行列合并，增加列。\n接下来，我们通过函数quantile给出三科成绩综合评价得分的百分位数，将学生的百分位数排名重编码为一个新的类别型成绩变量grade，如下：\n\ny= quantile(Score, c(0.8,0.6,0.4,0.2))\ny\n\n       80%        60%        40%        20% \n 0.7430341  0.4356302 -0.3576808 -0.8947579 \n\nscoredata2$grade[Score &gt;=y[1]]= \"A\"\nscoredata2$grade[Score &lt; y[1] & Score &gt;=y[2]]= \"B\"\nscoredata2$grade[Score &lt; y[2] & Score &gt;=y[3]]= \"C\"\nscoredata2$grade[Score &lt; y[3] & Score &gt;=y[4]]= \"D\"\nscoredata2$grade[Score &lt; y[4]]= \"E\"\nscoredata2\n\n         studentname math science English      Score grade\n1         John Davis  502      95      25  0.5592028     B\n2    Angela Williams  600      99      22  0.9238259     A\n3   Bullwinkle Moose  412      80      18 -0.8565414     D\n4        David Jones  358      82      15 -1.1620473     E\n5  Janice Markhammer  495      75      20 -0.6289776     D\n6     Cheryl Cushing  512      85      28  0.3532485     C\n7     Reuven Ytzrhak  410      80      15 -1.0476242     E\n8          Greg Knox  625      95      30  1.3378934     A\n9       Joel England  573      89      27  0.6978361     B\n10      Mary Rayburn  522      86      18 -0.1768163     C\n\n\nquantile函数求分位数，这里求Score数值型向量的20%、40%、60%和80%分位点。\n接下来，我们使用strsplit函数以空格把学生姓名拆分为姓氏和名字：\n\nname= strsplit(scoredata2$studentname, \" \")\nname\n\n[[1]]\n[1] \"John\"  \"Davis\"\n\n[[2]]\n[1] \"Angela\"   \"Williams\"\n\n[[3]]\n[1] \"Bullwinkle\" \"Moose\"     \n\n[[4]]\n[1] \"David\" \"Jones\"\n\n[[5]]\n[1] \"Janice\"     \"Markhammer\"\n\n[[6]]\n[1] \"Cheryl\"  \"Cushing\"\n\n[[7]]\n[1] \"Reuven\"  \"Ytzrhak\"\n\n[[8]]\n[1] \"Greg\" \"Knox\"\n\n[[9]]\n[1] \"Joel\"    \"England\"\n\n[[10]]\n[1] \"Mary\"    \"Rayburn\"\n\nlastname= sapply(name, \"[\", 2)\nfirstname= sapply(name, \"[\", 1)\nscoredata3= cbind(firstname, lastname, scoredata2[-1])\nscoredata3\n\n    firstname   lastname math science English      Score grade\n1        John      Davis  502      95      25  0.5592028     B\n2      Angela   Williams  600      99      22  0.9238259     A\n3  Bullwinkle      Moose  412      80      18 -0.8565414     D\n4       David      Jones  358      82      15 -1.1620473     E\n5      Janice Markhammer  495      75      20 -0.6289776     D\n6      Cheryl    Cushing  512      85      28  0.3532485     C\n7      Reuven    Ytzrhak  410      80      15 -1.0476242     E\n8        Greg       Knox  625      95      30  1.3378934     A\n9        Joel    England  573      89      27  0.6978361     B\n10       Mary    Rayburn  522      86      18 -0.1768163     C\n\n\nstrsplit函数应用到一个字符串向量上会返回一个列表，使用sapply函数分别提取列表中每个成分的第一个元素和第二个元素，[是一个可以提取某个对象的一部分的函数，再使用cbind函数将它们添加到学生成绩的原始数据中。\n最后，我们使用order函数依姓氏和名字对数据集进行排序，完成这个数据任务。\n\nscoredata4= scoredata3[order(lastname, firstname),]\nscoredata4\n\n    firstname   lastname math science English      Score grade\n6      Cheryl    Cushing  512      85      28  0.3532485     C\n1        John      Davis  502      95      25  0.5592028     B\n9        Joel    England  573      89      27  0.6978361     B\n4       David      Jones  358      82      15 -1.1620473     E\n8        Greg       Knox  625      95      30  1.3378934     A\n5      Janice Markhammer  495      75      20 -0.6289776     D\n3  Bullwinkle      Moose  412      80      18 -0.8565414     D\n10       Mary    Rayburn  522      86      18 -0.1768163     C\n2      Angela   Williams  600      99      22  0.9238259     A\n7      Reuven    Ytzrhak  410      80      15 -1.0476242     E"
  },
  {
    "objectID": "blog/2024/05/17/calibration/index.html",
    "href": "blog/2024/05/17/calibration/index.html",
    "title": "Python中机器学习模型的校准",
    "section": "",
    "text": "calibration\n在我们利用机器学习模型来建模分类预测时，首要关注的指标能力当然是dircrimination，即模型的预测区分能力。常见的指标有sensitivity、specificity、AUROC等。我们在上一篇文章中介绍了如何选择最优分类阈值，这里我们接着介绍在选择了最优阈值后，如何评估模型的校准能力。\n所谓校准能力，即模型预测的概率与实际发生的概率一致。\n通俗来解释这个事情：比如说，我们模型预测某个病人患病的概率是0.8，那么，按照概率定义理解，模型预测概率为0.8时，100个人中应该有80个人最终患病，这个结果体现了模型的校准能力和稳定性。如果模型预测概率为0.8时，实际只有20个人患病，那么，模型的校准能力就不够好，你也不会信任这个模型在实际应用中的预测结果。这就是校准能力的重要性，即你的模型最终输出的概率值要准确反映出事件实际发生的概率。\n\n\n如何评价calibration\n\ncalibration plot\n\n\n\ncalibration curve\n\n\n上图是一个典型的calibration curve，也是我们在文章中常见的图。\n我们将模型预测概率cut或者quantile成5或者10个区间（bin），每个区间预测概率的均值作为x轴，每个区间实际发生的概率作为y轴，然后画出来这个曲线。这个图是评价模型校准能力的一个直观指标。python中可以轻松实现这个工作：\n\nfrom sklearn.calibration import calibration_curve\ny_means, pred_means = calibration_curve(y_true, y_pred, n_bins=10,strategy)\n\n理想情况下，所有点都在对角线上，即模型预测的概率与实际发生的概率完全一致。如果点在对角线上方，说明模型低估，反之，高估。\ncalibration level的定义有：\n\n\n\ncalibration level(Alonzo 2009)\n\n\n\n\n其他指标\n除了calibration plot，我们还可以用其他指标来评价模型的校准能力，比如说Brier score、Hosmer-Lemeshow test、calibration in the large等。这里不做详细介绍。\n我们感兴趣的是，当我们通过上述方法评价了模型的校准能力后，如果发现模型的校准能力不够好，我们应该怎么办？\n\n\n\ncalibrate model\n我们已经发现，模型输出值并不能代表概率。python中一般有predict_proba方法，即这个方法其实并不能保证输出的概率是真实的概率。\n\nfrom sklearn.ensemble import RandomForestClassifier\nmodel= RandomForestClassifier().fit(X_train, y_train)\ny_pred = model.predict_proba(X_test)[:,1]\n\n所以，我们需要对模型进行校准。\n\nPlatt scaling\nPlatt scaling是一种常见的校准方法，其原理是对模型输出的概率以及真实标签，用一个logistic regression模型来拟合，从而实现对模型输出的概率进行校准，拿到最终的概率。\n\nfrom sklearn.calibration import CalibratedClassifierCV\ncalibrated = CalibratedClassifierCV(model, method='sigmoid', cv=5)\ncalibrated.fit(X_train, y_train)\n\n\n\nIsotonic regression\nIsotonic regression是另一种校准方法，其原理是对模型输出的概率以及真实标签，用一个isotonic regression模型来拟合，从而实现对模型输出的概率进行校准。\n\nfrom sklearn.isotonic import IsotonicRegression\nir = IsotonicRegression().fit(y_pred, y_test)\n\n\n\nbayesian binning into quantiles\nBBQ是一种基于贝叶斯的校准方法，其原理是将预测概率分成若干个区间，然后在每个区间内对概率进行校准。该方法结合了分箱（binning）和贝叶斯推断的优点，可以在样本量较小时仍然保持较好的校准效果。\n还有其他方法可以供尝试。\n\n\n\ntake home message\n在利用机器学习模型进行分类预测时，我们不可忽视模型的校准能力。\n代码已经放进了星球里。\n\n\n\n\n\n\n\nReferences\n\nAlonzo, T. A. 2009. “Clinical Prediction Models: A Practical Approach to Development, Validation, and Updating: By Ewout w. Steyerberg.” Generic. Oxford University Press.\n\nCitationBibTeX citation:@online{lu2024,\n  author = {Lu, Zhen},\n  title = {Python中机器学习模型的校准},\n  date = {2024-05-17},\n  url = {https://leslie-lu.github.io/blog/2024/05/17/calibration/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLu, Zhen. 2024. “Python中机器学习模型的校准.” May 17, 2024.\nhttps://leslie-lu.github.io/blog/2024/05/17/calibration/."
  },
  {
    "objectID": "blog/2024/08/20/nested_layout/index.html",
    "href": "blog/2024/08/20/nested_layout/index.html",
    "title": "大图嵌小图",
    "section": "",
    "text": "由来\n星球里不断有同学问到如何在一个大图中嵌入小图，这里简单介绍一下。\n\n\n\nQ1\n\n\n\n\n\nQ2\n\n\n我们使用生存曲线及risk table作为例子，其中生存曲线是大图，risk table是小图。常见的图形为：\n\n\n\nsurvival curve\n\n\n想要把risk table嵌入到生存曲线中。\n\n\n方法一\n使用grid包，借助于grid包中的viewport函数。viewport用于定义一个绘图区域，可以在一个图形设备中创建多个独立的绘图区域，每个区域都有自己的坐标系和尺寸。\n\nsubvp &lt;- viewport(width = 0.35, height = 0.35, x = 0.75, y = 0.75)\nggsurv$plot\nprint(ggsurv$table, vp = subvp)\n\nviewport创建了一个子视口，它定义了一个相对主视口的区域。效果如下：\n\n\n\noption 1\n\n\n\n\n方法二\n使用annotation_custom函数，它可以在图形中添加自定义的图形元素。\n\nggsurv$plot + annotation_custom(ggplotGrob(ggsurv$table), xmin=1900, xmax=3000, ymin=0.6, ymax=1)\n\nggplotGrob将ggsurv$table转换为grob对象，以便在图形中使用。效果如下：\n\n\n\noption 2\n\n\n\n\n方法三\n使用ggpp包。\n\nsub_plot= tibble::tibble(\n    x= .98, y= .98, plot= list(ggsurv$table)\n)\nggsurv$plot + \n    geom_plot_npc(data = sub_plot, aes(npcx = x, npcy = y, label = plot))\n\n使用geom_plot_npc函数将子图添加到主图中，label表示要添加的子图。效果如下：\n\n\n\noption 3\n\n\n完整代码已经放在了星球里，感兴趣的同学可以自行查看。"
  },
  {
    "objectID": "blog/2020/01/06/index.html",
    "href": "blog/2020/01/06/index.html",
    "title": "Python基础要素之数值",
    "section": "",
    "text": "前面我们已经了解了如何创建、运行脚本，接下来我们了解下Python中最常用的数据类型。\n\n数值\nPython中最主要的4种数值类型分别是整数、浮点数、长整数和复数，这里只介绍整数和浮点数（即带小数点的数）。\n整数：\n\nx = 9\nprint(\"Output #4: {0}\".format(x))\nprint(\"Output #5: {0}\".format(3**4))\nprint(\"Output #6: {0}\".format(int(8.3)/int(2.7)))\n\nOutput #4: 9\nOutput #5: 81\nOutput #6: 4.0\n\n\nOutput #6演示了将数值转换成整数并进行除法运算。\n浮点数：\n\nprint(\"Output #7: {0:.3f}\".format(8.3/2.7))\ny = 2.5*4.8\nprint(\"Output #8: {0:.1f}\".format(y))\nr = 8/float(3)\nprint(\"Output #9: {0:.2f}\".format(r))\nprint(\"Output #10: {0:.4f}\".format(8.0/3))\n\nOutput #7: 3.074\nOutput #8: 12.0\nOutput #9: 2.67\nOutput #10: 2.6667\n\n\n\n#!/usr/bin/env python3\nfrom math import exp, log, sqrt\nprint(\"Output #11: {0:.4f}\".format(exp(3)))\nprint(\"Output #12: {0:.2f}\".format(log(4)))\nprint(\"Output #13: {0:.1f}\".format(sqrt(81)))\n\nOutput #11: 20.0855\nOutput #12: 1.39\nOutput #13: 9.0\n\n\n\n\n.format格式化\n\n# Add two numbers together\nx = 4\ny = 5\nz = x + y\nprint(\"Output #2: Four plus five equals {0:d}.\".format(z))\n# Add two lists together\na = [1, 2, 3, 4]\nb = [\"first\", \"second\", \"third\", \"fourth\"]\nc = a + b\nprint(\"Output #3: {0}, {1}, {2}\".format(a, b, c))\n\nOutput #2: Four plus five equals 9.\nOutput #3: [1, 2, 3, 4], ['first', 'second', 'third', 'fourth'], [1, 2, 3, 4, 'first', 'second', 'third', 'fourth']\n\n\n\n\ntype函数\nPython提供一个名为type的函数，可以对所有对象调用这个函数，来获得关于Python如何处理这个对象的更多信息。\n函数的语法非常简单：type(variable)会返回Python中的数据类型。如果你对一个数值变量调用这个函数，它会告诉你这个数值是整数还是浮点数，还会告诉你这个数值是否能当作字符串进行处理。\n此外，由于Python同样是面向对象的语言，所以你可以对Python中所有命名对象调用type函数，不仅是变量，还有函数、语句等。"
  },
  {
    "objectID": "blog/2024/09/04/casual_inference/index.html",
    "href": "blog/2024/09/04/casual_inference/index.html",
    "title": "从随意推断（casual inference）到因果推断（causal inference）",
    "section": "",
    "text": "因果分析的核心首先是因果问题，它决定了我们分析什么数据、如何分析这些数据以及我们的推断结论适用于什么人群。提出一个好的因果问题是比较难的，相对来讲，因果分析则要简单得多。本书属于方法应用性质，主要聚焦于因果推断的分析阶段。在本书前六章中，我们将讨论什么是因果问题，如何改进我们的问题，以及思考一些例子。\n\n因果问题是我们可以通过统计方法提出的一组更广泛问题中的一部分，这些问题依据数据科学的主要目的可以大致分为：描述、预测和因果推断 (Hernán, Hsu, and Healy 2019)。然而，在实际中，受到所使用统计方法的影响（如，这三类问题都可使用回归来处理）以及我们讨论它们的方式影响，我们常常会将这三种问题混淆在一起。当我们实际想要进行的是基于非随机数据的因果推断时，我们经常使用“关联”（association）这样的委婉说法，而不是直接声明我们想要估计因果效应 (Hernán 2018)。\n例如，最近一项关于流行病学研究中所使用语言的研究显示，估计效应的描述中最常见的词根是 “associate”，而许多研究人员也默认 “associate” 至少暗示了某种因果效应 (Figure 1) (Haber et al. 2022)。在分析的全部研究中，只有大约 1% 使用了 “cause” 这个词根。此外，三分之一的研究基于结论提出了相关的行动建议，而其中 80% 的建议都暗含了某种因果效应。而提出了行动建议的这些研究通常比那些只是描述效应的研究（如，使用 “associate” 和 “compare” 这样的词根）要有更强的因果效应暗示。另一方面，尽管一些研究暗示其目标是因果推断，但只有大约 4% 使用了本书将讨论的那些正式的因果推断模型，其它研究更多的做法是通过对先前相关研究或理论的引用讨论来证明他们建立的因果关系的合理性。\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\nFigure 1: 研究人员使用的词根的因果强度排名。具有更多 “Strong” 排名的词根比那些具有更多 “None” 或 “Weak” 评分的词根具有更强的因果暗示。数据来自 Haber 等人。\n\n\n\n\n\n因为没有明确地提出带有具体因果假设和目标的因果问题，我们最终都得到了“薛定谔的因果推断”：\n\n我们的结果表明，“薛定谔的因果推断”是很常见的，即研究一方面避免声明（甚至明确否认）对估计因果效应的兴趣，但另一方面又充满了因果意图、推断、因果暗示和行动建议。\n— Haber et al. (2022)\n\n这种方法是随意推断（casual inference）的一个例子：在没有做必要的工作来理解因果问题并处理因果假设的情况下进行推断。\n\n\n\n\n\n\nReferences\n\nHaber, N. A., S. E. Wieten, J. M. Rohrer, O. A. Arah, P. W. G. Tennant, E. A. Stuart, E. J. Murray, et al. 2022. “Causal and Associational Language in Observational Health Research: A Systematic Evaluation.” Am J Epidemiol 191 (12): 2084–97.\n\n\nHernán, Miguel A. 2018. “The C-Word: Scientific Euphemisms Do Not Improve Causal Inference From Observational Data.” American Journal of Public Health 108 (5): 616–19. https://doi.org/10.2105/ajph.2018.304337.\n\n\nHernán, Miguel A., John Hsu, and Brian Healy. 2019. “A Second Chance to Get Causal Inference Right: A Classification of Data Science Tasks.” CHANCE 32 (1): 42–49. https://doi.org/10.1080/09332480.2019.1579578.\n\nCitationBibTeX citation:@online{lu2024,\n  author = {Lu, Zhen},\n  title = {从随意推断（casual {inference）到因果推断（causal}\n    Inference）},\n  date = {2024-09-04},\n  url = {https://leslie-lu.github.io/blog/2024/09/04/casual_inference/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLu, Zhen. 2024. “从随意推断（casual Inference）到因果推断（causal\nInference）.” September 4, 2024. https://leslie-lu.github.io/blog/2024/09/04/casual_inference/."
  },
  {
    "objectID": "news/index.html",
    "href": "news/index.html",
    "title": "What I’m doing now",
    "section": "",
    "text": "May 11 2024: 🚀✨🎉 网站由 rmarkdown 更新为 quarto 支持。Hello World！"
  },
  {
    "objectID": "blog/index.html#section",
    "href": "blog/index.html#section",
    "title": "Blog",
    "section": "2024",
    "text": "2024\n\n\n    \n    \n                  \n            September 4, 2024\n        \n        \n            从随意推断（casual inference）到因果推断（causal inference）\n\n            \n            \n                \n                \n                    causal inference\n                \n                \n            \n            \n\n            因果推断\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            August 30, 2024\n        \n        \n            星球JC | 胃癌早期筛查工具\n\n            \n            \n                \n                \n                    journal club\n                \n                \n            \n            \n\n            预测模型星球Journal Club\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            August 29, 2024\n        \n        \n            github actions使用docker渲染quarto文档\n\n            \n            \n                \n                \n                    github\n                \n                \n                \n                    quarto\n                \n                \n                \n                    docker\n                \n                \n            \n            \n\n            Dockerfiles\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            August 20, 2024\n        \n        \n            大图嵌小图\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    ggplot2\n                \n                \n            \n            \n\n            调包\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            August 14, 2024\n        \n        \n            欢迎加入预测模型星球\n\n            \n            \n                \n                \n                    workshop\n                \n                \n                \n                    prediction model\n                \n                \n            \n            \n\n            Clinical Prediction Model\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            August 11, 2024\n        \n        \n            星球第二期workshop上线\n\n            \n            \n                \n                \n                    workshop\n                \n                \n                \n                    sample size\n                \n                \n                \n                    clinical research\n                \n                \n            \n            \n\n            Sample Size Calculations in Clinical Research\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            August 4, 2024\n        \n        \n            星球第一期workshop上线\n\n            \n            \n                \n                \n                    workshop\n                \n                \n                \n                    missing data\n                \n                \n                \n                    statistical methods\n                \n                \n            \n            \n\n            Statistical Methods for Analysis with Missing Data\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            August 3, 2024\n        \n        \n            倾向性评分加权\n\n            \n            \n                \n                \n                    propensity score\n                \n                \n                \n                    weighting\n                \n                \n            \n            \n\n            倾向性评分加权的具体介绍\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            July 19, 2024\n        \n        \n            预测模型领域新书推荐\n\n            \n            \n                \n                \n                    prediction model\n                \n                \n                \n                    book\n                \n                \n            \n            \n\n            临床预测模型方法与应用\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            June 23, 2024\n        \n        \n            2023年最新JCR影响因子发布\n\n            \n            \n                \n                \n                    sci\n                \n                \n                \n                    jcr\n                \n                \n            \n            \n\n            2023年最新JCR影响因子\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            June 7, 2024\n        \n        \n            常用损失函数\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    machine learning\n                \n                \n                \n                    deep learning\n                \n                \n                \n                    loss function\n                \n                \n            \n            \n\n            常用损失函数介绍\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            May 17, 2024\n        \n        \n            Python中机器学习模型的校准\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    machine learning\n                \n                \n                \n                    calibration\n                \n                \n            \n            \n\n            预测模型如何校准\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            May 13, 2024\n        \n        \n            Hierarchical composite endpoints治疗效应的可视化\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    clinical trial\n                \n                \n                \n                    endpoint\n                \n                \n            \n            \n\n            复合终点治疗效应的可视化\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            May 11, 2024\n        \n        \n            最优分类阈值\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    machine learning\n                \n                \n            \n            \n\n            分类问题中阈值的选择\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-1",
    "href": "blog/index.html#section-1",
    "title": "Blog",
    "section": "2023",
    "text": "2023"
  },
  {
    "objectID": "blog/index.html#section-2",
    "href": "blog/index.html#section-2",
    "title": "Blog",
    "section": "2022",
    "text": "2022"
  },
  {
    "objectID": "blog/index.html#section-3",
    "href": "blog/index.html#section-3",
    "title": "Blog",
    "section": "2021",
    "text": "2021"
  },
  {
    "objectID": "blog/index.html#section-4",
    "href": "blog/index.html#section-4",
    "title": "Blog",
    "section": "2020",
    "text": "2020\n\n\n    \n    \n                  \n            April 26, 2020\n        \n        \n            Python与矩阵\n\n            \n            \n                \n                \n                    linear algebra\n                \n                \n                \n                    matrix\n                \n                \n                \n                    python\n                \n                \n            \n            \n\n            linear algebra\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            April 25, 2020\n        \n        \n            R语言4.0发布上线\n\n            \n            \n                \n                \n                    r\n                \n                \n            \n            \n\n            R version update\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            March 22, 2020\n        \n        \n            概率密度与累积分布\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    probability\n                \n                \n            \n            \n\n            probability\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            March 12, 2020\n        \n        \n            空间中的向量\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    linear algebra\n                \n                \n            \n            \n\n            linear algebra\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            March 11, 2020\n        \n        \n            Python与向量\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    linear algebra\n                \n                \n                \n                    python\n                \n                \n            \n            \n\n            linear algebra\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            March 10, 2020\n        \n        \n            线性代数之向量\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    linear algebra\n                \n                \n            \n            \n\n            linear algebra\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            February 23, 2020\n        \n        \n            协方差分析\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    ancova\n                \n                \n            \n            \n\n            ANCOVA\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            February 22, 2020\n        \n        \n            试验设计与方差分析（4）\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    anova\n                \n                \n            \n            \n\n            ANOVA\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            February 20, 2020\n        \n        \n            试验设计与方差分析（3）\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    anova\n                \n                \n            \n            \n\n            ANOVA\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            February 18, 2020\n        \n        \n            试验设计与方差分析（2）\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    anova\n                \n                \n            \n            \n\n            ANOVA\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            February 17, 2020\n        \n        \n            试验设计与方差分析（1）\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    anova\n                \n                \n            \n            \n\n            ANOVA\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            February 15, 2020\n        \n        \n            方差分析\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    anova\n                \n                \n            \n            \n\n            ANOVA\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            February 6, 2020\n        \n        \n            R语言数据处理分析实例\n\n            \n            \n                \n                \n                    r\n                \n                \n            \n            \n\n            R Training\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            February 5, 2020\n        \n        \n            计算机概论7--操作系统\n\n            \n            \n                \n                \n                    linux\n                \n                \n            \n            \n\n            linux\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            February 4, 2020\n        \n        \n            样本量和检验效能的估计问题\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    sample size\n                \n                \n                \n                    power\n                \n                \n            \n            \n\n            sample size and power estimation\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            February 2, 2020\n        \n        \n            计算机概论6\n\n            \n            \n                \n                \n                    linux\n                \n                \n            \n            \n\n            linux\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            February 1, 2020\n        \n        \n            R语言基础--数据的输入\n\n            \n            \n                \n                \n                    r\n                \n                \n            \n            \n\n            R Training\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 31, 2020\n        \n        \n            计算机概论5\n\n            \n            \n                \n                \n                    linux\n                \n                \n            \n            \n\n            linux\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 30, 2020\n        \n        \n            R语言基础--函数\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    programming\n                \n                \n            \n            \n\n            R Training\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 29, 2020\n        \n        \n            R语言基础--运算符\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    programming\n                \n                \n            \n            \n\n            R Training\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 28, 2020\n        \n        \n            R语言基础--因子\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    programming\n                \n                \n            \n            \n\n            R Training\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 27, 2020\n        \n        \n            计算机概论4\n\n            \n            \n                \n                \n                    linux\n                \n                \n            \n            \n\n            linux\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 22, 2020\n        \n        \n            R语言编程入门\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    programming\n                \n                \n            \n            \n\n            R Training\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 21, 2020\n        \n        \n            上手vim编辑器\n\n            \n            \n                \n                \n                    vim\n                \n                \n                \n                    linux\n                \n                \n            \n            \n\n            vim\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 20, 2020\n        \n        \n            计算机概论3\n\n            \n            \n                \n                \n                    linux\n                \n                \n            \n            \n\n            linux\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 19, 2020\n        \n        \n            谈谈电脑的CPU\n\n            \n            \n                \n                \n                    linux\n                \n                \n            \n            \n\n            linux\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 18, 2020\n        \n        \n            Python正则表达式与模式匹配\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    regular expression\n                \n                \n                \n                    pattern matching\n                \n                \n            \n            \n\n            Python Training\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 17, 2020\n        \n        \n            你真的了解自己的电脑吗？\n\n            \n            \n                \n                \n                    linux\n                \n                \n            \n            \n\n            linux\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 16, 2020\n        \n        \n            R语言的初体验\n\n            \n            \n                \n                \n                    r\n                \n                \n            \n            \n\n            R Training\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 11, 2020\n        \n        \n            kNN改进约会网站的配对效果\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    kNN\n                \n                \n            \n            \n\n            kNN\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 10, 2020\n        \n        \n            Python基础要素之字符串\n\n            \n            \n                \n                \n                    python\n                \n                \n            \n            \n\n            Python Training\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 9, 2020\n        \n        \n            笔记\n\n            \n            \n                \n                \n                    biostatics\n                \n                \n                \n                    causal inference\n                \n                \n            \n            \n\n            causal inference\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 8, 2020\n        \n        \n            如何用Python自编k-近邻算法？\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    kNN\n                \n                \n            \n            \n\n            kNN\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 7, 2020\n        \n        \n            NumPy函数库基础\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    numpy\n                \n                \n            \n            \n\n            Python Training\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 6, 2020\n        \n        \n            Python基础要素之数值\n\n            \n            \n                \n                \n                    python\n                \n                \n            \n            \n\n            Python Training\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 5, 2020\n        \n        \n            可惜没如果——因果关系的三个层级\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    causal inference\n                \n                \n            \n            \n\n            因果推断的基本知识\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 4, 2020\n        \n        \n            Python初体验\n\n            \n            \n                \n                \n                    pycharm\n                \n                \n                \n                    python\n                \n                \n            \n            \n\n            Python Training\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 2, 2020\n        \n        \n            因果推断-2\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    causal inference\n                \n                \n            \n            \n\n            关于因果推断的基本概念\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 1, 2020\n        \n        \n            鼠年加油\n\n            \n            \n                \n                \n                    happy new year\n                \n                \n            \n            \n\n            新年快乐\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-5",
    "href": "blog/index.html#section-5",
    "title": "Blog",
    "section": "2019",
    "text": "2019\n\n\n    \n    \n                  \n            December 11, 2019\n        \n        \n            因果推断\n\n            \n            \n                \n                \n                    statistics\n                \n                \n                \n                    biostatistics\n                \n                \n                \n                    causal inference\n                \n                \n            \n            \n\n            关于因果推断的基本概念\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            November 28, 2019\n        \n        \n            统计学是干嘛的？\n\n            \n            \n                \n                \n                    statistics\n                \n                \n                \n                    biostatistics\n                \n                \n            \n            \n\n            聊一聊什么是统计学\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "I have made contributions to neurosyphilis and cancer research, and my work has been published in journals including eClinicalMedicine, JMIR Public Health and Surveillance, among others. Besides, I have patented a clinical diagnosis system for neurological syphilis (LU504466, rank 1/1).\nIn addition to my work on neurosyphilis and cancer, I have extensive experience applying advanced statistical concepts, artificial intelligence, and causal inference methods to conduct real-world research based on electronic health records. I have conducted multiple studies in the areas of COVID-19, HPV and HIV infections, and cardiovascular disease."
  },
  {
    "objectID": "publications/index.html#journal-articles",
    "href": "publications/index.html#journal-articles",
    "title": "Publications",
    "section": "Journal articles",
    "text": "Journal articles"
  },
  {
    "objectID": "publications/index.html#working-papers",
    "href": "publications/index.html#working-papers",
    "title": "Publications",
    "section": "Working papers",
    "text": "Working papers"
  },
  {
    "objectID": "publications/index.html#book-chapters",
    "href": "publications/index.html#book-chapters",
    "title": "Publications",
    "section": "Book chapters",
    "text": "Book chapters"
  },
  {
    "objectID": "publications/index.html#reviews",
    "href": "publications/index.html#reviews",
    "title": "Publications",
    "section": "Reviews",
    "text": "Reviews"
  },
  {
    "objectID": "publications/index.html#selected-seminar-papers",
    "href": "publications/index.html#selected-seminar-papers",
    "title": "Publications",
    "section": "Selected seminar papers",
    "text": "Selected seminar papers"
  },
  {
    "objectID": "publications/index.html#translations",
    "href": "publications/index.html#translations",
    "title": "Publications",
    "section": "Translations",
    "text": "Translations"
  },
  {
    "objectID": "blog/2024/08/29/docker/index.html",
    "href": "blog/2024/08/29/docker/index.html",
    "title": "github actions使用docker渲染quarto文档",
    "section": "",
    "text": "在用github pages更新静态网站内容时，发现github actions突然报之前并未出现过的错误：\n\n\n\nerror\n\n\n错误表明系统在尝试调用UPower、Vulkan服务时遇到了问题，但是我也没找到到底是在哪里调用。而且UPower是一个用于管理电池电量和电源管理的服务，通常在桌面环境中使用。我尝试更新unbuntu上upower这些包的时候，发现github pages这种方式并没有给使用这类包的权限，看google上似乎也没有针对这个错误的比较好的解决方式。\n没办法，只能选择换一种方法来render网站的内容，我又不想在本地利用quarto每次手动来render网页，最后只能尝试选择利用docker中配置的ubuntu环境来render github repository中更新的内容，然后再像以前一样，把render出来的html，重新利用actions publish到新的repository中，这样来更新网页内容，避开直接使用actions出现的这个错误。\n我给docker的配置Dockfiles如下，给有可能遇见同样错误的同学参考：\n# install R and dependencies\nENV DEBIAN_FRONTEND=noninteractive\n# Update package indices\nRUN apt-get update -qq\n# Install helper packages\nRUN apt-get install --no-install-recommends -y software-properties-common dirmngr\n# Add the signing key for the R repository\nRUN wget -qO- https://cloud.r-project.org/bin/linux/ubuntu/marutter_pubkey.asc | tee -a /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc\n# Add the R 4.0 repository from CRAN\nRUN add-apt-repository \"deb https://cloud.r-project.org/bin/linux/ubuntu $(lsb_release -cs)-cran40/\"\nRUN apt-get install --no-install-recommends -y r-base \nRUN R -e \"if (!requireNamespace('renv', quietly=TRUE)) install.packages('renv', repos = c(CRAN = 'https://cloud.r-project.org'))\"\nWORKDIR /project\nCOPY renv.lock .\nENV RENV_PATHS_LIBRARY /renv\nRUN mkdir -p renv\nCOPY .Rprofile .Rprofile\nCOPY renv/activate.R renv/activate.R\nCOPY renv/settings.json renv/settings.json\nRUN R -e \"renv::activate(); renv::restore(repos = 'https://cloud.r-project.org')\"\n\n# install python and dependencies\nRUN apt-get update && apt-get install -y python3.10 python3-pip \\\n    && rm -rf /var/lib/apt/lists/*\nCOPY requirements.txt .\nRUN pip3 install --no-cache-dir --break-system-packages -r requirements.txt\n这样使用docker的方式就不会报错了：\n\n\n\nsuccess\n\n\n完整代码已经放在了星球里，感兴趣的同学可以自行查看。"
  },
  {
    "objectID": "blog/2024/08/11/workshop_002/index.html",
    "href": "blog/2024/08/11/workshop_002/index.html",
    "title": "星球第二期workshop上线",
    "section": "",
    "text": "我们星球正式上线第二期workshop啦！\n\n\n\nworkshop\n\n\n本期workshop主题是“Sample Size Calculations in Clinical Research”。本期workshop将从临床研究中不同试验设计的角度出发，介绍如何基于不同设计类型（平行设计、交叉设计、析因设计、成组序贯设计等）、比较类型（非劣效、等效、优效试验）、主要终点指标等因素进行样本量计算。\n\n\n\nscreenshot\n\n\n目前暂定的安排是每周一节一小时，直到本期workshop的全部内容结束。\n\n\n\nused files\n\n\n后续我们会根据星球中同学们的需求，不定时地开展更多主题的workshop，具体时间请关注我们的公众号和星球，我们会在这两个平台上发布最新的信息。"
  },
  {
    "objectID": "blog/2024/08/03/propensity_score_weighting/index.html",
    "href": "blog/2024/08/03/propensity_score_weighting/index.html",
    "title": "倾向性评分加权",
    "section": "",
    "text": "背景介绍\n对于非RCT类的观察性研究，由于分组的非随机性，导致了研究偏倚的存在，致使观察到的效应很多时候往往并不可用。为了解决这个问题，研究者们提出了倾向性评分的方法，通过倾向性评分的计算，可以使得试验组和对照组之间的分布更加接近，从而减少了研究偏倚的影响。\n我们公众号以往有过五篇类似的介绍内容，分别是：\n\n倾向性评分分析\n倾向性评分分析的统计学考虑\n倾向性评分匹配的生存分析怎么做\n倾向性评分overlapping weighting的SAS实现（一）\n生存资料倾向性评分OW的SAS实现（二）\n\n其中，后面两篇文章提到了倾向性评分加权中的overlapping weighting方法，这篇文章将对倾向性评分加权的方法进行详细介绍。\n\n\n因果效应\n首先，我们来看下几种因果效应。\nATE即平均处理效应（average treatment effect），是指在试验组和对照组之间的处理效应的差异。理想情况下，随机对照试验估计出来的效应即ATE，但是在实际研究中，由于种种原因，我们往往无法进行随机对照试验。由于ATE的估计人群是试验组和对照组的总体，ATE假设两组受试者是有相同的概率/机会接受某一种处理的，然而，实际研究中，研究者往往更加关注的是ATE的局部估计，即在某一特定的人群中（一般是接受治疗的试验组），处理的效应是多少，而这个效应即为ATT（average treatment effect on the treated）。由于ATT只需要对处理组人群估计因果处理效应，对于RCT而言，潜在的治疗效果和治疗组分配是相互独立的，因此，ATT即为ATE；然而，对于非RCT类研究而言，二者是不同的。\n我们还可以计算ATC（average treatment effect on the control），即对于未接受治疗的人群，如果接受治疗，其效应是多少。此外，还有ATM（average treatment effect among the evenly matchable），即在对照组中，找到与试验组相匹配的人群，计算在这个匹配的总体人群中的治疗效应；ATO（average treatment effect among the overlap population），即在试验组和对照组的重叠人群中，计算治疗效应。相比于ATM，ATO有着更好的方差属性，由于其不像ATM那样匹配要求，转而是选择两组重叠的中间人群，因此，ATO的估计更加稳健。\n\n\n倾向性评分加权\n针对以上五种因果效应，我们可以通过倾向性评分加权的方法来进行相应的估计。这里直接给出五种权重的计算公式：\n\nATE：\\(w_{ATE} = \\frac{Z_i}{e_i} + \\frac{1 - Z_i}{1 - e_i}\\)\nATT：\\(w_{ATT} = \\frac{e_iZ_i}{e_i} + \\frac{e_i(1-Z_i)}{1-e_i}\\)\nATC：\\(w_{ATC} = \\frac{(1-e_i)Z_i}{e_i} + \\frac{(1-e_i) (1-Z_i)}{1 - e_i}\\)\nATM：\\(w_{ATC} = \\frac{(1-e_i)Z_i}{e_i} + \\frac{(1-e_i) (1-Z_i)}{1 - e_i}\\)\nATO：\\(w_{AT0} = (1-e_i)Z_i + e_i(1-Z_i)\\)\n\n以上五种加权的示例以及具体实现的全部代码，我们已经放在了星球里，感兴趣的同学可以自行查看。\n这里我们展示下加权后人群的分布情况。\n\n原始人群的ps分布\n\n\n\nps of original population\n\n\n\n\nATE\n\n\n\nATE\n\n\n\n\nATT\n\n\n\nATT\n\n\n\n\nATC\n\n\n\nATC\n\n\n\n\nATM\n\n\n\nATM\n\n\n\n\nATO\n\n\n\nATO\n\n\n\n\n\n总结\n相信通过以上可视化的展示，大家会更容易理解倾向性评分加权的方法对目标人群的选择以及治疗效应的解释。借助于合适的效应加权，我们可以估计出治疗效应并对于以上五种治疗效应的估计值。"
  },
  {
    "objectID": "blog/2024/06/23/JCR_2023/index.html",
    "href": "blog/2024/06/23/JCR_2023/index.html",
    "title": "2023年最新JCR影响因子发布",
    "section": "",
    "text": "2023年最新JCR影响因子\n最新的影响因子前几天已经发布了，和去年一样，大家在公众号后台回复”JCR2023”，即可拿到最全的总结excel表格，包括2023年的最新影响因子，以及各个学科的排名，希望对大家有所帮助。\n\n\n关注的一些期刊\n几乎全部的期刊影响因子都回落到了几年前的水平。\n四大神刊中JAMA几近腰斩，柳叶刀系列的多个子刊也是如此。\n\n\n\n柳叶刀系列\n\n\n医工交叉领域也是普遍下滑。medical informatics数字医疗部分，lancet digital health和npj Digital Medicine分别是23.8和12.4分。\n以往动辄二三十分的盛况不再。"
  },
  {
    "objectID": "blog/2020/03/22/index.html",
    "href": "blog/2020/03/22/index.html",
    "title": "概率密度与累积分布",
    "section": "",
    "text": "概率分布是统计学的基础，统计学中不少概念和思想都与概率分布有关系。理解了概率分布，很多原来不明白的问题很可能就迎刃而解了。\n\n理解累积分布和概率密度的概念。\n累积分布通俗地说，就是从0一直累积到100%，累积的速度可以相等，也可以不相等，累积分布函数一般用F(x)来表示；概率密度也就是概率的密度，通俗而言，密度就是在某个点上数据比较集中，反映了数据的不同变化特征，概率密度函数一般用f(x)表示。\n理解累积分布的斜率与概率密度的关系。\n累积分布的斜率越大，概率密度也越大。事实上，密度值等于累积分布中对应点的斜率。\n理解累积分布与概率密度的关系。\n密度与累积分布的关系是：密度=累积分布的增加量/长度，或者，累积分布的增加量=密度*长度，即概率密度曲线下面积等于相应累积分布函数增加量。对于累积分布而言，如果看整条曲线，即从0增加到100%，因此对应的概率密度曲线下，其面积一定也是100%。\n\n我们常说的正态分布、二项分布、t分布等概率分布其实都是概率密度函数，所以它们的曲线下方的面积都是100%，只是不同点的密度不同而已。如有的分布可能在0的时候密度较高（数据主要集中在0附近）。"
  },
  {
    "objectID": "blog/2020/03/10/index.html",
    "href": "blog/2020/03/10/index.html",
    "title": "线性代数之向量",
    "section": "",
    "text": "空间是贯穿整个线性代数的主干脉络和核心概念。下面我们学习利用向量这个工具对空间进行定量描述。\n\n关于向量\n直观地说，把一组数字排列成一行或一列，就称为向量。它可以作为对空间进行描述的有力工具。\n如一个简单的二维向量[4,5]⊤，这个向量有两个成分：第一个成分是数字4，第二个成分是数字5。其可以理解为二维平面中x坐标为4，y坐标为5的一个点，也可以理解为以平面中的原点(0,0)为起点，(4,5)为重点的一条有向线段，这就是二维向量的空间表示。一个向量中成分的个数就是该向量的维数。\n不过，向量也不局限于用来直接描述空间中的点坐标和有向线段，也可以凭借基础的数据表示功能，成为一种描述事物属性的便捷工具。\n如你的考试成绩为：语文85分，数学92分，英语89分。由于这三门课具有不同科目属性，因此，可以使用一个三维向量来对其进行表示，即score=[85,92,89]⊤。此时不仅仅可以把向量score看作是一个盛放数据的容器，似乎也可以利用它讲科目考试成绩和空间建立起某种关联。\n又如，在自然语言处理中，也少不了向量这个重要的工具。程序在进行文本阅读时，首先会对文本材料进行分词处理，然后使用向量对词汇进行表示。这是因为向量很适合将对象的属性和特征对应到高维空间中进行定量表达，同时在此基础上进行进一步的后续处理，如判断词汇之间的相似性等。\n我们陆续接触到一些数据处理的基本方法：如投影、降维等，这些方法都是在向量描述的基础上实现的。\n\n\n列向量\n向量对应地就拥有两种表达方式：若元素是纵向排列的，就称为列向量；若元素是横向排列的，就成为行向量。在没有特殊说明的情况下，一般都默认为列向量。为什么会偏爱列向量呢？主要是为了方便后续的向量坐标变换、空间之间的映射等计算过程的处理。\n将一个矩阵A所表示的映射作用于某个向量x上时，习惯上将其写成矩阵乘以向量的表达形式，即Ax。这种写法的数据表示基础就是：向量x必须是一个列向量。"
  },
  {
    "objectID": "blog/2020/02/22/index.html",
    "href": "blog/2020/02/22/index.html",
    "title": "试验设计与方差分析（4）",
    "section": "",
    "text": "重复测量设计\n重复测量是指对同一观察对象的同一观察指标在不同的时间点上进行多次测量，用于分析观察指标在不同时间上的变化规律。\n这类测量资料在医学研究中比较常见，例如，药效分析中常分析给药后不同时间的疗效比较，由于同一受试对象在不同时点的观测值之间往往彼此不独立，存在某种程度的相关，因此不能满足常规统计方法所要求的独立性假定，使得其分析方法有别于一般的统计分析方法。\n最常见的重复测量资料是前后测量设计，如高血压患者治疗前后的血压，其设计与配对设计t检验的试验表达完全相同，但却是两种不同类型的设计，其区别在于：\n\n配对设计中同一对子的两个试验单位可以随机分配处理，两个试验单位同期观察试验结果，可以比较处理组间差异，而前后测量设计不能同期观察试验结果；\n配对t检验要求同一对子的两个试验单位的观察结果分别与差值相互独立，差值服从正态分布，而前后测量设计前后两次测量的结果通常与差值不独立；\n配对设计用平均差值推断处理作用，而前后测量设计除了分析平均差值外，还可以进行相关回归分析。\n\n重复测量设计大体有两类，一类是对每个人在同一时间不同因子组合间测量，另外一类是对每个人在不同时间点上重复。前者常见于裂区设计，而后者常见于经典试验设计，即包括前测、处理一次或几次后测的情况，后者比前者要多见。不论沿裂区方向还是沿时间点方向重复，个体内因子无一例外的都是重复测量因子。\n重复测量设计的特点是一定有个体内因子，但不一定有个体间因子，后者是不同处理组合或不同个体组，而且即使有不同组群（如男性和女性），但人人都经历重复测量，而不是一组接受重复测量，另一组不接受。\n具有重复测量的设计，即在给予某种处理后，在几个不同的时间点上从同一个受试对象身上重复获得指标的观测值，有时是从同一个个体的不同部位上重复获得指标的观测值。由于这种设计符合许多医学试验本身的特点，故在医学科研中应用的频率相当高。\n如果试验中共有k个试验因素，其中只有m个因素与重复测量有关，则称为具有m个重复测量的k因素设计。重复测量资料来源于同一受试对象的某一观测值的多次重复测量，常见的重复测量数据的基本格式：N个观测个体，g个处理组，X为观测指标，p为重复测量次数。\n在对重复测量资料进行方差分析时，除了要求样本是随机的，在处理的同一水平上观测是独立的，及每一水平的测定值都来自正态总体外，特别强调协方差的复合对称性或球形性。因此，在进行重复测量资料的方差分析前，应先对资料的协方差阵进行球形性检验。若满足球形性要求，则直接进行方差分析；若不满足球形性要求，则需对与时间有关的F统计量分子、分母的自由度进行校正，以减少犯I类错误的概率，或直接进行多变量方差分析。\n对重复测量试验数据的方差分析，需考虑两个因素的影响：一个因素是处理分组，可通过施加干预和随机分组来实现；另一个因素是测量时间，由研究者根据专业知识和要求确定。因此重复测量资料的变异可分解为处理因素、时间因素、处理和时间的交互作用、受试对象间的随机误差和重复测量的随机误差等5部分。\n重复测量设计的优点：每一个体作为自身的对照，克服了个体间的变异，分析时可更好地集中于处理效应，且被试者自身差异的问题不再存在，即减少了一个差异来源；重复测量设计的每一个体作为自身的对照，研究所需的个体相对较少，因此更加经济。\n重复测量设计的缺点：滞留效应，前面的处理效应有可能滞留到下一次的处理；潜隐效应，前面的处理效应有可能激活原本以前不活跃的效应；学习效应，由于是逐步熟悉试验，因此研究对象的反应能力有可能逐步得到提高。\n对于重复测量资料的分析处理，我们应用较多的是单变量方差分析的一般线性模型方法。在R数据格式中，重复测量资料同一观察单位在各测量点的测量值用一组变量来表示，计算时将这一组变量当作一个整体作为反应变量来处理。"
  },
  {
    "objectID": "blog/2020/02/18/index.html",
    "href": "blog/2020/02/18/index.html",
    "title": "试验设计与方差分析（2）",
    "section": "",
    "text": "拉丁方设计\n完全随机设计只涉及一个处理因素，随机区组设计涉及一个处理因素、一个区组因素/配伍因素。若实验研究涉及一个处理因素和两个控制因素，每个因素的类别数或水平数相等，此时可采用拉丁方设计来安排试验，将两个控制因素分别安排在拉丁方设计的行和列上。\n将k个不同符号排成k行k列，使得每一个符号在每一行、每一列都只出现一次的方阵，叫做k*k拉丁方。\n拉丁方设计就是将处理从纵横两个方向排列为区组/重复，使每个处理在每一行和每一列中出现的次数相等（通常为1次），即在行和列两个方向都进行局部控制，所以它是比随机区组多一个方向局部控制的随机排列设计，因而具有较高精确性。\n拉丁方设计的特点是处理数、重复数、行数、列数都相等，即处理数=行区组数=列区组数=重复次数，它的每一行和每一列都是一个区组或一次重复，而每一个处理在每一行或每一列都只出现1次\n拉丁方试验设计的步骤：\n\n选择标准方：标准方是指代表处理因素水平的字母。在进行拉丁方设计时，首先要根据试验处理水平数k从标准方表中选定一个k*k的标准方，随后要对选定的标准方的行、列和处理进行随机化排列；\n列随机\n行随机\n处理随机\n\n拉丁方设计的特点是纵横两个方向都设了区组，从而可在两个方向上对土壤等差异（指田间试验时）进行局部控制。试验有k个处理，便有k*k个观测值。进行方差分析时，从总变异方差中除了分解出处理间方差和误差项方差外，还可以分解出纵横两个区组的方差，这样可使误差项方差进一步减小，所以拉丁方试验的精确度比随机区组试验更高。\n拉丁方设计的优点是：精确度高；缺点是：由于重复数与处理数必须相等，使得两者之间相互制约，缺乏伸缩性。因此采用拉丁方设计时试验的处理数不能太多，一般以4～10个为宜。\n\n\n析因设计\n单因素方差分析只涉及一个处理因素，该因素至少有两个水平，只是根据试验对象的属性和控制试验误差的需要，采用的试验设计方法有所不同。如比较注射4种不同剂量雌激素对大白鼠子宫重量的影响，处理因素是注射不同剂量的雌激素，有4个水平。完全随机设计是将n只白鼠随机分4组，随机区组设计是将n只白鼠按出生体重相近的原则，4只一组配成区组后，每个区组内随机分配4个处理水平，拉丁方设计则是在随机区组设计的基础上增加了一个列区组，如白鼠有4个种系（行区组），每个种系的4只白鼠按体重分成4个级别（列区组）。可以看出，完全随机设计、随机区组设计和拉丁方设计的处理因素没有变化，都是比较注射4种不同剂量雌激素带来的差别，只是改变了设计的方法，在同样的试验条件下，通过改进试验设计方法可以大大提高试验的效率，如上述试验，白鼠按体重配成区组后再施加处理（随机区组设计），试验的误差均方通常小于完全随机设计。\n而上述介绍的各种试验设计方法，严格地说，它们仅适用于只有1个处理因素的试验问题之中，其他因素都属于区组因素，即与处理因素无交互作用。若试验所涉及的处理因素的个数为2或以上，当各因素在试验中所处的地位基本平等，且因素之间存在1级（即2因素之间）、2级（即3因素之间）乃至更复杂的交互作用时，则需选用析因设计。\n在评价药物疗效时，除需知道A药和B药各剂量的疗效外（主效应），还需知道两种药同时使用的协同疗效，析因设计及其相应的方差分析用于分析药物的单独效应、主效应和交互效应。\n主效应：某因素各水平的平均效应；单独效应：在每个B水平，A的效应；或在每个A水平，B的效应；交互效应：某因素各水平的单独效应随另一因素水平变化而变化，则称两因素之间存在交互效应，包括协同、拮抗作用。\n析因设计有交互作用的二因子方差分析将总偏差平方和做如下分解：\n\n误差偏差平方和：反映随机误差对试验结果的影响；\n因子A引起的偏差平方和：除含有误差波动外，反映因子A对试验结果的影响；\n因子B引起的偏差平方和：除含有误差波动外，反映因子B对试验结果的影响；\n因子A与B的交互作用的偏差平方和：反映因子A与B的交互作用对试验结果的影响。\n\n如果不存在交互效应，则只需考虑各因素的主效应；在方差分析中，如果存在交互效应，解释结果时，要逐一分析各因素的单独效应，找出最优搭配。在两因素析因设计时，只需考虑一阶交互效应；3个因素及以上时，除一阶交互效应外，还需考虑二阶、三阶等高阶交互效应，解释将更复杂。"
  },
  {
    "objectID": "blog/2020/02/15/index.html",
    "href": "blog/2020/02/15/index.html",
    "title": "方差分析",
    "section": "",
    "text": "方差分析\nt检验和u检验不适用于多个样本均数的比较，而用方差分析比较多个样本均数，可以有效地控制I类错误。\n方差分析（analysis of variance，ANOVA）由英国统计学家R.A.Fisher首先提出，以F命名其统计量，故方差分析又称F检验。\n方差分析的基本思想是根据研究的目的和设计类型，将总变异的离均差平方和SS及其自由度v分别分解成相应的若干部分，然后求各相应部分的变异（数理统计证明，总的离均差平方和等于各部分离均差平方和之和）；再用各部分的变异与组内（或误差）变异进行比较，得出统计量F值；最后根据F值的大小确定p值，作出统计推断。\n方差分析的用途很广，包括两个或多个样本均数间的比较，分析两个或多个因素间的交互作用，回归方程的线性假设检验，多元线性回归分析中偏回归系数的假设检验，两样本的方差齐性检验等。\n方差分析的应用条件为：各样本需是相互独立的随机样本；各样本来自正态分布总体；各总体方差相等，即方差齐性。\n\n\n方差分析基本术语\n实验设计和方差分析都有自己相应的语言。\n以研究某药物对某癌细胞株增殖影响的研究为例，现有两种药物：新研究药物（Treatdrug）和对照组药物（Controldrug）。\n我们提取培养10个某癌细胞株作为研究对象，随机分配一半癌细胞株接受为期96h的Treatdrug治疗，另一半接受为期96h的Controldrug治疗。研究结束时，对两组细胞株的细胞抑制率进行评估。\n在这个实验设计中，治疗方案是两水平（Treatdrug和Controldrug）的组间因子，之所以称作组间因子是因为每个患者都仅被分配到一个组别中，没有患者同时接受Treatdrug和Controldrug。\n细胞抑制率是因变量，治疗方案是自变量。由于在每种治疗方案下观测数相等，因此这种设计也称为均衡设计；若观测数不同，则称为非均衡设计。\n因为仅有一个类别型变量，这种设计又称为单因子方差分析或进一步称为单因子组间方差分析。\n方差分析主要是通过F检验来进行效果评测，若治疗方案的F检验显著，则说明96h后两种药物的细胞抑制率均值不同。\n假设只对Treatdrug的效果感兴趣，则需要将10个癌细胞株都放在Treatdrug组中，然后在治疗24h和96h后分别评估疗效。此时，时间是两水平（24h和96h）的组内因子，因为每个癌细胞株在时间的所有水平下都进行了测量，因此这种设计称为单因子组内方差分析；又由于每个癌细胞株都不止一次被测量，也称作重复测量方差分析。若时间的F检验显著，则说明细胞抑制率在24h和96h间发生了改变。\n现假设对治疗方案差异和它随时间的改变都感兴趣，则可以将两个设计结合起来：随机分配一半癌细胞株到Treatdrug组，另一半到Controldrug组，在24h和96h分别评估它们的细胞抑制率。治疗方案和时间都作为因子时，既可以分析治疗方案的影响和时间的影响，也可以分析治疗方案和时间的交互作用。前两个为主效应，交互部分为交互效应。在这种情况下，需要进行3次F检验，治疗方案因素1次，时间因素1次，两者的交互因素1次。若治疗方案显著，说明Treatdrug和Controldrug对癌细胞的抑制效果不同；若时间显著，表明细胞抑制率在24h和96h间发生了改变；若两种因素交互效应显著，说明两种药物随着时间变化对癌细胞的一直效果不同（即细胞抑制率从24h到96h的改变程度在Treatdrug和Controldrug之间是不同的）。\n当设计中包含两个甚至更多因子时，便是多因子方差分析设计。两个因子时称为双因子方差分析，三因子时称为三因子方差分析。若因子设计包括组内因子和组间因子，又称为混合模型方差分析。\n这里，即使不同的癌细胞株被随机分配到不同的治疗方案中，但在研究开始时两组癌细胞株的增殖速度可能不同，治疗后的差异可能是最初的增殖速度不同导致的，而不是实验方案的影响。增殖速度也可以解释因变量的组间差异，因此它常被称为混杂因素。如果我们在评测治疗方案类型的影响前，对组建的统计学差异进行统计性调整，将初始增殖速度作为协变量，这样的设计称为协方差分析。\n当因变量不止一个时，该设计被称为多元方差分析，若还存在协变量，则称为多元协方差分析。"
  },
  {
    "objectID": "blog/2020/02/04/index.html",
    "href": "blog/2020/02/04/index.html",
    "title": "样本量和检验效能的估计问题",
    "section": "",
    "text": "统计分析人员经常会被问到这样一个问题：我这个研究到底需要多少个研究对象呢？\n这个问题可以通过检验效能分析或样本量估算来解决，它在实验设计中占有重要地位。检验效能分析可以帮助在给定置信度的情况下，判断检测到给定效应值时所需的样本量；反过来，它也能够在给定置信度水平情况下，计算在某样本量内能检测到给定效应值的概率，如果该概率过低，可以考虑修改或放弃该实验。\n由于检验效能分析针对的是假设检验，我们回顾下假设检验的过程。\n在统计假设检验中，首先要对总体分布参数作出一个假设（无效假设），然后从总体分布中抽样，通过样本计算所得的统计量来对总体参数进行推断。假定无效假设为真，若计算获得观测样本的统计量或更大统计量的概率（p值）非常小，小于预先设定的阈值（检验的显著性水平），便可以拒绝无效假设，接受备择假设。\n科学研究中，越来越强调样本量的估算。确定适当的样本含量可以节约资源，并可防止因为样本含量过少引起的检验效能偏低，出现假阴性错误，这是当前医学研究中值得注意的问题。\n样本量的估算方法很多，不同的统计检验方法使用的计算公式也不一样。一般影响样本量的因素有以下7种：\n\n研究事件的发生率：研究事件预期的发生率越高，所需的样本量越小，反之则越大；\n研究因素的有效率：有效率越高，即实验组和对照组比较数值差异越大，样本量就可以越小，使用小样本就能够达到统计学上的显著性，反之则越大；\n设定假设检验的I类错误概率α，即检验水准或显著性，为假阳性错误出现的概率。α越小，所需的样本量越大，反之则越小。α水平由研究者根据具体情况决定，通常α取0.05或0.01；\n设定假设检验的II类错误概率β，或检验效能1-β。II类错误为假阴性错误，即在特定的α水准下，若总体参数之间确实存在着差别，此时该次实验能发现此差别的概率。检验效能又称把握度，即避免假阴性错误的能力，β越小，检验效能越高，所需的样本量越大，反之就越小。β水平由研究者根据情况决定，通常取β为0.2、0.1或0.05，即1-β=0.8、0.9或0.95，也就是说把握度为80%、90%或95%；\n了解由样本推断总体的一些信息。总体标准差一般未知，可用样本标准差代替；\n处理组间差别的估计，即确定容许误差。容许误差越小，需要的样本量越大；\n采用统计学检验时，当研究结果高于和低于效应指标的界限均有意义时，应该选择双侧检验，所需的样本量就大。当研究结果仅高于或低于效应指标的界限有意义时，则应该选择单侧检验，所需的样本量就小。\n\n在这些影响因素中，确定样本含量最重要的4个因素为I类错误概率、II类错误概率、推断总体的一些信息和容许误差。\n研究者放宽显著性水平时（换句话说，使得拒绝无效假设更容易时），检验功效增加。类似地，样本量增加，检验功效增加。\n通常来说，研究目标是维持一个可接受的显著性水平，尽量使用较少的样本，然后最大化统计检验的功效，也就是说，最大化发现真实效应的概率，并最小化发现错误效应的概率，同时把研究成本控制在合理的范围内。"
  },
  {
    "objectID": "blog/2020/02/01/index.html",
    "href": "blog/2020/02/01/index.html",
    "title": "R语言基础–数据的输入",
    "section": "",
    "text": "前言\nR可从键盘、文本文件、Excel、流行的统计软件、特殊格式的文件，以及多种关系型数据库中导入数据。\n\n\n键盘输入数据\nR中的函数edit()会自动调用一个允许手动输入数据的文本编辑器，步骤如下：\n\n创建一个空数据框(或矩阵)，其中变量名和变量的模式需与预期的最终数据集一致；\n针对这个数据对象调用文本编辑器，输入数据，并将结果保存回此数据对象中。\n\n\n\n\nexample\n\n\n类似于age=numeric(0)的赋值语句创建一个指定模式但不含实际数据的变量。编辑的结果需要赋值回对象本身，函数edit()事实上是在对象的一个副本上进行操作的，如果不将其赋值到一个目标，所有修改会全部丢失。单击列的标题，可以用编辑器修改变量名和变量类型（数值型、字符型），可通过单击未使用列的标题来添加新的变量。mydata= edit(mydata)的更简洁的等价写法是fix(mydata)。\n\n\n从带分隔符的文本文件导入数据\n可以使用read.table()函数，此函数可读入一个表格格式的文件并将其保存为一个数据框，语法如下：\ndata= read.table(file,header = TRUE,sep= \"delimiter\",row.names='name')\n其中，file是一个带分隔符的ASCII文本文件，header表示文件是否在首行包含变量名，sep指定分隔行内数据的分隔符，row.names指定一个或多个行标记符（指定某变量为行名，该列即不再有标签，导致数据会少一列）。\n默认情况下，字符型变量将转换为因子。设置选项stringsAsFactors=FALSE，将停止对所有字符型变量的转换，或者使用选项colClasses指定每一列的类，如logical（逻辑型）、numeric（数值型）、characer（字符型）、factor（因子）。\n\n\n导入Excel数据\n读取一个Excel文件可以在Excel中将其导出为一个逗号分隔文件csv，再使用前文描述的方式将其导入R中。或者可以使用xlsx包，函数read.xlsx()导入一个工作表到一个数据框中：\nread.xlsx(file, sheetIndex, sheetName, rowIndex)\nread.xlsx()允许指定工作表中特定的行（rowIndex）和列（colIndex），配合上对应每一列的类（colClasses）。对于大型的工作簿，可以使用函数read.xlsx2()，该函数用Java来运行更加多的处理过程，可获得可观的质量提升。"
  },
  {
    "objectID": "blog/2020/01/30/index.html",
    "href": "blog/2020/01/30/index.html",
    "title": "R语言基础–函数",
    "section": "",
    "text": "前言\nR中作为数据处理基石的函数，可分为数值（数学、统计、概率）函数和字符处理函数。\n\n\n数值函数\n对数据做变换是数学函数的一个主要用途。数学函数也被用作公式中的一部分，用于绘图函数和在输出结果之前对数值做格式化。\n统计函数我们在进行统计学方法分析时一定会用到。\n概率函数通常用来生成特征已知的模拟数据，以及在用户编写的统计函数中计算概率值。\n\n\n字符处理函数\n数学和统计函数是用来处理数值型数据的，而字符处理函数可以从文本型数据中抽取信息，或者为打印输出和生成报告重设文本的格式。\n这里我们不举例子，大家可以下载下图所示的一份文档，可以打印出来，随时查阅。\n\n\n\ncheat sheet"
  },
  {
    "objectID": "blog/2020/01/28/index.html",
    "href": "blog/2020/01/28/index.html",
    "title": "R语言基础–因子",
    "section": "",
    "text": "因子\n因子在R中非常重要，因为它决定了数据的分析方式以及如何进行结果展示。因子也在R中具有许多强大运算的基础，包括许多针对表格数据的运算。因子的设计思想来源于统计学中的名义变量或分类变量，这些变量本质上不是数字，而是对应分类。例如血型，尽管可以用数字对它们进行编码。\n变量可分为名义型(无序分类变量)、有序型(表示顺序而非数量关系)和连续型变量。连续型变量可以呈现某个范围内的任意值，同时表示顺序和数量，如年龄是一个连续型变量。R中名义型变量和有序型变量称为因子。\n函数factor()以一个整数向量的形式存储类别值，同时一个由字符串（原始值）组成的内部向量将映射到这些整数上，如：\n\n\n\nfactor\n\n\n将此向量存储为(1, 2, 1, 1)，并在内部将其关联为1=Type1和2=Type2（具体赋值根据字母顺序决定）。针对向量diabetes进行的任何分析都会将其视为名义型变量并自动选择合适的统计方法。\n在R中，因子可以简单地看作一个附加更多信息的向量（尽管它们内部机理是不同的）。这额外的信息包括向量中不同值的记录，我们称之为“水平”。\n要表示有序型变量，需要为函数factor()指定参数ordered=TRUE，如：\n\n\n\nordered-factor\n\n\n此时顺序为’Excellent’‘Improved’‘Poor’（对于字符型向量，因子的水平默认依字母顺序创建），这里恰好与逻辑顺序一致。若不一致，可以通过指定levels选项覆盖默认排序：\n\n\n\nordered-factor-levels"
  },
  {
    "objectID": "blog/2020/01/22/index.html",
    "href": "blog/2020/01/22/index.html",
    "title": "R语言编程入门",
    "section": "",
    "text": "前言\n类似其他计算机高级语言，R用户只需要熟悉其命令、语句及简单的语法规则，就可以做数据管理和分析处理工作。R把大部分常用的复杂数据计算的算法作为标准函数调用，用户仅需要指出函数名及其必要的参数即可，这一特点使得R编程十分简单。\nR是面向对象的、区分大小写的解释型数组编程语言，输入后可直接给出结果。R中功能靠函数实现。R的函数分为“高级”和“低级”函数，高级函数可调用低级函数，这里的高级函数习惯上称为泛型函数。plot()就是泛型函数，可以根据数据的类型调用底层的函数，应用相应的方法绘制相应的图形。这就是面向对象编程的思想。\n\n\n数据集\n创建含有研究信息的数据集，这是任何数据分析的第一步。数据集通常是由数据构成的一个矩形数组，行表示观测，列表是变量。\nR可以处理的数据类型包括数值型（如100）、字符型（如“流光相约”）、逻辑型（TRUE/FALSE）、复数型（如2+3i）和因子型（表示不同类别）。R中有许多用于存储数据的结构，包括标量、向量、矩阵、数组、数据框和列表。多样化数据结构赋予了R极其灵活的数据处理能力。\n\n\n标量与向量\n标量可以看成是只含一个元素的向量，用于保存常量，如f=3、g=’US’和h=TRUE。\n向量是一系列元素的组合，用于存储数值型、字符型或逻辑型数据的一维数组。执行组合功能的函数c()可用来创建向量：\n\n\n\nvector\n\n\n提示：单个向量中的数据必须是相同的类型或模式（数值型、字符型或逻辑型），同一向量中不可混杂不同类型数据。\n通过在方括号中给定元素所处位置的数值访问向量中的元素，如a[c(2, 4)]用于访问向量a中第2个和第4个元素。\n\n\n矩阵\n矩阵是一个二维数组，和向量类似，其中元素必须类型相同，即一个矩阵中只能包含一种数据类型（数值型、字符型或逻辑型），可通过函数matrix()创建矩阵：\nmymatrix= matrix(vector,nrow = ,ncol = ,byrow = ,dimnames = list())\n其中，vector包含了矩阵的元素，nrow和ncol用于制定行和列的维数，dimnames包含了可选的、以字符型向量表示的行名和列名，byrow表明矩阵应当按行填充（byrow=T）还是按列填充（byrow=F），默认按列填充。\n\n\n\nmatrix\n\n\n我们可以使用下标和方括号来选择矩阵中的行、列或元素。X[i,]表示矩阵X中的第i行，X[,j]表示第j列，X[i, j]表示第i行第j个元素。\n\n\n数组\n矩阵都是二维的，仅能包含一种数据类型，当维度超过2时，需要使用数组，可通过函数array()创建：\nmyarray= array(vector,dimensions,dimnames)\n其中，vector包含了数组中的数据，dimensions是一个数值型向量，给出了各个维度下标的最大值，dimnames是可选的、各维度名称标签的列表。\n\n\n\narray\n\n\n数组是矩阵的一个自然推广，从数组中选取元素的方式与矩阵相同。\n\n\n数据框\n与通常在SAS、SPSS和STATA中看到的数据集类似，不同的列可以包含不同类型的数据。数据框是R中最常处理的数据结构，可使用函数data.frame()创建：\nmydata=data.frame(col1,col2,col3,...)\n其中，列向量col1，col2，col3等可谓任何类型（如字符型、数值型或逻辑型）。\n选取数据框中元素的方式比较多，既可以使用下标记号，也可以直接指定列名，如patientdata\\(age，其中\\)被用来选取一个给定数据框中的某个特定变量。\n在每个变量名前都输入一次数据框名可能十分麻烦，我们可以联合使用函数attach()和detach()，或单独使用函数with()来简化代码。函数attach()将数据框添加到R的搜索路径中，R在遇到一个变量名后，将检查搜索路径中的数据框，以定位到这个变量，如\n\n\n\ndataframe\n\n\n函数detach()将数据框从搜索路径中移除，不会影响数据框本身。注意：函数attach()和detach()最好是分析一个单独的数据框，并且不太可能有多个同名对象。当同名时，原始对象将进行优先运算。另一种方式是使用函数with()，如\n\n\n\nwith function\n\n\n花括号{}之间的语句都针对该数据框执行，无需担心名称冲突；若仅有一条语句，花括号可以省略。函数with()的局限性在于赋值仅在此函数的括号内有效。若需要创建在with()结构外依然存在的对象，使用特殊赋值符号 -&gt;&gt; ，即可保存对象到全局环境中。\n\n\n列表\n列表是一些对象（成分）的有序集合，允许整合若干对象到单个对象名下，其中的对象可以是任何数据结构，如某个列表可以是若干向量、矩阵、数据框甚至其他列表的组合，使用函数list()创建列表：\nmylist=list(object1,object2,...)\n可以通过在双重方括号中指明代表某个成分的数字或名称来访问列表中的元素，如mylist[[4]]。由于列表允许以一种简单的方式组织和重新调用可能不相干的信息，且许多R函数的运行结果都是以列表的形式返回的，由分析人员决定需要取出其中哪些成分，列表是R中的重要数据结构。"
  },
  {
    "objectID": "blog/2020/01/20/index.html",
    "href": "blog/2020/01/20/index.html",
    "title": "计算机概论3",
    "section": "",
    "text": "内存\n前面提到CPU所使用的数据都是来自内存（Main Memory），不论是软件程序还是文件数据，都必须要读入内存后CPU才能利用。个人电脑的内存主要组件为动态随机存取内存（Dynamic Random Access Memory，DRAM），随机存取内存只有在通电时才能记录与使用，断电之后数据就消失，因此我们也称这种RAM为挥发性内存。\nDRAM根据技术的更新又分好几代，而使用上较广泛的有所谓的SDRAM与DDR SDRAM两种。新一代的PC大多使用DDR内存。\n在某种意义上，内存的容量有时比CPU的速度还要重要。如果内存容量不够大的话将会导致某些大容量数据无法被完整地加载，此时已存在内存当中但暂时没有被使用到的数据就必须要先被释放，使得可用内存容量大于该数据，那份新数据才能够被加载。所以。通常越大的内存代表越快速的系统，这是因为系统不用常常释放一些内存中的数据。\n\n\nCPU的二级高速缓存\n除了内存外，个人电脑中还有许多类似内存的存储结构存在，最为我们所熟知的还有CPU内的二级高速缓存。我们现在知道CPU的数据都由内存提供，但CPU到内存之间还是得要通过内存控制器。如果某些很常用的程序或数据可以放置到CPU内存的话，那么CPU数据的读取就不需要跑到内存重新读取，这对于性能来说是一个很大的提升，这就是二级缓存的设计理念。新一代的CPU都有内置容量不等的L2缓存在CPU内部，以加快CPU的运行性能。\n\n\n只读存储器\n还记得你的电脑在开机的时候可以按下[Del]按键来进入一个名为BIOS（Basic Input Output System）的界面吧？BIOS是一个程序，这个程序是写死到主板上面的一个存储芯片中的，这个存储芯片在没有通电时也能够记录数据，这就是只读存储器（Read Only Memory，ROM）。ROM是一种非易失性的存储。\nBIOS对于个人电脑来说是非常重要的，它是系统在启动的时候首先会去读取的一个小程序。另外，固件（firmware）很多也是使用ROM来进行软件的写入（固件：固定在硬件上面的控制软件）。BIOS就是个固件，控制着启动时各项硬件参数的获取与启动设备的选择。"
  },
  {
    "objectID": "blog/2020/01/18/index.html",
    "href": "blog/2020/01/18/index.html",
    "title": "Python正则表达式与模式匹配",
    "section": "",
    "text": "很多商业分析都依赖模式匹配，也称为正则表达式（regular expression）。举例来说，我们可能需要分析一下来自深圳的所有订单。此时，你需要识别的模式就是“深圳”这个词。同样，你可能需要分析来自某个供应商的商品质量，此时你要识别的模式就是供应商的名字。\nPython包含了re模块，它提供了在文本中搜索特定模式/正则表达式的强大功能。要在脚本中使用re模块提供的功能，我们需要在脚本上方加入代码import re。\n导入re模块后，可以使用一大波函数和元字符来创建和搜索任意复杂的模式。元字符（metacharacter）是正则表达式中具有特殊意义的字符，使正则表达式能够匹配特定的字符串。\n常用的元字符包括 |、()、[]、.、*、+、?、^、$和(?P&lt;name&gt;)。如果你在正则表达式中见到这些字符，要知道程序不是要搜索这些字符本身，而是要搜索它们要描述的东西。\nre模块包含了很多有用的函数，用于创建和搜索特定的模式。一起来看一个示例代码：\n\n\n\nsample code\n\n\n第一行赋值字符串变量string，下一行将字符串拆分为列表，列表中的每个元素都是一个单词。使用re.compile和re.I函数以及用r表示的原始字符串，创建一个名为pattern的正则表达式。re.compile函数将文本形式的模式编译成为正则表达式（正则表达式不是必须编译的，但编译是个好习惯，因为这样可以显著地提高程序运行速度），re.I函数确保模式是不区分大小写的，即能同时在字符串中匹配“The”和“the”，原始字符串标志r可确保Python不处理字符串中的转义字符（如、），这样在进行模式匹配是，字符串中的转义字符和正则表达式中的元字符就不会有意外的冲突。利用for循环在列表变量sring_list的各个元素之间进行迭代，取出列表中所有的单词，使用re.search函数将每个单词与正则表达式进行比较，如果相匹配，那么count的值就加1。print语句打印出正则表达式在字符串汇总找到模式“The”（不区分大小写）的次数。\n再看另一个示例：\n\n\n\nsample code 2\n\n\n这个示例想要打印出相匹配的字符串，而不是相匹配的次数，这里使用到了(?P&lt;name&gt;)元字符和group函数。(?P&lt;name&gt;)元字符使匹配的字符串可以在后面的程序中通过组名符号name来引用，这里称为match_word。后面if语句中使用了group函数获取分段截获的字符串，如果相匹配，那么就在search函数返回的数据结构中找出match_word组合中的值，并打印出来。\n最后一个示例：\n\n\n\nsample code 3\n\n\n我们演示了使用re.sub函数在文本中用一种模式替换另一种模式。将正则表达式赋给变量string_to_find不是必需的，但若正则表达式特别长或复杂的话，将它赋给一个变量，然后传入re.compile函数有助于理解。最后使用re.sub函数以不区分大小写的方式在变量string中寻找模式，将发现的每个模式替换成a。"
  },
  {
    "objectID": "blog/2020/01/16/index.html",
    "href": "blog/2020/01/16/index.html",
    "title": "R语言的初体验",
    "section": "",
    "text": "R语言是从起源于贝尔实验室的S统计绘图语言演变而来的。与S语言类似，R也是一种为统计计算和绘图而生的语言和环境，它是一套开源的数据分析解决方案，由一个庞大且活跃的全球性研究型社区维护。\n\nR的特点总结\n\n软件本身及程序包的源代码公开；\n涵盖了多种行业数据分析中几乎所有的方法；\n任意一个分析步骤的结果均可被轻松保存、操作，并作为进一步分析的输入；\nR拥有顶尖水准的制图功能；\nR可运行于多种平台上，包括Windows、UNIX和Mac OS X；\n可轻松地从各种类型的数据源读写数据，包括文本文件、数据库管理系统、统计软件，乃至专门的数据仓库；\n每个函数都有统一格式的帮助和运行实例。\n\n\n\nR的帮助系统\nR提供了大量的帮助功能，学会如何使用这些帮助文档有助于编程。R的内置帮助系统提供了当前已安装包中所有函数的细节、参考文献以及使用示例。帮助文档可以通过以下函数进行查看。\n\nhelp.start()：打开帮助文档首页\nhelp(foo)或?foo：查看函数foo的描述说明等帮助信息(如返回值)\nhelp.search(‘foo’)或??foo：以foo为关键词搜索本地帮助文档\nRSiteSearch(‘foo’)：以foo为关键词搜索在线文档和邮件列表存档\napropos(‘foo’, mode=‘function’)：列出名称中含有foo的所有可用函数，在只知道函数的部分名称时搜索可用\nexample(foo)：查看函数foo的使用范例\ndata() 列出当前已加载包中所含的所有可用示例数据集\nvignette() 列出当前已安装包中所有可用的vignette文档\nvignette(‘foo’) 为主题foo显示指定的vignette文档\n\n\n\n工作空间和目录\n工作空间（workspace）是当前R的工作环境，存储着所有你定义的对象（向量、矩阵、函数、数据框和列表）。在一个R会话结束时，你可以将当前工作空间保存到一个镜像中，以便在下次启动R时自动载入它。当前的工作目录（working directory）是R用来读取文件和保存结果的默认目录。\n用于管理工作空间和目录的部分标准命令如下：\n\ngetwd()：查看当前工作目录\nsetwd()：重新设定当前工作目录。如果需要读入一个不在当前工作目录下的文件，需要在调用语句中写明文件的完整路径。setwd()命令的路径中使用正斜杠/。R将反斜杠。即使在Windows平台上运行R，在路径中也要使用正斜杠。\nls()：列出当前工作空间中的对象\nrm(objectlist)：删除一个或多个对象\noptions()：显示或设置当前选项\nhistory(#)：显示最近使用的#个命令(默认值为25)\nsavehistory(‘myfile’) 保存命令历史到文件myfile.Rhistory中\nloadhistory(‘myfile’) 载入命令历史文件myfile.Rhistory\nsave.image(‘myfile’) 保存工作空间到文件myfile.RData中\nload(‘myfile’) 读取工作空间myfile.RData到当前会话中\nsave(objectlist, file=‘myfile’) 保存指定对象到一个文件中\nq()：结束对话退出R，并询问是否保存工作空间\n\n\n\nR包\nR提供了大量备用功能，通过可选模块的下载和安装来实现。目前有15364个称为包的用户贡献模块可从https://cran.r-project.org/web/packages下载。这些包提供了横跨各种领域、数量庞大的功能，包括分析地理数据、处理蛋白质质谱，甚至是心理测验分析的功能。\nR包是R函数、数据、预编译代码以一种定义完善的格式组成的集合，具有详细的说明和示例。计算机上存储包的目录称为库（library）。.libPaths()显示库所在位置，library()则可以显示库中包。\n第一次安装一个包，使用命令install.packages()即可，在括号中输入要安装的包名称，一个包仅需安装一次。update.packages()更新已安装的包。installed.packages()列出已安装的包的相关信息(如版本号、依赖关系等)。Windows下的R包是经过编译的zip文件，安装时不要解压缩。安装路径为“Pacakges&gt;install packages from local files”，选择本地磁盘上存储zip包的文件夹。\n包的安装是指从某个CRAN镜像站点下载它并将其放入库中的过程。安装好以后，必须被载入到会话中才能使用包，需要使用library()函数载入该包。在一次应用中，包只需载入一次，如果需要，我们可以自定义启动环境以自动载入会频繁使用的包。search()显示已加载并可使用的包。help(‘package_name’)输出某个包的简短描述以及包中可用的函数名称和数据集名称的列表，help()查看包中任意函数或数据集的描述，R的帮助系统包含了每个函数的一个描述（同时带有示例），每个数据集的信息也被包括其中。\n\n\nR的使用\nR是面向对象的，区分大小写的解释型数组编程语言。R中多数功能是由程序内置函数、用户自编函数和对对象的创建和操作所实现的。一次交互式会话期间的所有数据对象都被保存在内存中。R语句由函数和赋值构成，R使用 -&gt; 而非 = 作为赋值符号。R也允许使用 = 为对象赋值，但是它不是标准语法，某些情况下会出现问题。R具有完备的数据存取、管理、分析和显示等功能，将数据处理和统计分析融为一体。以后我们继续学习R语言。"
  },
  {
    "objectID": "blog/2020/01/10/index.html",
    "href": "blog/2020/01/10/index.html",
    "title": "Python基础要素之字符串",
    "section": "",
    "text": "字符串是Python中的另一种基本数据类型。它通常是指人类可以阅读的文本，但更广泛地说，它是一个字符序列，并且字符只有在组成这个序列时才有意义。一些对象看上去是数值，但实际上是字符串，比如邮政编码，你对邮政编码做加减乘除是没有意义的，所以最好在代码中将其作为字符串来处理。接下来介绍用于字符串管理的一些模块、函数和操作。\n\n字符串\n字符串可以包含在单引号、双引号、3个单引号或3个双引号之间：\n\n\n\n字符串\n\n\nOutput #14展示了一个包含在单引号之间的简单字符串。\n\n\n+、*、len()\n\n\n\noperator\n\n\nOutput #18展示了使用+操作符将两个字符串相加。+操作符按照原样相加，如果你想在结果字符串中留出空格的话，就必须在原字符串中加上空格（Output #18在字母a后加了空格），Output #19中的*操作符也是这样，其将字符串重复一定的次数。Output #20展示了使用内置函数len来确定字符串中字符的数量。len函数将空格与标点符号都计入字符串长度，所以结果是23个字符。\n\n\nsplit函数\n使用split函数将一个字符串拆分成一个子字符串列表（列表中的子字符串正好可以构成原字符串）。split函数可以在括号中使用两个附加参数，第一个附加参数表示使用哪个字符进行拆分，第二个参数表示进行拆分的次数。\n\n\n\nsplit\n\n\n在Output #21中，括号中没有附加参数，所以split函数使用空格字符（默认值）对字符串进行拆分。因为这个字符串有5个空格，所以被拆分成具有6个子字符串的列表。Output #22中，第一个附加参数是用空格来拆分字符串，第二个附加参数是2，说明只想用前两个空格进行拆分，生成一个带有3个元素的列表。第二个参数会在我们解析数据的时候派上用场。举例来说，你可能会解析一个日志文件，文件中包含时间戳、错误代码和由空格分隔的错误信息。在这种情况下，可以使用前两个空格进行拆分，解析出时间戳和错误代码，但是不使用剩下的空格进行拆分，以便完整地保留错误信息。Output #23和Output #24中，括号中的附加参数是都好，split函数在出现逗号的位置拆分字符串。\n\n\njoin函数\n使用join函数将列表中的子字符串组合成一个字符串。join函数将一个参数放在join前面，表示使用这个字符（或字符串）在子字符串之间进行组合。print(\"Output #25: {0}\".format(','.join(string2_list)))这里join函数将子字符串组合成一个字符串，子字符串之间为逗号。因为列表中有6个子字符串，所以组合后有5个逗号。\n\n\nstrip函数\n使用strip、lstrip和rstrip函数从字符串两端删除不想要的字符，这三个函数都可以在括号中使用一个附加参数来设定要从字符串两端删除的字符（或字符串）。\n\n\n\nstrip\n\n\n可以看到，string3的左侧有几个空格，右侧包含制表符（、几个空格和换行符（）。在Output #26中，你会看到句子前面有空白，句子下面有一个空行，句子后面有你看不到的制表符和空格。{0:s}中的s表示传入的值应该格式化为一个字符串。下面展示了从字符串两端删除其他字符的方法，将要删除的字符作为strip函数的附加参数即可：通过将美元符号、下划线、短划线和加号作为附加参数，通知程序从字符串两端删除它们。\n\n\nreplace函数\n使用replace函数将字符串中的一个或一组字符替换为另一个或另一组字符。replace函数在括号中使用两个附加参数，第一个参数作用是在字符串中查找要替换的字符或一组字符，第二个参数是要用来替换的一个或一组字符。\n\n\n\nreplace\n\n\nOutput #32展示了使用replace函数将字符串中的空格替换为!@!。Output #33展示了使用逗号替换字符串中的空格。\n\n\nlower、upper、capitalize函数\nlower和upper函数分别用来将字符串中的字母转换为小写和大写，capitalize函数对字符串中的第一个字母应用upper函数，对其余字母应用lower函数：\n\n\n\nlower-upper-capitalize\n\n\nOutput #36对句子的首字母大写。Output #37将capitalize函数放在一个for循环中，对string8_list这个列表中的每个元素首字母大写，其余字母小写。"
  },
  {
    "objectID": "blog/2020/01/08/index.html",
    "href": "blog/2020/01/08/index.html",
    "title": "如何用Python自编k-近邻算法？",
    "section": "",
    "text": "k-近邻算法概述\n简单地说，kNN依据不同特征值之间的距离进行分类。它不具有显式的学习过程，实际上是利用训练数据集对特征向量空间进行划分，并作为其分类的模型，即我们知道训练集中每一数据与所属分类的对应关系，输入没有标签的新数据后，将新数据的每个特征与训练集中数据对应的特征进行比较，然后算法提取训练集中特征最为相似数据的分类标签，选择k个最相似数据中出现次数最多的类别作为新数据的分类。\n\n\n自编kNN函数\n\n\n\nkNN\n\n\nclassify()函数有4个输入参数：待分类的输入向量inX，训练集dataSet，训练集标签向量labels，参数k为选择最近数据点个数，其中，inX维度为1xN，dataSet维度为MxN，labels维度为1xM，k为奇数。\ndataSet.shape以元组形式返回训练集维度(M, N)，dataSetsize为训练集的样本个数M。这里距离度量采用欧式距离，因而tile函数将输入数据重复M行1列（从而与训练集维度相同），分别和训练集中的每个数据点对应特征相减再平方，再按行相加，不保持其二维特性，即得输入数据与训练集中每个数据点之间的欧式距离。\n计算完距离之后，argsort函数对距离按照从小到大的次序排列，并返回排序后对应的原始索引值。使用for循环确定前k个距离最小元素所属的类别voteIlabel，使用get函数按照字典classCount键值取得相应的字典值，如果字典中存在这个键，get函数就返回对应的字典值，如果不存在，则返回0，用这种方式计数k个数据中每个标签出现的次数。因而字典classCount中键值为标签，字典值为k个标签对应的个数。\n使用sorted函数对字典classCount进行排序：items函数同时引用字典的键和值，结果是一个列表，其中包含的是键-值对形式的元组。由于字典没有隐含排序，我们可以按照字典的键或字典值来排序，这里的key就是排序的规则，关键字函数设置用于排序的关键字。使用operator模块中的itemgetter函数对列表按照每个元组第二个索引位置（即字典值，标签个数）进行排序，reverse=True对应降序。所以最后返回sortedClassCount列表中第一个元组的第一个值，即在k个标签中出现次数最多的标签，这样即完成了一个简单的kNN算法。\n\n\n创建训练集\n\n\n\n训练集\n\n\n我们创建了一个简单的训练集，有4组数据，每组数据有两个我们已知的属性/特征值。向量labels包含了每个数据的标签信息，labels包含的元素个数等于group矩阵行数。\n\n\n运行kNN\n我们需要在脚本中导入两个模块NumPy和运算符operator（kNN执行排序操作时使用到）：\n\n\n\nimport modules\n\n\n保存脚本文件，改变当前路径到存储脚本文件位置，进入Python：\n\n\n\nrun kNN\n\n\n输出结果应该是B，也可以改变输入数据运行。这样，我们已经构造了一个简单的kNN分类器。"
  },
  {
    "objectID": "blog/2020/01/05/index.html",
    "href": "blog/2020/01/05/index.html",
    "title": "可惜没如果——因果关系的三个层级",
    "section": "",
    "text": "前言\n我们平时在统计中最常接触到的就是相关关系，而在因果推断中，实际上相关关系是处在因果关系三个层级的最低层级。正如我们所熟知的，相关不能说明因果，我们从观察到的数据中得到的相关对于因果的解释并不能起到直接的作用。\n\n\n因果关系的三个层级\n\n第一层级：关联\n我们一般通过观察寻找规律。如果观察到某一事件改变了观察到另一事件的可能性，我们便说这一事件与另一事件相关联。更进一步地，我们基于被动观察做出预测。\n典型的数据预测问题就是：“如果我观察到···会怎样？”例如，一家商店可能会问你：“购买牙膏的顾客同时购买牙线的可能性有多大？”这类问题是统计学的安身立命之本，统计学家主要通过收集和分析数据给出答案。\n我们可以这样解答：首先采集所有顾客购物行为的数据，筛选出购买牙膏的顾客，计算出当中购买牙线的人数比例。这个比例也就是我们所说的条件概率，用来反映(针对大数据)买牙膏和买牙线两种行为之间的关联程度，即P(牙线|牙膏)。典型常用的关联度量方法即相关分析或回归分析，具体操作就是将一条直线拟合到数据点集中，再去确定这条直线的斜率。\n我们在学习统计的时候，几乎所有老师都会和你说：“相关不代表因果。”统计学本身不能告诉我们，牙膏或牙线哪个是因，哪个是果。但是从商店的角度看，因果这件事并不重要——好的预测无需好的解释。\n当前的机器学习程序(包括应用深度神经网络的程序)几乎仍然完全是在关联模式下运行的。它们由一系列观察结果驱动，致力于拟合出一个函数，就像我们试图用点集拟合出一条直线一样。深度神经网络为拟合函数的复杂性增加了更多的层次，但其拟合过程仍然由原始数据驱动。如果无人驾驶汽车的程序设计者想让汽车在新情况下做出不同的反应，他就必须明确地在程序中添加这些新反应的描述代码，否则机器没有应对新情况的灵活性和适应性。\n\n\n第二层级：干预\n在第一层级中，我们基于被动观察发现规律，做出预测。而当我们开始寻求主动对环境做出改变时，我们就迈上了因果关系的第二层级。这一层级的一个典型问题是：“如果我们把牙膏的价格翻倍，牙线的销售额将会怎样？”问出这个问题的时候，我们实际上已经脱离了收集到的数据本身，而要对数据的环境做出干预。我们把这样的问题记作P(牙线|do(牙膏))，即“如果我们实施…行动，将会怎样”。\n毫无疑问，干预比关联更高级，因为其不仅涉及被动观察，还涉及主动改变现状。无论你的数据集有多大、神经网络有多深，只要你使用的是被动收集的数据，就无法回答有关干预的问题。从统计学中学到的任何方法都不足以让我们明确表述类似“如果价格翻倍将会发生什么”这样的问题，更谈不上回答了。\n为什么我们无法仅仅通过观察来回答牙线的问题呢？为什么不直接进入存有历史购买信息的数据库中，看看在牙膏价格翻倍的情况下对应的牙线的销售情况呢？原因在于，在历史销售信息中，牙膏涨价可能是出于完全不同的原因，如产品供不应求，其他商店也不得不涨价等。而我们只想刻意干预牙膏价格，这一结果就可能与历史上顾客在别处买不到便宜牙膏时的购买行为大相径庭。简单地说，我们只想知道单纯的牙膏涨价这个因所对应的牙线的果，而历史数据中各种影响因素完全超出了我们所提出问题的范畴，因而无法仅仅利用观察历史数据来回答干预的问题。\n因果推断则可以帮助我们解决这一问题。\n我们知道预测干预结果的一种非常直接的方法是：在严格控制的条件下进行实验。更加有趣的是，一个足够强大准确的因果模型可以在不进行实验的前提下，利用第一层级（关联）的数据来回答第二层级（干预）的问题。\n日常生活中，我们一直都在实施干预，尽管我们不会这么一本正经地称呼它。当我们服用阿司匹林试图治疗头痛时，就是在干预一个变量（人体内阿司匹林的量），以影响另一个变量（头痛的状态）。如果我们关于阿司匹林治愈头痛的因果知识是正确的，那么我们的结果变量的值将会从“头痛”变为“不头痛”。\n\n\n第三层级：反事实\n但是到此仍然不能回答所有我们感兴趣的因果问题：现在已经不头痛了，是因为我吃的阿司匹林么？是因为我吃的食物么？是因为我心情变好了么？\n正是这些问题将我们带入到因果关系的第三层级：反事实。要回答以上问题，我们就必须回到过去改变历史，“假如我们没有服用过阿司匹林，会怎样？”世界上没有哪个实验可以撤销对一个已接受过治疗的人所进行的治疗，进而比较治疗与未治疗两种条件下的结果。\n数据就是事实，而在反事实世界里，观察到的事实被完全否定。回到牙膏的例子，第三层级的问题是：“假如我们把牙膏价格翻倍，之前买了牙膏的顾客仍然选择购买的概率是多少？”在这个问题中，我们所做的就是将真实的世界（我们知道顾客以当前的价格购买了牙膏）和虚构的世界（牙膏价格翻倍）进行对比。\n“倘若那天，把该说的话好好说，该体谅的不执着，你会怎么做？”对这类问题的回答让我们得以从历史和他人的经验中获取经验教训。从想象的反事实中，我们获得了灵活性、反省能力以及改善行为的能力。\n因果关系第三层级的典型问题就是：”假如我当时做了…会怎样？“和“为什么？”两者都涉及观察到的世界与反事实世界的比较。仅靠干预实验无法回答这样的问题。"
  },
  {
    "objectID": "blog/2020/01/02/index.html",
    "href": "blog/2020/01/02/index.html",
    "title": "因果推断-2",
    "section": "",
    "text": "统计值的不确定性意味着什么？\n统计推断利用统计方法生成一个问题答案的实际估计值，并给出对该估计值的不确定性大小的统计估计。这种不确定性反映了样本数据集的代表性以及可能存在的测量误差或数据缺失。数据永远是从理论上无限的总体中抽取的有限样本。我们无法避免根据样本测量的概率无法代表整个总体的相应概率的可能性。统计学提供了很多方法来应对这种不确定性，包括极大似然估计、倾向评分、置信区间、显著性检验等。\n\n\n相关与独立\n以因果模型的路径图(因果图)来表示的变量之间的听从模式通常会导向数据中某种显而易见的模式或者相关关系。“A和B之间没有连接路径”翻译成统计语言，就是“A和B相互独立”，即发现A的存在不会改变B发生的可能性。\n\n\n想象力与规划\n历史学家尤瓦尔·赫拉利在他的《人类简史》一书中指出，人类祖先想象不存在之物的能力是一切的关键，正是这种能力让他们得以交流得更加顺畅。在获得这种能力之前，他们只相信自己的直系亲属或者本部落的人。而此后，信任就因共同的幻想(例如信仰无形但可想象的神，信仰来世，或者信仰领袖的神性)和期许而延伸到了更大的群体。我们的智人祖先新掌握的因果想象力使他们能够通过一种被我们称为“规划”的复杂过程更有效地完成许多事情，例如他们可以通过想象和比较几个狩猎策略的结果来完成一次狩猎活动。而要做到这一点，思维主体必须具备一个可供参考并且可以自主调整的关于狩猎现实的心理模型。心理模型是施展想象力的舞台，它使我们可以通过对模型局部的自主调整修改来试验重估不同情景的概率，人类的心理模型因而具有一种模块性，其涉及预测对环境进行刻意改变后的结果，并根据预测结果选择行为方案以催生出自己所期待的结果。"
  },
  {
    "objectID": "blog/2019/12/11/index.html",
    "href": "blog/2019/12/11/index.html",
    "title": "因果推断",
    "section": "",
    "text": "因果推断\n禁止言论就意味着禁止了思想，同时也扼杀了与此相关的原则、方法和工具。\n\n\n干预\ndo算子表明正在进行主动干预而非被动观察，这一概念在经典统计学中没有涉及。临床试验中使用do算子来确保观察到的病人存活期的变化能完全归因于药物本身，而没有混杂其他影响寿命长短的因素。如果不进行干预而让病人自行决定是否服用该药物，那么其他因素就可能会影响病人的决定，而服药和未服药的两组病人的存活期的差异也就无法再被仅仅归因于药物。例如，假设只有重症期的病人服用了这种药，那么两组之间的比较结果实际上反映的是其病情的严重程度，而非药物的影响。在数学上，我们把自行服药病人的生存期的观测概率称为条件概率，这里的概率是以观察到病人服用药物为条件的。【观察到】和【进行干预】是有本质的区别的。我们不认为气压计读数下降是风暴来临的原因，因为观察到气压计读数下降意味着风暴来临的概率增加，但人为使气压计读数下降并不能影响风暴来临的概率。因果推断要做的就是如何在不实际实施干预的情况下预测干预的效果。\n\n\n反事实\n假如某人在服用某种药物一个月后死亡，我们现在要关注的问题就是这种药物是否导致了他的死亡。为了回答这个问题，我们需要假设：如果他没服用这种药物，是否会避免死亡？反事实推理输出有关反事实世界的答案。\n语言会塑造思想。你无法回答一个你提不出来的问题；你也无法提出一个你的语言所不能描述的问题。"
  },
  {
    "objectID": "blog/2019/11/28/index.html",
    "href": "blog/2019/11/28/index.html",
    "title": "统计学是干嘛的？",
    "section": "",
    "text": "统计学之所以存在，关键的原因只有一个，那就是变异及由此产生的抽样误差。没有变异，没有抽样误差，就没有统计学存在的理由。当我们把多个随机结果放在一起的时候，却能发现一定的规律性。正是因为这种规律的存在，所以我们仍然可以在变异中寻找规律，这也正是统计学的主要目的：从各种看似杂乱的现象中找出潜在的规律。\n\n抽样调查\n既然是规律，那就一定要在大多数人中存在，只在一小部分人中存在的现象不是规律，而是偶然，因为更多的是大多数人没有存在该现象，这才是规律。要证明一种现象是不是真正的规律，需要在大量人群中进行验证。由于我们无法接触到理论意义上的总体，因而我们换一种思路，调查部分具有代表性的样本，然后用统计学方法将样本的结果推广到总体，这就是我们所说的抽样调查。\n\n\n统计推断与参数估计\n统计学通常利用样本数据来推断总体结果，就是我们所说的用样本统计量推断总体参数。总体参数是客观存在的，经典的频率主义学派认为，总体参数是一个客观存在且固定的数值，而贝叶斯学派认为连总体参数自身也是个随机变量，所以也需要我们去估计。样本随机，样本统计量也是随机的，用它来估计总体参数，估计结果会存在一定的误差。但科学合理的抽样调查，其推断的结果是可靠的。偏差的样本会导致偏差的结论。样本必须足够代表总体。当然还需要考虑其他因素，比如调查员的水平、总体人群的变化等影响因素。\n\n\n抽样误差\n然而，即使代表性非常好的样本，也是无法真正等同于总体的，总会存在一定的抽样误差。样本统计量之间的差异就反映了抽样误差。由于抽样误差的存在，如果用样本统计量直接估计总体参数，那么肯定会有一定的偏差。所以在估计总体参数时需要考虑到抽样误差带来的偏差，因而我们在点估计之外，用置信区间来估计总体参数。抽样误差带来的偏差是多大呢？在实际中，我们不可能通过多次抽样，计算每个样本间统计量的差异大小从而去估计偏差大小，我们只能通过一次样本计算。这种根据一次样本计算抽样误差的大小就是标准误（standard error）。标准误几乎在所有统计方法中都会出现，因为它可以提示结果的可靠性：如果标准误较小，则说明抽样误差小，这意味着样本很稳定，对总体的代表性很好，由此推论结果较为可靠；如果标准误较大，则说明抽样误差大，提示样本代表性不强，这种情况下一般需要加大样本量，否则结果不可靠。"
  },
  {
    "objectID": "blog/2020/01/01/index.html",
    "href": "blog/2020/01/01/index.html",
    "title": "鼠年加油",
    "section": "",
    "text": "回望一幕幕送别 何尝不轻描淡写 独自出姑苏城外 流光未曾相约 他日他乡重逢 风轻柔河流缓缓\n\n\n新年快乐，鼠年加油 没有珍惜的时间，2020开始去珍惜 没有完成的事情，2020开始去完成 从来不相信鸡汤，也不制造鸡汤 等2021年再回顾这一年 希望感受到的不再是很多尴尬的空白 每到新的一年，你的朋友圈是不是也有刷屏的感慨和祝福 一年初始，心愿是美好的 如果能坚持做下去，该是多么圆满 新年少偷点懒，多学习新东西 多看看书，多写点公众号😂 多做一些分享，和小伙伴们共同进步 希望每个你新年新气象，活成你想要的样子 感谢你的关注❤️"
  },
  {
    "objectID": "blog/2020/01/04/index.html",
    "href": "blog/2020/01/04/index.html",
    "title": "Python初体验",
    "section": "",
    "text": "目前Python这门语言有多火也不用多说，各种公众号推送制造的焦虑让你感觉实在不学不行，接下来我们就来体验一下别人口中的这门似乎很神奇的编程语言。\n\n创建Python脚本\nPython的安装这里就不说了，当然如果你是mac用户，恭喜你的笔记本自带了Python2（前几天官方已停止对2的更新了）。如何在Python shell中简单地运行代码呢？Windows用户打开命令行窗口，mac用户打开终端，输入“python3”，按下回车键，就能看见Python提示符（&gt;&gt;&gt;）：\n\n\n\nPython提示符\n\n\n面对复杂多代码的任务，我们需要把代码都写在Python脚本上，然后运行脚本 ，提高工作效率。我们可以选择一个自己喜欢的文本编辑器，可供选择的有很多：Spyder、Pycharm、Jupyter notebook、Visual Studio code等。打开编辑器，一般我们将 #!/usr/bin/env python3 作为第一行。以井号开头的代码行为注释行，Windows系统不读取也不执行该行代码，但是像macOS这样的基于Unix的系统会根据这一行来找到执行该脚本的Python版本，加入这一行可以使你的脚本在不同操作系统之间具有可移植性。我们将上面这俩行代码放到Pycharm中，保存为first-script.py文件，这就是一个简单的Python脚本了。\n\n\n运行Python脚本\n对于在编辑器内运行，编辑器会有一个绿色三角运行按钮，点击一下即可运行输出：\n\n\n\nPycharm运行按钮\n\n\n当然，我们也可以选择在命令行或者终端中运行脚本：打开命令行或者终端，提示符会是一个具体的文件夹，即目录，如mac：/Users/luzhen。我们将脚本保存在桌面上，同时在终端中切换到桌面目录：\n\n\n\n终端切换目录\n\n\nmac上下一步就是为脚本添加执行权限，输入命令：chmod +x first-script.py。chmod是一个Unix命令，表示改变访问权限（change access mode）。+x表示在访问设置中添加执行权限，而非读、写权限。这样Python就可以执行脚本了。mac上只要你在一个脚本上运行了chmod命令，以后就可以随意运行该脚本，无需第二次执行chmod命令。\n接下来就可以运行脚本了：\n\n\n\n终端运行脚本\n\n\n可以看到终端窗口已经完成了脚本输出的打印，脚本运行成功！当然，Windows上还有一种运行方法，直接输入\\(python3 first-script.py\\)，也可成功执行脚本，mac上同样适用。\n\n\n与命令行交互的几个小技巧\n\n使用向上箭头键得到以前的命令\n\n在命令行和终端窗口中，你可以通过按向上箭头键找到以前输入的命令，可以减少每次运行Python脚本时必需的输入量，特别是当Python脚本的文件名特别长或需要在命令行上提供额外的参数（比如输入文件名或输出文件名）的时候。\n\n用Ctrl+c停止脚本\n\n我们已经学会了运行脚本，那么如何提前中断和停止Python脚本呢？Windows是Ctrl+C，mac是Control+c，就可以停止通过命令开始的进程。（进程：计算机对一系列命令的处理过程。对于一个脚本或程序，计算机将它解释成一个进程，如果这个程序非常复杂，就解释成一系列进程，这些进程可以顺序执行，也可以并发执行。）\n\n读懂出错信息并找到解决方案\n\n当窗口显示了错误信息时，我们先读懂出错信息。某些情况下，出错信息中明确指出了代码中出现错误的行，我们可以集中精力解决这一行的错误（你的文本编辑器应该设置成显示行号，可以在网上搜索一下）。出错信息也是编程的一部分，学会编程也包括学会如何有效地调试程序错误。最好的做法是将整个错误信息（至少是信息的主要部分）复制到搜索引擎上，看看别人是如何调试这种错误的。\n这样以后，接下来我们就可以来了解认识Python的语言基础要素了。人生苦短，一起学习Python。"
  },
  {
    "objectID": "blog/2020/01/07/index.html",
    "href": "blog/2020/01/07/index.html",
    "title": "NumPy函数库基础",
    "section": "",
    "text": "机器学习算法涉及很多线性代数知识，我们会经常使用NumPy函数库，用线性代数简化不同的数据点上执行的相同数学运算。将数据表示为矩阵形式，只需要执行简单的矩阵运算而不需要复杂的循环操作。\n在Python shell开发环境中输入下面命令：from numpy import *，将NumPy函数库中的所有模块引入到当前的命名空间，输入以下命令：\n\n\n\nNumPy\n\n\n上述命令构造了一个4*4的随机数组（随机数组在不同计算机上输出可能不同）。\n在NumPy函数库中存在两种不同的数据类型（矩阵matrix和数组array），二者都可以用于处理行列表示的数字元素。虽然看起来很相似，但在这两个数据类型上执行相同的数学运算可能得到不同的结果，其中的矩阵matrix与MATLAB中matrices等价。\n调用mat()函数可将数组转换为矩阵：\n\n\n\nmat\n\n\n如何进行矩阵求逆呢？不记得或者没学过矩阵求逆也没关系，NumPy库.I操作符可以很方便地算出矩阵的逆运算：\n\n\n\n逆矩阵\n\n\n接着执行矩阵乘法，得到矩阵与其逆矩阵相乘的结果：\n\n\n\n矩阵乘法\n\n\n结果应该是单位矩阵，除了对角线元素是1，4*4矩阵的其他元素应该全是0。上面实际输出略有不同，矩阵里还留下了很多数值非常小的元素，这是计算机处理误差产生的结果，我们来看一下误差值：\n\n\n\n误差\n\n\n函数eye(4)创建4*4的单位矩阵。\n只要能够顺利完成上面的例子，你就已经正确地安装并初步使用了NumPy函数库。后面我们会对它有更深的了解。"
  },
  {
    "objectID": "blog/2020/01/09/index.html",
    "href": "blog/2020/01/09/index.html",
    "title": "笔记",
    "section": "",
    "text": "表示的重要性\n表示问题必须优先于获取问题。如果缺少问题的表示方法，我们也就不知道如何存储信息以供使用。人工智能对认知研究的一个主要贡献就是确立“表示第一，获取第二”的范式。\n通常，在寻求一个好的表示方法的过程中，关于如何获取知识的洞见就会自然产生，无论这种洞见是来自数据，还是来自程序员。\n人类的大脑肯定拥有某种简洁的信息表示方式，同时还拥有某种十分有效的程序用以正确解释每个问题，并从存储的信息表示中提取正确答案。我们需要给机器装备同样高效的表示信息和提取答案的算法，因果图和因果推断就派上了用场。\n\n\n打破规则\n计算机不能理解因果关系，我们必须教会它如何打破规则，让它理解“观察到某事件”和“使某事件发生”之间的区别。\n我们需要告诉计算机：“无论何时，如果你想使某事发生，那就删除因果图中指向该事的所有箭头，之后继续根据逻辑规则进行分析。”这样做的原因很简单：使某事发生就意味着将它从所有其他影响因子中解放出来，并使它受限于唯一的影响因子——能强制其发生的那个因子。\n计算机能够进行因果推理的前提是，计算机懂得有选择地打破逻辑规则。\n\n\n概率的重要性\n构建因果模型不仅仅是画箭头，箭头背后还隐藏着概率。当我们绘制一个从X指向Y的箭头时，我们是在暗指，某些概率规则或函数具体说明了“如果X发生改变，Y将如何变化”。在某些情况下我们可能知道这个规则具体是什么，但更多时候，我们不得不根据数据来对这个规则进行估计。因果革命最有趣的特点之一就是，在许多情况下，我们可以对这些完全不确定的数学细节置之不理。通常情况下，因果图自身的结构就足够让我们推测出各种因果关系和反事实关系：简单的或复杂的、确定的或概率的、线性的或非线性的。\n\n\n概率与因果关系\n概率能将我们对静态世界的信念进行编码，而因果论则告诉我们，当世界被改变时（无论改变是通过干预还是通过想象实现的），概率是否会发生改变以及如何改变。"
  },
  {
    "objectID": "blog/2020/01/11/index.html",
    "href": "blog/2020/01/11/index.html",
    "title": "kNN改进约会网站的配对效果",
    "section": "",
    "text": "前言\n前面我们已经初步了解了kNN——如何用Python自编k-近邻算法？今天我们试着进行一个实例上kNN的应用。\n海伦一直使用在线约会网站寻找自己心仪的约会对象。经过一番总结，她发现曾交往过三种类型的人：不喜欢的人、魅力一般的人和极具魅力的人。她发现自己无法直接将约会网站推荐的匹配对象归入恰当的上述类别之中，希望我们的分类软件能够更好地帮助她进行确切的分类。此外，她还收集了一些约会网站未曾记录的数据信息，提供给了我们。\n\n\n准备数据：从文本文件中解析数据\n海伦将准备的数据存放在文本文件datingTestSet2.txt中，每个样本数据占据一行，总共有1000行。每个样本主要收集了3种特征：每年飞行里程数、玩视频游戏所耗时间百分比和每周消费的冰淇淋公升数。如下：\n\n\n\ndatingTestSet2\n\n\n而在我们将上述特征数据输入到分类器之前，必须将待处理数据的格式转换为分类器可以接受的格式。我们之前在kNN.py中已经创建了kNN分类器函数，接下来我们创建用于处理输入数据格式的file2matrix函数，此函数的输入为文件名字符串，输出为训练样本矩阵和类标签向量，将文本记录转换为NumPy。\n\n\n\nfile2matrix\n\n\n首先我们以r模式（只读模式）打开要处理的文本文件，readlines函数读取整个文件所有行，保存在一个列表变量中，每行作为一个元素，我们计数文件的行数。然后创建以零填充的矩阵，我们将该矩阵的另一维度设置为固定值3。循环处理文件的每一行数据：使用strip函数截取掉所有的回车字符，使用tab字符（将上一步得到的整行数据分割成一个元素列表，选取前3个元素存储到特征矩阵中，使用索引值-1将文件的最后一列存储到向量classLabelVector中，这里，我们必须明确地通知解释器存储的的元素值为整型，否则Python会将这些元素当作字符串进行处理。\n使用函数file2matrix读取文件数据，必须确保文件存储在我们的工作目录中。重新加载kNN.py模块，以确保更新的内容可以生效，否则Python将继续使用上次加载的kNN模块。\n\n\n\nload data\n\n\n现在我们已经从文本文件导入了数据并将其格式化为想要的格式，接下来我们以图形化的方式直观地展示数据内容，以便辨识出一些数据模式。\n\n\n分析数据：使用Matplotlib创建散点图\n首先我们使用Matplotlib制作原始数据的散点图：\n\n\n\nscatter\n\n\n输出效果如下图，散点图使用特征矩阵的第二、三列数据，分别为玩视频游戏所耗时间百分比和每周消费的冰淇淋公升数。由于没有使用样本类别标签，我们很难看出有用的数据模式信息。\n\n\n\nplot\n\n\n为了更好地理解数据，我们以不同的方式来标记不同的样本类别。Matplotlib库提供的scatter函数支持个性化标记散点图上的点。重新输入上面的代码，调用scatter函数时使用下列参数，利用变量datingLabels存储的类别标签属性，在散点图上绘制色彩不等、尺寸不同的点：\n\n\n\nscatter function\n\n\n\n\n\ncolored plot\n\n\n我们基本上能够看到数据点所属三个类别的区域轮廓，但还不是十分明显，接下来我们使用特征矩阵的第一、二列属性作图：\n\n\n\nclusters\n\n\n此时我们可以看到图中清晰地标示了三个不同的样本类别区域，通过这两个特征更容易区分数据点。\n\n\n准备数据：归一化数值\n观察原始数据我们发现：每年飞行里程数的数量级远大于其余两个特征。在利用kNN计算样本之间的距离时，数值大的该特征会极大地影响最终的结果，也就是说，数量值大小会影响特征对结果影响的权重，而我们这里认为三个特征是同等重要的。\n因而在处理这种情况时，我们通常采用的方法是将数值归一化，如将取值范围处理为0到1或者-1到1之间。下面的公式可将特征值转化为0到1区间内的值：\\(newValue = \\frac{oldValue - min}{max - min}\\)，其中 \\(min\\) 和 \\(max\\) 分别是数据集中的最小特征值和最大特征值。我们需要在脚本 kNN.py 中增加一个函数 autoNorm，自动将数字特征值转化为0到1区间内的值。\n\n\n\nautoNorm\n\n\n我们将每列的最小值放在变量minVals中，每列最大值放在变量maxVals中，其中的参数0使得函数可以从列中选取最小值和最大值。然后函数计算可能的取值范围，并创建新的返回矩阵。\n正如前面给出的公式，为了归一化特征，我们使用当前值减去最小值，然后除以取值范围。而需要注意的是，特征值矩阵有1000*3个值，而minVals和maxVals的值都为1*3。为了解决这个问题，我们使用NumPy库中函数tile将变量内容复制成输入矩阵同等大小的矩阵，然后再利用具体特征值相除得到归一化后的特征矩阵。需要注意的是：对于某些数值处理软件包，/可能意味着矩阵除法，但在NumPy库中，矩阵除法需要使用函数linalg.solve(matA,matB)。\n我们重新加载kNN.py模块，执行函数autoNorm：\n\n\n\nreload autoNorm\n\n\n这里我们也可以只返回normMat矩阵，但是后面我们需要取值范围和最小值来归一化需要测试的新数据。\n\n\n测试算法：作为完整程序验证分类器\n我们已经对数据按照需求进行了处理，下面我们来测试分类器的效果。我们将已有数据的90%作为训练集来训练分类器，使用余下的10%作为测试集，检测分类器的错误率。这里由于海伦提供的数据并没有按照特定目的来排序，因而我们可随意选择10%数据而不影响测试集选择的随机性。\n\n\n\ndatingClassTest\n\n\n转换数据格式并归一化后，我们决定哪些数据用于测试，然后将训练集和测试集输入到kNN分类器classify函数中，计算错误率并输出分类结果。\n\n\n\ntest result\n\n\nkNN分类器在测试集上的错误率为5%。我们可以改变函数datingClassTest内变量hoRatio和变量k的值，看看错误率是否会发生一些变化。\n现在，海伦可以输入未知对象的特征信息，由的分类器来帮助她判定某一对象的可交往程度：讨厌、一般喜欢、非常喜欢。\n\n\n使用算法：构建完整可用系统\n我们会给海伦一小段程序，通过该程序海伦会在约会网站上找到某个人并输入他的信息，程序会给出她对对方喜欢程度的预测值。\n\n\n\nclassifyPerson\n\n\n这里使用input函数获取用户控制台的输入。我们让海伦给出她在约会网站上新找的一个人信息。由于NumPy库提供的数组操作并不支持Python自带的数组类型，因此在编写代码时要注意不要使用错误的数组类型。另外在输入新样本时注意将其归一化处理。\n\n\n\nclassify result\n\n\n这样，我们就完成了kNN对约会网站的配对效果的改进了。"
  },
  {
    "objectID": "blog/2020/01/17/index.html",
    "href": "blog/2020/01/17/index.html",
    "title": "你真的了解自己的电脑吗？",
    "section": "",
    "text": "前言\n我们现在基本上人手一台电脑，无论你是Mac、Windows还是Linux，你真的了解你手头上的电脑么？当你在选购新电脑或者购买部件的时候，是否了解该怎么挑选呢？接下来我们一起了解下最常见也是容易搞不懂的电脑/计算机。\n\n\n什么是计算机\n一般地，接受用户输入命令与数据，经由中央处理器的数学与逻辑单元运算处理后，以产生或存储成有用的信息的机器，我们就称之为计算机。从这个意义上讲，我们日常用的计算器、手机、GPS卫星定位系统、ATM取款机、台式电脑、笔记本电脑、iPad、Apple Watch等都是计算机。我们常说的计算机是其中的台式或笔记本电脑。\n\n\n电脑硬件\n电脑的三大部分（以台式电脑为例）包括：\n\n输入单元：包括键鼠、读卡器、扫描仪、手写板、触控屏幕等\n主机部分：即系统单元，在主机机箱内，里面含有一堆板子、CPU与内存等\n输出单元：屏幕、打印机等\n\n主机里面最重要的就是一块主板，上面安装了中央处理器（Central Processing Unit, CPU）以及内存、硬盘（或存储卡）还有一些适配卡设备。大部分智能手机是将这些组件直接焊接在主板上面而不是插卡。\n\n\nCPU\n整台主机的重点在于CPU，CPU为一个具有特定功能的芯片，里面含有指令集，如果你想要让主机进行什么操作，就得要参考这块CPU是否有相关内置的指令集才可以。由于CPU的工作主要在于管理和运算，因此在CPU内又可分为两个主要的单元，分别是算术逻辑单元和控制单元。其中算术逻辑单元主要负责程序运算与逻辑判断，控制单元则主要协调各周边组件与各单元间的工作。CPU是整个电脑系统的最重要部分。\nCPU依设计理念不同，主要分为：\n\n精简指令集（RISC）系统：ARM公司的ARM CPU系列等。我们常使用的各品牌手机、导航系统、路由器等，几乎都是使用ARM架构的CPU。\n复杂指令集（CISC）系统：AMD、Intel等x86架构的CPU。由于x86架构CPU被大量用于个人电脑，因此个人电脑常被称为x86架构电脑。64位的个人电脑CPU又被统称为x86-64架构。x86架构的称呼来源于Intel最早研发出来的CPU代号。所谓的位（bit），指的是CPU一次读取数据的最大量。64位CPU表示CPU一次可以读写64位的数据，一般32位CPU所能读写的最大数据量大概是4GB。\n\n\n\n电脑上面常用的计算单位（容量、速度等）\n\n容量单位：电脑对数据的判断主要依据有没有通电来记录信息，所以理论上对于每一个记录单位而言，电脑只认识0与1而已。0/1这个二进制的单位我们称之为位（bit，比特）。但位实在太小，因而每份数据都使用8个位来记录，8位为一字节（Byte）。同样的，字节依然太小，因而有K代表1024B，M代表1024K，G代表1024M，T代表1024G，P代表1024T，E代表1024P。一般来说，数据容量使用二进制，所以1GB的文件大小为102410241204B。\n速度单位：CPU的命令周期常使用MHz或GHz之类的单位，这个Hz是“次数/秒”的意思。而在网络传输方面，由于网络使用的是位为单位，因此网络常使用的单位为Mbit/s（每秒多少Mbit）。大家常听到的“20M/5M”光纤传输速度，如果转成数据容量的字节时，其实理论最大传输值为：每秒2.5MB/每秒625KB的下载或上传速度。\n\n假设你今天购买了一块500GB的硬盘，但是格式化完毕后只剩下460GB左右的容量，这是为什么呢？一般硬盘制造商使用十进制的单位，所以500GB代表50010001000*1000B，转成数据的容量单位时使用二进制（1024为基数），所以就成为466GB左右的容量了。并非厂商骗人，只是因为硬盘的最小物理量为512B，最小的组成单位为扇区（sector），通常硬盘容量的计算采用多少个扇区，所以才会使用十进制来处理。\n\n\n内存\nCPU读取的数据完全从内存中来（无论是程序还是一般文件数据），如果要读取硬盘中的数据，也要将数据挪到内存当中，再交由CPU来读取。内存中的数据则是从输入单元所传输进来的，而CPU处理完毕的数据也必须要先写回内存，最后数据才从内存传输到输出单元。\n这就是我们常说的，要加快系统性能，通常将内存容量加大就可以获得相当好的效果。因为所有的数据都是要经过内存的传输，所以内存的容量如果太小，数据读写性能就不足，对性能的影响相当大，尤其在Linux作为服务器操作系统的环境下。这也是为什么在买手机时，人们对可用内存（运行内存）的要求都很高的原因。"
  },
  {
    "objectID": "blog/2020/01/19/index.html",
    "href": "blog/2020/01/19/index.html",
    "title": "谈谈电脑的CPU",
    "section": "",
    "text": "前言\n前面我们已经初步了解了计算机–你真的了解自己的电脑吗？接下来我们继续深入计算机的CPU单元。\n\n\nCPU\n前面我们提过，一般我们常说的电脑指的是x86的个人电脑架构。Linux操作系统最早在发展的时候，就是依据个人电脑的架构来设计的。而在个人电脑架构中，充当“大脑”的无疑是CPU。\n由于CPU负责大量运算，因而它是电脑中具有相当高发热量的组件。现在的所谓多内核CPU，是在一块CPU封装内嵌入两个以上的运算内核，即含有两个以上的CPU单元。\n我们已经知道，CPU内部指令集的不同会导致其工作效率的高低，那么CPU性能的比较还有什么呢？答案是频率。CPU的频率就是CPU每秒钟可以进行的工作次数，频率越高表示这块CPU在单位时间内可以做更多的事情。举例来说，Intel的i7-4790CPU频率为3.6GHz，即表示这块CPU在一秒内可以进行3.6*10的九次方次工作。但是需要注意的是：只能在同款CPU间比较频率的快慢，不同CPU由于指令集、架构、使用的二级缓存及其运算机制的可能不同，单纯看频率没有可比性。\n我们可能听过“超频”这个词，它是什么意思呢？CPU在出厂时，厂商已经设置了这款CPU正常稳定工作的频率，一些电脑硬件玩家要发挥出CPU最大的性能，往往会手动将CPU的外频通过主板提供的设置功能更改成较高频率。现在Intel的CPU会主动帮你超频，以合理利用CPU以及节能。\n最常听见的还有32位与64位电脑，我们可能也一头雾水。其实这个也与CPU相关。我们将CPU每次能够处理的数据量称为字长（word size），字长依据CPU的设计而有32位与64位，而32位与64位电脑主要就是依据这个CPU所能解析的字长而来的。早期的32位CPU中，由于CPU每次解析的数据量有限，因此从内存传来的数据量就有所限制，这也导致32位的CPU最多只能支持最大到4GB的内存。目前的64位CPU统称为x86-64。"
  },
  {
    "objectID": "blog/2020/01/21/index.html",
    "href": "blog/2020/01/21/index.html",
    "title": "上手vim编辑器",
    "section": "",
    "text": "我们平时已经接触了不少的程序编辑器，今天我们要上手一种命令行模式下的文本编辑器——vim编辑器。说它是文本编辑器有点小瞧它的功能，实际上，它也可以作为程序编辑器使用，且功能十分强大。\n在所有的Linux发行版上面都会有一个文本编辑器，那就是vi，vim是高级版的vi。vim不仅可以用不同的颜色显示文字内容，还能够进行诸如shell脚本、C语言等程序编辑，搭配Python也是十分的香，甚至不少人用它来作为写作的专用编辑器。由此可知它的功能有多么的强大。\n如果你在学习Linux，身边的人都会建议你：学习使用命令行模式来处理Linux系统的设置问题，而尽量少去使用图形窗口模式。在配置Linux参数文件时，我们就需要一款强大稳定的文本编辑器。而Linux在命令行模式下的文本编辑器有哪些呢？我们常听到emacs、nano、vim等，而在其中，其实vim并非是对用户最友善的文本编辑器。但是为什么这么多人推荐使用呢？原因有几点：\n\n所有的UNIX-like系统都会内置vi文本编辑器，其他的文本编辑器不一定会存在；\n很多软件的编辑接口都会主动调用vi；\nvim具有程序编辑的能力，可以主动地以字体颜色辨别语法的正确性，方便程序设计；\n编辑速度相当快速。\n\n可以说，如果不上手vim，Linux中很多命令根本无法操作。\n我们提到，vim是高级版本的vi，它可以用颜色或下划线的方式来显示一些特殊的信息，可以依据文件的扩展名或是文件内的开头信息，判断该文件的内容而自动调用该程序的语法判断样式，再以颜色来显示程序代码与一般信息，vim是程序开发者的一项非常好用的工具，就连vim的官方网站（http://www.vim.org）都认为自己是一款程序开发工具而非仅仅是文本处理软件。\n由于是命令行模式下的编辑器，当我们在编辑程序或者制作网页的时候，vim不能做到一般编辑器那样所见即所得，这是它的一个特色。vim同样也有一些非常好用的功能，如支持正则表达式的查找方式、多文件编辑、区块复制等，非常的棒。我们会在日后持续更新vim的使用分享。"
  },
  {
    "objectID": "blog/2020/01/27/index.html",
    "href": "blog/2020/01/27/index.html",
    "title": "计算机概论4",
    "section": "",
    "text": "显卡\n显卡又称为VGA（Video Graphics Array），它对于图形影像的显示扮演着相当关键的角色。一般对于图形影像的显示重点在于分辨率与颜色深度，因为每个图像显示的颜色会占用内存，因此显卡上面会有集成内存并被称为显存，这个显存容量将会影响到你的屏幕分辨率与颜色深度。\n假设你的显示器使用1024*768分辨率，且使用全彩（每个像素占用3B的容量），至少需要多少内存才能使用这样的饱和度？\n因为1024*768分辨率中会有786432个像素，每个像素占用3B，所以总共需要2.25MB以上才行。但如果考虑屏幕的刷新率（每秒钟屏幕的刷新次数），显卡的内存还是越大越好。\n除了显存之外，现在显卡的运算能力也越来越重要，所以显卡厂商直接在显卡上面嵌入一个3D加速的芯片，这就是所谓的GPU称谓的由来。\n显卡主要也是通过GPU的控制芯片来与CPU、内存等通信，也是需要高速运算的一个组件，所以数据的传输也是越快越好。\n显卡与电脑屏幕（或电视）连接的主要接口有：\n\nD-Sub（VGA接口）：较早之前的连接接口，当初设计是针对传统的CRT显示器而来；\nDVI：常见于液晶屏幕的连接；\nHDMI：可同时传输影像与声音，被广泛地使用于电视屏幕中，电脑屏幕目前也经常都会支持HDMI格式；\nDisplayPort：与HDMI相似，可同时传输影像与声音。\n\n\n\n硬盘\n硬盘是由许多的圆形碟片、机械手臂、磁头与主轴马达所组成的。实际的数据都是写在具有磁性物质的碟片上面，而读写主要是通过在机械手臂上的磁头来完成的。实际运行时，主轴马达让碟片转动，然后机械手臂可伸展让磁头在碟片上面进行读写的操作。另外，由于单一碟片的容量有限，因此有的硬盘内部会有两个以上的碟片。\n由于碟片是圆的，且通过机械手臂去读写数据，碟片要转动起来才能够让机器手臂读写，因而通常数据就是以圆圈转圈的方式读写。当初设计在类似碟片同心圆上面切出一个一个的小区块，让磁头去读写，这个小区块就是磁盘的最小物理存储单位，称之为扇区（sector），同一个同心圆的扇区组合成的圆就是所谓的磁道（track）。由于磁盘里可能会有多个碟片，因此在所有碟片上面的同一个磁道可以组合成所谓的柱面（cylinder）。\n我们知道同心圆外圈的圆比较大，占用的面积比内圈多，所以外圈的圆有更多的扇区，通常数据的读写也是默认从外圈开始往内写。原本硬盘的扇区都是设计成512B的大小，目前绝大部分的高容量硬盘已经使用了4KB大小的扇区设计。也因为这个扇区设计，在磁盘分区方面，目前有旧式的MBR模式（MS-DOS兼容模式），以及较新的GPT模式。在较新的GPT模式下，磁盘的分区通常使用扇区号码来划分，和过去旧的MS-DOS是通过柱面号码来划分的方式不同。\n注意：由于硬盘内部机械手臂上的磁头与碟片的接触是很细微的空间，如果有抖动或是污物附着在磁头与碟片之间就会造成数据的损坏或是物理磁盘整个损坏，因而，在电脑通电之后避免震动硬盘。另外，因为机械手臂必须要回归原位，不要随便将电脑电源插头拔掉就以为是顺利关机。\n\n\n固态硬盘\n传统硬盘有个很致命的问题，就是需要驱动马达来转动碟片，这会造成很严重的磁盘读取延迟。因此有厂商拿闪存去制作高容量的设备，而且外形还做的和传统磁盘一样。所以，这类设备已经和传统的机械磁盘（Hard Disk Drive，HDD）不同，我们称之为固态硬盘（Solid State Disk或Solid State Driver，SSD）。\n固态硬盘的最大好处是：它没有马达要去转动，而是通过闪存直接读写的特性，因此除了没数据延迟且快速之外，还很省电。测试磁盘的性能时，有个很特殊的度量单位，称为每秒读写操作次数（Input/Output Operations Per Second，IOPS），这个数值越大，代表可操作次数较高，当然性能也越好。\n目前大家对于HDD和SSD的使用方式大多是：使用SSD作为系统盘，将数据存储放在HDD上，这样系统运行快速，而数据存储量也大。"
  },
  {
    "objectID": "blog/2020/01/29/index.html",
    "href": "blog/2020/01/29/index.html",
    "title": "R语言基础–运算符",
    "section": "",
    "text": "前言\n运算符是一些符号，进行算术运算、比较运算或逻辑运算等。\n\n\n算术运算符\n指数学运算中常用的5种运算符号，有：\n\n^ 幂\n* 乘\n/ 除\n+ 加\n- 减\n%% 模运算\n%/% 整数除法\n\n\n\n比较算符\n建立两个量之间的一种关系，并要求R确定这种关系是否成立。若成立，输出的运算结果为1（TRUE），若不成立，运算结果为0（FALSE）。\n\n== 等于\n!= 不等于\n&gt; 大于\n&lt; 小于\n&gt;= 大于等于\n&lt;= 小于等于\n\n\n\n逻辑算符\n通常用来连接一系列比较式，有：\n\n&& 标量的逻辑“与”运算\n|| 标量的逻辑“或”运算\n& 向量的逻辑“与”运算\n| 向量的逻辑“或”运算\n! 逻辑“非”\n\nR语言表面上没有标量的类型，标量可以看作是含有一个元素的向量，但逻辑运算符对标量和向量有着不同的形式。\n\n\n运算次序\n复杂表达式运算次序的准则：\n\n括号里的表达式先计算；\n较高优先级的运算先执行，具体的优先级的顺序为：\n\n\n第一级（最高级）：^（幂）；!（非）\n第二级：*（乘）；/（除）\n第三级：+（加）；-（减）\n第四级：&lt;；&lt;=；&gt;；&gt;=；==；!=\n第五级：&；&&；|；||\n\n\n对于相同优先级的算符，先做左边的运算。"
  },
  {
    "objectID": "blog/2020/01/31/index.html",
    "href": "blog/2020/01/31/index.html",
    "title": "计算机概论5",
    "section": "",
    "text": "CMOS与BIOS\n前面我们提过CMOS与BIOS的功能：CMOS主要记录主板上面的重要参数，包括系统时间、CPU电压与频率、各项设备的I/O地址与IRQ等。BIOS是写入到主板上某一块flash的程序，它可以在计算机启动的时候执行，以加载CMOS当中的参数，并尝试调用存储设备中的引导程序，进一步进入操作系统当中。BIOS程序可以修改CMOS中的数据，每种主板进入BIOS设置程序的按键都不同，一般桌面电脑常见的是使用[Del]按键进入BIOS设置界面。\n\n\n设备I/O地址与IRQ中断请求\n主板是负责各个电脑组件之间的通信，但是电脑组件实在太多，有输出/输入不同的存储设备等，这个时候主板芯片组就需要用到I/O地址与IRQ进行设备间的通信。I/O地址有点类似于每个设备专属的门牌号码，一般来说，不能有两个设备使用同一个I/O地址，否则系统会不知道该运行哪个设备。IRQ可以想成是各个设备门牌连接到邮件中心（CPU）的专门路径，各设备可以通过IRQ中断请求来告知CPU该设备的工作情况，以方便CPU进行工作分配的任务。\n\n\n数据表示方法\n事实上我们的电脑只认识0与1，记录的数据也是只能记录0与1而已。早期的电脑使用的是利用通电与否的特性制造的电子管，如果通电就是1，没有通电就是0，后来沿用至今，也就是我们说的二进制（binary）。电脑在表示利用数字时，利用二进制的转换进行对数字的处理。\n而对于文字的记录，事实上文本文件也是被记录为0与1，而这个文件的内容在被读取查看时，必须要经过一个编码系统的处理才行。所谓的编码系统可以看成是一个字码对照表。当我们要写入文件的字符数据时，该文字数据会由编码对照表将该字符转成数字后，再存入文件当中。同样，当我们要将文件内容的数据读出时，也会经过编码对照表将该数字转成对应的字符后，再显示到屏幕中。如果编码对照表写错，导致对照的字符产生误差，就会出现乱码。\n常用的英文编码表为ASCII系统，这个编码系统中，每个符号（英文、数字或符号等）都会占用1字节的记录，因此总共会有2的8次方即256种变化。国际组织ISO/IEC制定了所谓的Unicode编码系统，即我们常常说的UTF-8。这个编码打破了所有国家不同的编码之间的限制，因此目前互联网网站大多以此编码系统为主。"
  },
  {
    "objectID": "blog/2020/02/02/index.html",
    "href": "blog/2020/02/02/index.html",
    "title": "计算机概论6",
    "section": "",
    "text": "机器语言程序与编译型程序\n我们在需要CPU工作时，就得要参考其内部指令集的内容，然后编写让CPU能够读得懂的脚本让其去执行，这样CPU才能执行我们所给的任务。\n这就带来了几个问题：\n\n程序开发者必须要了解机器语言：机器只认识0和1，因此开发者必须要学习写机器能直接看懂的语言，而这个毫无疑问，难度很大\n必须要了解所有硬件的相关功能函数：开发者当然需要参考机器本身的功能去编写相应的程序代码，而如果每个开发者都去了解系统的所有硬件，这个工作量实在太大\n程序具有不可移植性：每个CPU都有其独特的指令集，每个硬件都有其功能函数，因此不同平台之间程序代码当然无法直接通用\n\n为了解决这些问题，计算机科学家设计出一种人类能看得懂的程序语言，然后创造一种编译器将这些人类写的程序语言转译成为机器能看得懂的机器语言，如此一来，我们修改与编写程序就容易多了。目前常见的编译器有C、C++、Java、Fortran等。\n高级程序语言的程序代码是较容易查看的，这样我们就将程序的编写修改问题处理完毕。问题是，在这样的环境下面我们还是得要考虑整体的硬件系统，从而来设计编写程序。举个例子，当你需要将运行的数据写入内存中，你就要自行分配一个内存区块出来让这些数据能够填充上去，所以你必须要去了解内存的地址是如何定位的原理，这样一来，程序的编写又会变得麻烦。\n为了要解决硬件方面老师需要重复编写一些程序的问题，所以有了操作系统的出现。"
  },
  {
    "objectID": "blog/2020/02/05/index.html",
    "href": "blog/2020/02/05/index.html",
    "title": "计算机概论7–操作系统",
    "section": "",
    "text": "我们前面提到，在早期想要让计算机执行程序就得要参考一堆硬件功能函数，并且学习机器语言才能够编写程序，同时由于硬件与软件功能不一定一致，每次编写程序时都必须要重新改写，非常的麻烦。\n如果能够将所有的硬件都驱动，并且提供一个软件的参考接口来给工程师开发软件，开发软件无疑会变得简单得多，而这就是操作系统（Operating System，OS）。\n\n操作系统内核（Kernel）\n操作系统其实也是一组程序，这组程序的重点在于管理计算机的所有活动以及驱动系统中的所有硬件。硬件的所有操作都必须要通过操作系统来实现，而这一功能的实现就是靠操作系统的内核完成。你的计算机能不能完成一些任务，都与内核有关。只有内核提供的功能，你的计算机系统才能帮你完成。举例来说，如果你的内核并不支持TCP/IP的网络协议，那么无论你配置什么样的网卡，这个内核都无法提供网络功能。\n内核主要在管理硬件与提供相关的功能（读写硬盘、网络功能、CPU资源分配等），这些管理的操作都非常重要。如果用户能够直接使用到内核的话，一旦不小心将内核程序停止或破坏，将会导致整个系统的崩溃。因此内核程序放置到内存当中的区块是受保护的，并且启动后就一直常驻在内存之中。\n\n\n系统调用\n既然硬件都是由内核管理的，那么如果开发人员想要开发软件的话，自然就得要参考这个内核的相关功能。这样一来，还是从原本的参考硬件函数变成参考内核功能，还是一样的麻烦。\n为了解决这个问题，操作系统通常会提供一套应用程序编程接口（Application Programming Interface，API）即系统调用层给程序员来开发软件，开发人员只要遵守该API公认的系统调用参数就可以比较容易地开发软件了。举例来说，我们学习C语言只要参考C语言的函数即可，不需要再去考虑其他内核的相关功能，因为内核的系统调用接口会主动地将C语言的相关语法转成内核可以了解的任务函数，内核自然就能够顺利地运行该程序。\n内核只会进行计算机系统的资源分配，所以系统还需要有应用程序的提供，才能够供用户使用。应用程序与内核有比较大的关系，与硬件关系则不大；硬件也与内核有比较大的关系，至于与用户直接有关的则是应用程序。\n有几点需要注意：\n\n操作系统的内核层直接参考硬件规格写成，所以同一个操作系统程序不能够在不一样的硬件架构上运行；\n操作系统只是管理整个硬件资源，包括CPU、内存、输入输出设备及文件系统等，如果没有其他的应用程序辅助，操作系统只能让计算机处于准备妥当的状态之中，无法完成用户所想要的功能；\n应用程序的开发都是参考操作系统的API，所以该程序只能在该操作系统当中运行，不能在其他操作系统上运行。所以有些游戏不能够在Linux上安装运行。"
  },
  {
    "objectID": "blog/2020/02/17/index.html",
    "href": "blog/2020/02/17/index.html",
    "title": "试验设计与方差分析（1）",
    "section": "",
    "text": "完全随机设计\n完全随机设计采用完全随机化的分组方法，将全部试验对象分配到k个处理组，各组分别接受不同的处理，试验结束后比较各组均数之间的差别有无统计学意义，推断处理因素的效应。\n在方差分析中，常称上述的处理因素为因子，用A、B、C等表示因子在试验中所处的不同情况或状态称为水平。\n方差分析解决问题的思路是：从所有观测值的总变异（总方差）中分析出系统误差和随机误差，并用数量表示，在一定意义下比较系统误差和随机误差，若两者的差别不大，说明试验条件的变化（因素水平的不同）对试验结果影响不大；若两者相差较大，且系统误差大得多，说明系统条件变化引出的误差不可忽视。\n\n\n随机区组设计\n随机区组设计是根据局部控制和随机排列的原理进行的：将研究的对象按照不同的性质划分为等于重复次数的区组，例如将试验土地按土壤肥力程度等不同的性质划分为等于重复次数的区组，使区组内环境差异最小，而区组间环境允许存在差异，每个区组即为一次完整的重复，区组内的各处理都独立地随机排列，这是随机排列设计中最常用最基本的设计。\n随机区组设计的优点：富于伸缩性，单因素、复因素以及综合试验等都可应用；能提供无偏的误差估计，在大区域试验中能有效地降低非处理因素等试验条件的单向差异，降低误差；对试验地的地形要求不严，只对每个区组内的非处理因素等试验条件要求尽量一致，因此不同区组可分散设置在不同地段上。\n随机区组设计的缺点：这种设计方法不允许处理数太多，处理数过多，区组必然增大，局部控制的效率会降低，所以处理数一般不超过20个，最好在10个左右。\n随机区组设计考虑了个体差异的影响，可分析处理因素和个体差异对试验效应的影响，所以又称为两因素实验设计，比完全随机设计的检验效率高。\n该设计是将受试对象先按配比条件配成配伍组（如动物试验时，可按同窝别、同性别、体重相近等进行配伍），每个配伍组有3个或以上的受试对象，再按随机化原则分别将各配伍组中的受试对象分配到各个处理组。\n在进行单因素随机区组试验结果的统计分析时，可将处理看作A因素，区组看作B因素，剩余部分则为试验误差。总平方和=区组间平方和+处理间平方和+试验误差平方和，总自由度=区组自由度+处理自由度+误差自由度。"
  },
  {
    "objectID": "blog/2020/02/20/index.html",
    "href": "blog/2020/02/20/index.html",
    "title": "试验设计与方差分析（3）",
    "section": "",
    "text": "正交设计\n析因设计的缺点是当因素个数较多时（3个因素以上），所需试验单位数、处理组数、试验次数和方差分析的计算量会剧增。减少多因素试验次数的有效方法是采用正交试验设计。\n当析因设计要求的试验次数太多时，一个非常自然的想法就是从析因设计的全部水平组合中选择一部分有代表性的水平组合进行试验，因此就出现了分式析因设计。但是对于试验设计知识较少的实际工作者而言，选择适当的分式析因设计还是比较困难的，而正交试验设计是研究多因素多水平的又一种试验设计方法。\n它是根据正交性从全部的试验组合中挑选出部分有代表性的水平组合进行试验，这些有代表性的水平组合具备均匀分散、齐整可比的特点。正交试验设计是分式析因设计的主要方法，高效、快速、经济。日本著名统计学家田口玄一将正交试验选择的水平组合列成表格，称为正交表。\n例如做一个3因素3水平的试验，按全面试验要求，需进行333=27种水平组合的试验，且尚未考虑每一组合的重复数，而正交试验设计可以大大减少工作量。\n正交设计在医学研究中的用途相当广泛，在具体的操作上，也比析因设计简单，可寻找疗效好的药物配方、医疗仪器多个参数的优化组合、医疗产品的生成工艺、生物体的培养条件等。\n假定在一个农业试验中要考察3个小麦品种、3种不同的肥料和3种播种方式对小麦产量的影响，并假定有9个地力基本相同的试验小区。在这个问题中，有3个可能影响小麦产量的因子：品种、肥料和播种方式，每个因子有3个水平，如果要做完全试验，就需要333=27个小区，而实际上总共只有9个小区，显然，完全试验在当前的情况下行不通。\n因此我们可退一步考虑，按照上述正交设计的想法，设计要求品种、肥料和播种方式中的任意两个的不同水平的搭配都出现一次，满足这种性质的试验设计就是正交试验。\n下面给出正交设计的一般性陈述：考虑设计一个试验，安排m个因子，做n次试验，若它满足以下两个条件，则其为正交试验：\n\n每一因子的不同水平在试验中出现相同的次数（均衡性）；\n任意两因子的不同水平组合在试验中出现相同的次数（正交性）。\n\n就定义来说，等重复的完全试验显然满足上述两个条件，因此当然也是正交试验设计。但由于其要求的试验次数太多，所以实际上很难实施。我们通常所说的正交试验设计，是指既满足上述两条件，同时试验次数n又远远小于N的设计。\n正交试验设计的方案可以用一张表来表示，这张表就被称为正交设计表。一般来说，正交设计表的第1行为表头，标明每列所代表的因子，最左一列标明试验的序号，由1到n。注意：试验的序号并不表示试验的时间先后顺序，先后顺序要按照随机化原则来安排。表中每列的数字代表相应因子的水平序号，每行的数字代表在相应试验中各因子的水平序号，有：\n\n每列中不同数字出现的次数相同（试验的均衡性）；\n每两列中不同的数字组合出现的次数相同（试验的正交性）。\n\n假定因子对响应变量的影响无交互效应（许多实际情况正是这样），正交试验的优点是在很少的试验次数（与全面试验相比）中，所得数据可以简便而有效地对因子效应进行参数估计和方差分析。\n其方法可一般地归纳如下：\n\n总均值的估计=试验数据的总平均值\n某因子的某个主效应的估计=该因子的该主效应所出现的试验数据的平均值-总平均值\n总平方和=（试验数据-总平均值）的平方和，自由度=n-1\n某因子的主效应平方和=重复数*参数估计的平方和，自由度=水平数-1\n残差平方和=总平方和-因子效应平方和的和，自由度=总平方和-因子效应自由度的和"
  },
  {
    "objectID": "blog/2020/02/23/index.html",
    "href": "blog/2020/02/23/index.html",
    "title": "协方差分析",
    "section": "",
    "text": "医学试验设计一个很重要的目的就是为了排除非处理因素的干扰影响，使试验误差的估计降到最低限度，从而可以准确地获得处理因素的试验效应。但在某些实际问题中，有些因素在目前还不能控制或难以控制。如在动物饲养试验中，各组动物所增加的平均体重不仅仅与各种饲料营养价值高低有关，还与各动物的进食量有关，甚至与各动物的初始体重等因素及其交互作用都有关系。如果直接进行方差分析，会因为混杂因素的影响而无法得出正确的结论。\n协方差分析是将回归分析与方差分析结合起来使用的一种分析方法。在协方差分析中，先将定量的影响因素（即难以控制的混杂因素）看作自变量/协变量，建立因变量随自变量变化的回归方程，利用回归方程把因变量的变化中受不易控制的定量因素的影响扣除掉，从而能够较合理地比较定性的影响因素处在不同水平下，经回归分析手段修正以后的因变量的总体均数之间是否有显著性的差别，这就是协方差分析的基本思想。\n协方差分析用于比较一个变量Y在一个或几个因素不同水平上的差异，但Y在受这些因素影响的同时，还受到另一个变量X的影响，且X变量的取值难以人为控制，不能作为方差分析中的一个因素处理。此时如果X与Y之间可以建立回归关系，则可以用协方差分析的方法排除X对Y的影响，然后再用方差分析的方法对各因素水平的差异进行统计推断。在协方差分析中，我们称Y为因变量，X为协变量，即在方差分析中用来校正因变量的数值型变量。\n也许有人会问随机因素的影响也是不能人为控制的，为什么不能把X作为一种随机因素处理呢？\n这里的差异主要在于作为随机因素处理时，虽然每一水平的影响是不能人为控制的，但我们至少可以得到几个属于同一水平的重复，因此可以把它们分别用另一因素的不同水平处理，最后在方差分析时，我们才能排除这一随机因素的影响，对另一因素的各水平进行比较。\n例如，当我们考虑动物窝别对增重的影响时，一般可把它当作随机因素处理。一方面是由于它不易数量化，另一方面是同一窝一般有几只动物，可分别接受另一因素不同水平的处理。如果我们考虑试验开始时动物初始体重的影响，这时一般的方法是选初始体重相同的动物作为一组，分别接受另一因素的不同水平处理，此时用方差分析没有问题。但显然，这种方法往往是很困难的，一般需要很大的样本。若可供试验的动物样本很少，初始体重又有明显差异，无法选出体重相当的动物，那就只好认为初始体重X与最终体重Y有回归关系，采用协方差分析的方法排除初始体重的影响，再来比较其他因素如饲料种类、数量对增重的影响。协方差分析既利用了回归分析的基本方法，又用到了方差分析的基本方法，这就是协方差分析的基本思想。\n消除初始体重影响的另一种方法是对最终体重与初始体重的差值进行统计分析，这种方法与协方差分析的生物学意义是不同的。对差值进行分析的生物学假设是初始体重对以后的体重增量没有任何影响，而协方差分析则是假设体重增量中包含初始体重的影响（不仅仅是初始体重对最终体重的影响），这种影响的大小与初始体重成正比，如果这一比值为1，协方差分析与对差值进行方差分析是相同的。但如果比值不为1，它们的结果将是不同的。也就是说，协方差分析假设使初始体重不同的因素在以后的生长过程中也会发挥作用，而对差值进行方差分析则是假设这些因素以后不再发挥作用，这两种生物学假设是有很大区别的。\n在学习中需要注意统计学知识背后的研究假设。由于协方差分析包含了对协变量影响是否存在及其大小等一系列统计检验与估计，它显然比对差值进行分析等方法有更广泛的适用范围，因此除非有明显的证据说明对差值进行分析的生物学假设是正确的，一般情况下还是应采用协方差分析方法。\n在医学研究中，很多情况下都需要借助协方差分析来排除非处理因素的干扰，从而准确地估计处理因素的试验效应。如，评价3种药物治疗高脂血症的效果，寻求各方面自然条件基本相同的受试者是很困难的，但是把患者的年龄、体重指数、用药前的血脂水平等作为协变量进行协方差分析，就简单很多。同样，比较几种不同营养奶粉对婴幼儿体重增长的作用差异，把研究对象的性别、年龄、基线体重等混杂因子作为协变量进行协方差分析，则是非常有效的统计分析方法。\n和方差分析一样，协方差分析也属于参数分析，因变量Y应当满足以下假设条件：\n\n在效应因子的每一个水平上，因变量Y服从正态分布，且方差相等；\n在效应因子的每一个水平上，因变量Y和协变量X呈线性关系，且斜率相同。"
  },
  {
    "objectID": "blog/2020/03/12/index.html",
    "href": "blog/2020/03/12/index.html",
    "title": "空间中的向量",
    "section": "",
    "text": "我们知道，向量的坐标表示方法并不是唯一的，它的具体表示和空间中基底的选择密切相关。\n\n向量的坐标\n向量的坐标依赖于选取的基底。\n对于二维向量u=[4,5]⊤而言，我们一直以来都理所当然地认定一个事实：它表示一条在x轴上投影为4、y轴上投影为5的有向线段，它的坐标是(4,5)。这其实是基于一个没有刻意强调前提：利用方向为x轴、y轴正方向，且长度为1的两个向量，即Ex=[1,0]⊤，Ey=[0,1]⊤作为上述讨论的基础。因此，对于向量u而言，其完整的写法应该为u=4Ex + 5Ey，进一步展开就是u=4[1,0]⊤ + 5[0,1]⊤，这种形式的表意是最完整的。\n这里被选中作为向量u基准的一组向量是Ex和Ey，它们被称为基底。基底的每一个成员向量被称为基向量，而坐标对应的就是各个基向量前的系数。一般情况下，若不做特殊说明，那么基向量都是选取沿着坐标轴正方向且长度为1的向量，这样方便描述和计算。\n关于向量u的完整准确的说法是：在基底(Ex,Ey)下，其坐标是[4,5]⊤。也就是说，坐标必须依托于指定的基底才有意义。因此，要想准确地描述向量，首先就要确定一组基底，然后通过求出向量在各个基向量上的投影值，最后才能确定在这个基上的坐标值。\n\n\n向量在不同基底上表示为不同坐标\n一个指定的向量可以在多组不同的基底上进行坐标表示，在不同的基底表示下，坐标自然也是不同的。根据一组基底对应的坐标值去求另一组基底所对应的坐标值，这就是以后我们将会反复用到的坐标变换。\n根据我们之前关于向量内积的介绍，最好是事先把基向量的模长转化为1。这样一来，从向量内积的内涵可以看出，若基向量的模长是1，那么就可以用目标向量内积基向量，从而可以直接获得该向量在这个基向量方向上的对应坐标值。实际上，对于任何一个向量，想要找到同方向上模长为1的向量并不是一件难事，只要让向量的各成分分别除以向量的模长即可，就能使向量的模长为单位1。而向量的坐标就是指定基的对应系数。\n\n\n构成基底的条件\n在一个n维空间中，不是随便选取n个向量都能作为一组基底，构成基底的向量必须满足这样的条件：在n维空间中，任意一个向量都可以表示为这一组基向量的线性组合，并且这种线性组合的表示方式（也就是系数）必须是唯一的。\n\n向量数量足够\n若想成为三维空间中的一组基底，首先，其中的每个基向量的维数都必须是3；其次，基向量的个数也必须为3个。若数量不足，如只有两个三维向量a1和a2（假设它们是不共线的两个向量），那么无论对这两个向量怎么进行线性组合，它们都只能表示二者所构成的平面上的任意向量，而三维空间中位于该二维平面上外的任何一个向量，都无法由a1和a2的线性组合进行表示。\n满足线性无关\n如何确保表示方法的唯一性呢？这里我们引入向量线性无关的概念。一组向量需要满足线性无关的条件，即其中任何一个向量都不能通过其余向量的线性组合的形式进行表示。\n换句话说，当且仅当x1=x2=x3=…=xn=0的等式关系成立时，线性组合x1u1 + x2u2 + x3u3 + … + xnun才能生成零向量，若xi中有非零值存在，那么这一组向量就是线性相关的。一组向量满足线性无关的条件等效于满足线性组合表示方法的唯一性（可以从反证法的角度说明线性无关和表示方法的唯一性是等价的）。\n在这个三维空间中，要求所选取的3个基向量线性无关。若它们线性相关，那么x3就可以表示为x1和x2的线性组合，换句话说，备选的3个向量就处在一个平面上了。这样，自然无法通过线性组合的方法来表示三维空间中位于平面外的任何一个向量了，即3个三维向量之间由于彼此线性相关，因此无法张成整个三维空间，只能张成三维空间中的二维平面甚至是退化为一条直线。\n若三维空间中基向量的个数超过3个，则是不行的。如，假设有4个向量试图成为该空间的一组基向量，任选出其中的3个向量，按照前提，假设它们之间满足线性无关性，那么对于第4个向量，由于它也处于三维空间中，则它一定能够被前3个向量的线性组合所表示。那么，三维空间中的这4个向量显示是线性相关的，无法满足向量构成基底的唯一性条件。\n\n\n\n构成基底的条件\n对于一组向量，由它的所有线性组合所构成的空间称为这一组向量的张成空间。张成空间对所讨论向量的线性无关性没有要求，这些向量可以是线性相关的。\n两个线性无关的二维向量，它们构成了二维空间中的一组基底，因此它们的张成空间就是整个二维空间；两个线性相关的共线二维向量，它们的张成空间是一条穿过原点的一维直线；等。\n向量的个数和维数都不是其张成空间维数及形态的决定因素，具体的情况需要结合向量的线性无关性进行整体考量，这就会涉及秩的相关概念。"
  },
  {
    "objectID": "blog/2020/04/25/index.html",
    "href": "blog/2020/04/25/index.html",
    "title": "R语言4.0发布上线",
    "section": "",
    "text": "昨天，R语言的4.0正式版本已经上线，小伙伴们可以去更新下载了，mac版本的地址：https://mirrors.tuna.tsinghua.edu.cn/CRAN/bin/macosx/R-4.0.0.pkg。\n一些小知识点：\n\nR包安装路径：/Library/Frameworks/R.framework/Versions/Current/Resources/library/；\n不同版本的R包会放在不同的路径下：/Library/Frameworks/R.framework/Versions；\n一般软件都会有很好的向下兼容性，高版本普遍能很好地支持低版本中的功能，但R比较特殊，很多人的电脑上会安装多个版本的R。R包的版本是跟着R的版本走的，所以你要是想用新版R环境中的新R包，就必须更新R；要是想用旧版的低版本R包，最好使用旧版R。最好的应对R版本更迭的方式就是：在你的电脑中安装多个版本的R，这样，使用R包的时候，就不用担心受R版本的影响了。"
  },
  {
    "objectID": "blog/2024/07/19/new_book/index.html",
    "href": "blog/2024/07/19/new_book/index.html",
    "title": "预测模型领域新书推荐",
    "section": "",
    "text": "首先，今天这篇不是软文哦。\n\n\n\n\n临床预测模型方法与应用\n\n\n非常高兴向大家推荐和我们课题组一直保持良好合作的荷兰 Utrecht University 王俊峰教授参与主编的新书《临床预测模型方法与应用》。\n王老师是临床预测模型领域内的专家，大家感兴趣的可以去看王老师的google scolar。在和王老师合作做项目的过程中，我也是收获很多，扫除了一些知识上的盲点和疑区。因而，对于关注我的同学们而言，如果有对临床预测模型感兴趣的，我也是非常推荐这本书。\n这本书由南京医科大学公共卫生学院的陈峰教授作序，主编人员都是在预测模型、生物统计领域内有着丰富经验和深刻见解的科研人员，王老师在我们合作的项目文章里也给与了我悉心的指导，北京天坛医院谷鸿秋教授也是刚刚作为一作发表了NEJM，这本书可以说是大咖云集了。\n\n\n\n序言\n\n\n\n\n\n序言\n\n\n相信很多做科研的同学，一直想找一本这个方向领域的权威且全面的中文书，这本书应该是一个不错的选择。如果是对预测模型感兴趣的小伙伴可以直接下单预定啦，也可以关注下8月份王老师在北大、复旦的讲座。这本书8月份会正式上市，目前可以扫码下图进行预定。\n\n\n\n预定"
  },
  {
    "objectID": "blog/2024/08/04/workshop_001/index.html",
    "href": "blog/2024/08/04/workshop_001/index.html",
    "title": "星球第一期workshop上线",
    "section": "",
    "text": "我们星球正式上线第一期workshop啦！\n\n\n\nworkshop\n\n\n本期workshop主题是“Statistical Methods for Analysis with Missing Data”。本期workshop将从缺失数据的概念、缺失数据的类型、缺失数据的机制、缺失数据的影响、缺失数据的处理方法等方面展开讲解，帮助大家更好地理解缺失数据的问题，掌握缺失数据的处理方法。\n\n\n\nscreenshot\n\n\n目前暂定的安排是每周一节一小时，直到本期workshop的全部内容结束。\n\n\n\nused files\n\n\n后续我们会根据星球中同学们的需求，不定时地开展更多主题的workshop，具体时间请关注我们的公众号和星球，我们会在这两个平台上发布最新的信息。"
  },
  {
    "objectID": "blog/2024/08/14/clinical_prediction_model/index.html",
    "href": "blog/2024/08/14/clinical_prediction_model/index.html",
    "title": "欢迎加入预测模型星球",
    "section": "",
    "text": "由来\n前面我们给大家推荐了这本几位非常厉害的教授老师主编的《临床预测模型方法与应用》，陆陆续续地，在各个平台上，大家都反馈已经收到了这本书，并且这本书还很大很厚，涵盖了方法学、操作、专题以及案例。\n\n\n\nfeedback\n\n\n那这时候，就有同学和我说，按照以往看书的习惯，收到书的前一俩周，还是可以翻翻，但是后面就会慢慢地放在那里，然后就不了了之了。\n所以，我们就想，能不能有一个平台，让大家一起学习这本书，一起讨论、交流，一起进步呢？\n\n\n预测模型星球\n\n\n\n知识星球\n\n\n向大家完整介绍下这个星球，也鼓励更多想要讨论交流的小伙伴进入到我的星球里面，大家一起愉快地学习。\n\n首先，你要有实体书，不然我们每周讨论学习某一个章节的时候，你可能会有点懵。\n\n购买的二维码链接在这里。\n\n知识星球的目的是和大家一起营造一个国内高质量的预测模型类研究的知识圈，让星球里的人能够受益。\n\n来到这里你会遇到一群志同道合的人，一起学习、相互交流、共同促进。这个交流圈高度专注于预测类研究领域。\n\n\n\ncontent\n\n\n\n我们会每周一起学习讨论这本书的一个章节，每周一次，每次一个小时或一个章节，讨论的内容会包括这一章的重点内容、难点、案例分析等。\n除了这本书，如果未来人数足够，我们还会不定期地邀请一些同样从事预测类研究的博士生、学者，来和大家分享他们的研究成果、经验、心得等。\n\n后续我们会根据星球中同学们的需求，不定时地开展更多主题的workshop，具体时间请关注我们的公众号和星球，我们会在这两个平台上发布最新的信息。"
  },
  {
    "objectID": "blog/2024/08/30/docker/index.html",
    "href": "blog/2024/08/30/docker/index.html",
    "title": "星球JC | 胃癌早期筛查工具",
    "section": "",
    "text": "大家好，这一期预测模型星球Journal Club的分享来自中国医科大学的徐林玉同学，分享的是2019年发表在中科院医学1区的顶级期刊Gut上，题为“Development and validation of a prediction rule for estimating gastric cancer risk in the Chinese high-risk population: a nationwide multicentre study”的研究论文。\n\n\n研究背景\n胃癌是中国第二常见的癌症，早期检测和治疗可以显著降低其死亡率。然而，由于高风险人群庞大，全面的胃镜筛查在经济和操作上都不切实际。当前，中国的国家筛查指南建议对高风险人群从40岁开始进行筛查，但由于高风险人群估计超过3亿人，全面的胃镜筛查并不可行。因此，迫切需要一种风险分层工具，作为胃镜检查前的初步筛查工具，以进一步识别真正的高风险个体；且当前中国国内尚无类似的工具。\n现有的筛查工具主要基于已知的胃癌风险因素，如萎缩性胃炎和幽门螺杆菌感染。虽然有一些方法如ABC方法在日本被开发用于预测未来胃癌的发生，但其在中国高风险人群中的适用性仍存在疑问。此外，现有的生物标志物组合方法虽然在某些研究中表现良好，但其结果可能不适用于中国的高风险人群。因此，这项全国多中心横断面研究的目标是开发一种新的预测规则，用于二级预防（早发现、早诊断、早治疗），作为初步筛查工具，用于在中国无症状人群中识别高风险个体，以便进一步进行诊断性胃镜检查。\n\n\n研究方法\n\n研究类型\n全国、多中心、横断面研究。\n\n\n研究人群\n年龄在40至80岁之间、无胃肠道症状的个体，符合中国胃癌高风险标准，并前往医院进行胃镜筛查。\n\n\n数据收集\n通过问卷调查、血清学检测（PG I、PG II、G-17、抗幽门螺杆菌IgG抗体）、胃镜检查和组织学检查收集数据。\n\n\n统计分析\n文章对数据的管理采取了中心化的管理，尽可能确保数据质量。参与者按2:1比例被随机分为开发队列和验证队列。开发队列用于模型开发，验证队列用于外部验证。文章利用Logistic回归模型开发预测规则。在开发队列中，通过univariate and multivariate analyses评估危险因素与胃癌的关联，其中，univariate analyses阈值设为p&lt;0.25，且multivariate analyses采用backward stepwise进行进一步的变量筛选。文章预测规则的设定基于regression coefficient-based scoring method（Points were assigned by dividing the regression coefficients by the absolute value of the smallest coefficient in the model and rounding up to the nearest integer）。模型整体性能通过R²和Brier评分进行衡量，区分能力通过AUC和discrimination slope评估，校准能力通过Hosmer-Lemeshow χ2统计量和calibration in the large进行评估。同时，评估模型预测的敏感度、特异度、准确率、阳性预测值（positive predictive value）、阴性预测值（negative predictive value）、阳性似然比（positive likelihood ratio）、阴性似然比（negative likelihood rario）和number needed to screen（defined as the number of participants who would need to undergo gastroscopy for one patient with GC to be identified）。文章对开发队列进行bootstrap抽样1000次作为内部验证，验证队列上进行外部验证。同时，基于u test比较模型在开发队列、验证队列上的AUC表现。此外，文章额外做了一部分工作，即将自身模型与当前现有文献中的预测模型进行效果的对比。数据分析使用IBM SPSS和R软件进行。\n\n\n\n研究结果\n此研究收集了2015年6月至2017年3月期间来自中国115家医院的数据，最终纳入14,929名合格参与者。\n\n\n\n图1 研究对象纳入及定义\n\n\n\n预测规则的开发\n基于分数的预测规则的开发：采用单因素和多因素分析，确定了包括年龄、性别、PG I/II比率、G-17水平、幽门螺杆菌感染和饮食习惯（腌制和油炸食品消费）在内的7个预测因素。\n\n\n\nTable 1\n\n\n按照0–25分评分范围进行风险分层，将个体分为低风险（≤11）、中风险（12–16）和高风险（17–25）组。\n\n\n\n图2 logistic回归模型中胃癌的预测因素及相关预测规则\n\n\n其中，中高风险组的胃镜检查检测到70.8%的胃癌病例和70.3%的早期胃癌病例。低风险组的胃镜检查需求减少了66.7%。\n\n\n\nTable 4\n\n\n预测性能：开发的预测规则具有良好的性能表现。\n\n\n\n图3 预测模型的性能指标\n\n\n\n\n与其他预测模型的性能比较\n该预测规则表现良好，并显示出显著优于其他三种替代预测方法（即Miki等人的ABC方法、中国的基于ELISA的ABC方法以及Tu等人的五种生物标志物方法）在识别胃癌患者方面的区分能力。\n\n\n\n图4 验证队列中预测模型和替代预测模型的比较\n\n\n\n\n\nTake home message\n\n该预测规则在识别中国人群中胃镜检查前的高风险个体方面表现良好。可以作为一种准确且具有成本效益的大规模初步筛查工具，以提高胃癌（包括早期胃癌）的检测率，从而改善胃癌的二级预防。\n文章不仅比较了自身模型在多中心外部验证中的效果，更是对比了现有模型，且表现均优于现有模型。\n采用回归赋分的方式，对于人工智能算法解释性较好，能较好地在医院中进行推广应用；此外，采用赋分划分亚组的方式，关注到了模型实际可能带来的收益。"
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "This page contains the source code, links, and slides for various workshops, talks, lectures, and presentations I’ve given."
  },
  {
    "objectID": "talks/index.html#section",
    "href": "talks/index.html#section",
    "title": "Talks",
    "section": "2024",
    "text": "2024"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello there!",
    "section": "",
    "text": "I am a PhD candidate in epidemiology and biostatistics at Sun Yat-sen University. I have made contributions to neurosyphilis and cancer research, and my work has been published in journals including eClinicalMedicine, JMIR Public Health and Surveillance, among others. Besides, I have patented a clinical diagnosis system for neurological syphilis (LU504466, rank 1/1).\nIn addition to my work on neurosyphilis and cancer, I have extensive experience applying advanced statistical concepts, artificial intelligence, and causal inference methods to conduct real-world research based on electronic health records. I have conducted multiple studies in the areas of COVID-19, HPV and HIV infections, and cardiovascular disease.\nTo date (May 10, 2024), I have published 25 papers in peer-reviewed journals, serving as the first author and co-first author for 10 of these."
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitæ",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "blog/2024/05/13/hierarchical_composite_endpoints/index.html",
    "href": "blog/2024/05/13/hierarchical_composite_endpoints/index.html",
    "title": "Hierarchical composite endpoints治疗效应的可视化",
    "section": "",
    "text": "复合终点\n有时，根据主要研究目的，我们很难从多个终点指标中选出其中某一个作为主要终点，此时，我们可以利用复合终点来作为主要终点。\nHierarchical composite endpoints (HCE)可以整合不同类型、不同重要性终点成一个有序终点指标，以表示患者经历的不同严重程度的终点。如，在固定随访的RCT中，outcomes of interest可以是death、hospitalization，而这两个终点存在严重程度的差异。很明显，死亡是最严重的。同样最终死亡的两个患者，生存时间更长，意味治疗效应更好；同样最终住院的两个患者，入院前时间更长，治疗效应更好；同样未住院的两个患者，某一实验室指标的change from baseline更大，效应更好。\n对于这种HCE，我们可以计算win odds(Gasparyan et al. 2021)来比较组间差异，然而，治疗效应的可视化受到复合终点的影响，不容易像单纯的生存曲线那样用合适的工具可视化出来。\n针对这一问题，AstraZeneca的Martin Karpefors等人提出了一种新的方法，即maraca plot(Karpefors, Lindholm, and Gasparyan 2023)。这种方法可以将复合终点中time to event(TTE)以及连续性终点的治疗效应可视化出来，同时也可以用来比较不同治疗组之间的差异。对应的R包可以方便地实现这一点。\n\n\nmaraca plot\nmaraca基于ggplot2，其中，对于TTE采用Kaplan-Meier曲线展示cumulative proportions，对于连续性终点可选用箱线图、violin plot以及scatter plot展示连续性分布。这种方法可以同时展示HCE的不同组成成分。\n来看一个例子。\n\nlibrary(maraca)\ndata(hce_scenario_a, package = \"maraca\")\ndata &lt;- hce_scenario_a\ndata |&gt; head()\n\n  SUBJID              GROUP GROUPN      AVAL0       AVAL    TRTP\n1      1          Outcome I      0 120.440921   120.4409  Active\n2      2 Continuous outcome  40000   3.345229 40003.3452 Control\n3      3 Continuous outcome  40000  22.802615 40022.8026  Active\n4      4          Outcome I      0 577.311386   577.3114 Control\n5      5         Outcome II  10000 781.758081 10781.7581  Active\n6      6        Outcome III  20000 985.097981 20985.0980 Control\n\n\n具体变量意义，大家可以查看?hce_scenario_a。\n可视化如下：\n\ncolumn_names &lt;- c(outcome = \"GROUP\", arm = \"TRTP\", value = \"AVAL0\")\ntte_outcomes &lt;- c(\"Outcome I\", \"Outcome II\", \"Outcome III\", \"Outcome IV\")\ncontinuous_outcome &lt;- \"Continuous outcome\"\narm_levels &lt;- c(active = \"Active\", control = \"Control\")\nmaraca_object &lt;- maraca(\n  data, tte_outcomes, continuous_outcome, arm_levels, column_names,\n  fixed_followup = 3*365, compute_win_odds = TRUE\n)\nAZ_colors &lt;- c(\"#830051\", \"#F0AB00\")\nplot(maraca_object, density_plot_type = \"default\") + theme_bw() +\n  scale_color_manual(values = AZ_colors) +\n  scale_fill_manual(values = AZ_colors)\n\n\n\n\n\n\n\n\n\n\n结果解释\n怎么看这张图？\n首先是x轴上HCE的5个组成成分，x轴上每个成分的长度大小，代表了患者达到不同成分终点的比例，可以看到，continuous outcome的比例最大，说明这个终点的患者所占比例最大。其次，cumulative percentage显示active组在四个TTE终点上是存在差异的。再然后是continuous outcome的分布，偏向x轴右侧代表change from baseline更大。而这些结合起来，就是win odds的结果，可以看到，和我们从可视化的角度看到的结果是一致的。\n代码已经放进了星球里。\n\n\n\n\n\n\n\nReferences\n\nGasparyan, S. B., E. K. Kowalewski, F. Folkvaljon, O. Bengtsson, J. Buenconsejo, J. Adler, and G. G. Koch. 2021. “Power and Sample Size Calculation for the Win Odds Test: Application to an Ordinal Endpoint in COVID-19 Trials.” Journal Article. Journal of Biopharmaceutical Statistics 31 (6): 765–87.\n\n\nKarpefors, M., D. Lindholm, and S. B. Gasparyan. 2023. “The Maraca Plot: A Novel Visualization of Hierarchical Composite Endpoints.” Journal Article. Clinical Trials (London, England) 20 (1): 84–88. https://doi.org/10.1177/17407745221134949.\n\nCitationBibTeX citation:@online{lu2024,\n  author = {Lu, Zhen},\n  title = {Hierarchical Composite {endpoints治疗效应的可视化}},\n  date = {2024-05-13},\n  url = {https://leslie-lu.github.io/blog/2024/05/13/hierarchical_composite_endpoints/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLu, Zhen. 2024. “Hierarchical Composite\nEndpoints治疗效应的可视化.” May 13, 2024. https://leslie-lu.github.io/blog/2024/05/13/hierarchical_composite_endpoints/."
  },
  {
    "objectID": "blog/2020/04/26/index.html",
    "href": "blog/2020/04/26/index.html",
    "title": "Python与矩阵",
    "section": "",
    "text": "矩阵可以被看作是排列的向量或堆放在一起的数字。矩阵的意义非常重要，它可以作用在一个具体的向量上，使向量空间位置发生变换。\n\n矩阵的Python表示\n对于矩阵而言，最直观的描述就是一个m*n的数字方阵，它可以看作是n个m维列向量从左到右并排摆放，也可以看作是m个n维行向量从上到下进行叠放。\n\n# 矩阵的表示：\nimport numpy as np\nA = np.array([[1, 2],\n              [3, 4],\n              [5, 6],\n              [7, 8]])\nprint(A)\n# 在形容矩阵的形状和规模时，一般采用其行数和列数来进行描述\nprint(A.shape)\n# 通过矩阵A的shape属性，可以获取一个表示矩阵规模的元组对象\n# 这个元组对象包含两个元素：第一个元素表示行数，第二个表示列数\n\n[[1 2]\n [3 4]\n [5 6]\n [7 8]]\n(4, 2)\n\n\n前面提到，n维的行向量可以看作是一个1*n的特殊矩阵；同理，n维的列向量也同样可以看作是一个n*1的特殊矩阵。这样，一方面，可以将矩阵和向量的Python表示方法统一起来；另一方面，在接下来要介绍的矩阵与向量的乘法运算中，可以将其看作是矩阵与矩阵之间乘法的一种特殊形式，从而统一运算方式。\n\nB=np.array([[1,2,3,4]]) #行向量；用生成矩阵的方法生成了一个1*4的矩阵，用来表示一个四维的行向量\nprint(B.shape)\nprint(B.T) #转置成列向量；因为是矩阵形式，所以可以用转置方法，得到对应的四维列向量\nC=np.array([[1],\n            [2],\n            [3],\n            [4]]) #列向量\nprint(C)\nprint(C.shape)\n\n(1, 4)\n[[1]\n [2]\n [3]\n [4]]\n[[1]\n [2]\n [3]\n [4]]\n(4, 1)\n\n\n\n\n特殊形态的矩阵\n\n方阵：行数和列数相等的一类矩阵，称为方阵，其行数或列数称为它的阶数。\n\n\nA = np.array([[1, 1, 1, 1],\n              [2, 2, 2, 2],\n              [3, 3, 3, 3],\n              [4, 4, 4, 4]]) #4阶方阵\nprint(A)\nprint(A.shape)\n\n[[1 1 1 1]\n [2 2 2 2]\n [3 3 3 3]\n [4 4 4 4]]\n(4, 4)\n\n\n\n对称矩阵：开始介绍对称矩阵之前，先说明一下矩阵转置的概念。\n对于指定的矩阵，若将其行和列上的元素进行位置互换，即原行元素作新列，原列元素作新行，即可得到一个全新的矩阵，这个新矩阵称为原矩阵的转置矩阵，行和列互换的矩阵操作就称为矩阵的转置。\n若原矩阵和转置后新得到的矩阵相等，那么将原矩阵称为对称矩阵。由此可见，矩阵对称的前提条件是该矩阵首先必须是一个方阵；此外，对称矩阵的一个典型特征为：沿着从左上到右下的对角线，关于这条对角线相互对称的元素都是彼此相等的。\n可以说，对称矩阵是最重要的矩阵，在矩阵的相关分析中扮演极其重要的角色。\n\n\n# 矩阵的转置：\nimport numpy as np\nA = np.array([[1, 2, 3, 4],\n              [5, 6, 7, 8]])\nprint(A)\nprint(A.T)\n# 对称矩阵：\nimport numpy as np\nS = np.array([[1, 2, 3, 4],\n              [2, 5, 6, 7],\n              [3, 6, 8, 9],\n              [4, 7, 9, 0]])\nprint(S)\nprint(S.T)\n\n[[1 2 3 4]\n [5 6 7 8]]\n[[1 5]\n [2 6]\n [3 7]\n [4 8]]\n[[1 2 3 4]\n [2 5 6 7]\n [3 6 8 9]\n [4 7 9 0]]\n[[1 2 3 4]\n [2 5 6 7]\n [3 6 8 9]\n [4 7 9 0]]\n\n\n\n零矩阵：所有元素均为0的矩阵，称为零矩阵。\n\n\nA = np.zeros([5, 3])\nprint(A)\n\n[[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]]\n\n\n\n对角矩阵：若方阵在非对角线位置上元素全部为0，则称为对角矩阵，0元素的位置可以省去不写。\n\n\nA = np.diag([1, 2, 3, 4, 5])\nprint(A)\nprint(A.T)\n\n[[1 0 0 0 0]\n [0 2 0 0 0]\n [0 0 3 0 0]\n [0 0 0 4 0]\n [0 0 0 0 5]]\n[[1 0 0 0 0]\n [0 2 0 0 0]\n [0 0 3 0 0]\n [0 0 0 4 0]\n [0 0 0 0 5]]\n\n\n\n单位矩阵/单位阵：对角位置上元素全部为1，其余位置元素均为0的特殊对角矩阵，称为单位矩阵。\n\n\nI = np.eye(5)\nprint(I)\n\n[[1. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0.]\n [0. 0. 1. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 1.]]"
  },
  {
    "objectID": "blog/2024/06/07/loss_function/index.html",
    "href": "blog/2024/06/07/loss_function/index.html",
    "title": "常用损失函数",
    "section": "",
    "text": "loss function\n在机器学习/深度学习任务中，衡量模型预测值与真实值之间的差异的指标称为损失函数。损失函数是模型训练的关键组成部分，它可以帮助我们优化模型参数，使得模型的预测值更加接近真实值。预测任务的目标也是最小化损失函数，如，我们利用反向传播算法等方法，通过更新损失函数相对于模型参数的梯度来最小化损失函数，提高模型的预测能力。此外，有效的损失函数还可以帮助我们平衡模型的偏差和方差，提高模型的泛化能力。\n依据预测任务的不同，损失函数可以分为回归任务和分类任务两大类。回归任务的损失函数通常是均方误差（MSE）或平均绝对误差（MAE），而分类任务的损失函数则有交叉熵损失函数、Hinge损失函数等。本文将介绍常用的损失函数及其应用场景。\n\n均方误差（MSE）\n均方误差（Mean Squared Error，MSE）是回归任务中最常用的损失函数之一，它衡量模型预测值与真实值之间的差异。MSE的计算公式如下：\n\\[\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n\\]\n可以看到，MSE是预测值与真实值之间差值的平方和的均值，它对较大差异分配更高的惩罚。MSE非负，越小，说明模型的预测值与真实值之间的差异越小，模型的预测能力越好。MSE对异常值敏感，因为它是差值的平方和，异常值的平方会放大差异，导致模型的预测能力下降。\n其在pytorch中的实现：\n\ntorch.nn.MSELoss(reduction='mean')\n\n\n\n平均绝对误差（MAE）\n平均绝对误差（Mean Absolute Error，MAE）是回归任务中另一种常用的损失函数。MAE的计算公式如下：\n\\[\nMAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n\\]\n相比于MSE，MAE是预测值与真实值之间差值的绝对值的均值，它对异常值不敏感，因为它是差值的绝对值的和，不会对某一异常值的差异分配过高的权重。MAE的值越小，说明模型的预测值与真实值之间的差异越小，模型的预测能力越好。\n针对MAE和MSE的优缺点，我们可以根据具体的任务需求选择合适的损失函数。如果任务需要重点关注异常值，可以选择MSE，否则选择MAE。\n\ntorch.nn.L1Loss(reduction='mean')\n\n\n\nHuber loss\nHuber loss是一种结合了MSE和MAE的损失函数，它在差值较小的情况下使用MSE，差值较大的情况下使用MAE。Huber loss的计算公式如下：\n\\[\nL_{\\delta}(y, \\hat{y}) = \\left\\{\n\\begin{array}{ll}\n\\frac{1}{2}(y - \\hat{y})^2, & \\text{if } |y - \\hat{y}| \\leq \\delta \\\\\n\\delta |y - \\hat{y}| - \\frac{1}{2} \\delta^2, & \\text{otherwise}\n\\end{array}\n\\right.\n\\]\n其中，\\(\\delta\\)是一个超参数，用于控制MSE和MAE之间的平衡。Huber loss对异常值不敏感，同时保留了MSE的平滑性，是一种较为稳健的损失函数。\n\ntorch.nn.SmoothL1Loss(reduction='mean')\n\n\n\n二元交叉熵损失函数（Binary Cross Entropy Loss）\n交叉熵损失函数（Cross Entropy Loss）是二分类任务中最常用的损失函数之一，我们前面也以及介绍过。交叉熵损失函数的计算公式如下：\n\\[\nL(y, \\hat{y}) = -\\frac{1}{n} \\sum_{i=1}^{n} y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)\n\\]\n其中，\\(y_i\\)是真实标签，\\(\\hat{y}_i\\)是模型预测的概率值。交叉熵损失函数对于模型预测的概率值和真实标签之间的差异进行了惩罚，使得模型更加关注预测正确的类别。交叉熵损失函数是一种凸函数，可以通过梯度下降等方法进行优化。\n\ntorch.nn.BCELoss(weight=None, reduction='mean')\n\n\n\n多类交叉熵损失函数（Categorical Cross Entropy Loss）\n多类交叉熵损失函数是多分类任务中常用的损失函数之一，它是交叉熵损失函数的扩展。多类交叉熵损失函数的计算公式如下：\n\\[\nL(y, \\hat{y}) = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{m} y_{ij} \\log(\\hat{y}_{ij})\n\\]\n其中，\\(y_{ij}\\)是真实标签，\\(\\hat{y}_{ij}\\)是模型预测的概率值。\n\ntorch.nn.CrossEntropyLoss(weight=None,ignore_index=-100, reduction='mean')\n\n\n\nHinge损失函数\nHinge损失函数是支持向量机（SVM）中常用的损失函数之一，它适用于二分类任务。Hinge损失函数的计算公式如下：\n\\[\nL(y, \\hat{y}) = \\max(0, 1 - y \\cdot \\hat{y})\n\\]\nHinge损失函数旨在最大化决策边界的间隔，即使得正确分类的样本距离决策边界的距离尽可能大。Hinge损失函数对于误分类的样本进行了惩罚，使得模型更加关注分类边界附近的样本，从而尽可能把数据点推向远离决策边界的方向。\n代码已经放进了星球里。"
  },
  {
    "objectID": "blog/2020/03/11/index.html",
    "href": "blog/2020/03/11/index.html",
    "title": "Python与向量",
    "section": "",
    "text": "Python中一般使用numpy库生成一个向量，但其默认生成的是行向量。\n\nimport numpy as np\n# 行向量\na= np.array([1,2,3,4])\nprint(a)\n\n[1 2 3 4]\n\n\n但我们一般使用列向量的形式，因此需要对其做一些处理。有人想，转置处理就可以了，也就是把向量的行索引和列索隐交换位置。但是numpy重的转置方法对于一维数组是无效的：\n\na= np.array([1,2,3,4])\nprint(a.transpose()) #从程序的运行结果来看，确实是无效的\n\n[1 2 3 4]\n\n\n应该如何表示一个列向量呢？\n\nA= np.array([1,2,3,4])\nA_t= A[:, np.newaxis] #增加一个维度\nprint(A_t)\nprint(A_t.shape) #列向量本身就是二维表示的\n\n[[1]\n [2]\n [3]\n [4]]\n(4, 1)\n\n\n这种做法比较复杂，更直观更简单的实现方法是：显然，我们一直把向量看作是一个维数为1的数组，其实也可以看作是行数为1或列数为1的一个二维数组。而二维数组对应的就是矩阵，因此向量还可以看作是一个特殊的矩阵，即可以把行向量看作是一个1m的特殊矩阵，可以把列向量看作是一个n1的特殊矩阵。\n\n# 在对行向量进行初始化时，使用了numpy中的二维数组的初始化方法，因此在语句中多嵌套了一层中括号\nA= np.array([[1,2,3,4]])\nprint(A)\nprint(A.T) #此时可以直接通过行向量转置的方法生成对应的列向量\n\n[[1 2 3 4]]\n[[1]\n [2]\n [3]\n [4]]\n\n\n\n向量的加法\n两个维数相同的向量才能进行加法运算，只要将相同位置上的元素对应相加即可，结果向量的维数保持不变。\n\nu= np.array([[1,2,3]]).T\nu\nv= np.array([[5,6,7]]).T\nprint(u+v)\n\n[[ 6]\n [ 8]\n [10]]\n\n\n\n\n向量的数乘\n向量的数乘就是将参与乘法运算的标量同向量的每个元素分别相乘，以此得到最终的结果向量，结果向量的维数依然保持不变。从几何意义上来看，向量的数乘就是将向量沿着所在直线的方向拉伸相应的倍数，拉伸方向和参与运算的标量符号一致。\n\nu= np.array([[1,2,3]]).T\nprint(3*u)\n\n[[3]\n [6]\n [9]]\n\n\n\n\n向量间的乘法：内积和外积\n向量间的乘法分为内积和外积两种形式。\n向量的内积运算：参与内积运算的两个向量必须维数相等，运算规则是先将对应位置上的元素相乘，然后合并相加，最终运算结果是一个标量。可能这样说不太好理解，要是从几何表示上看，内积的意义就非常清晰了。\n内积的几何表示u*v= |u||v|cosθ，它表示向量u在向量v方向上的投影长度乘以向量v的模长。需要注意的是，在实际运算向量内积时，无论是行向量间的内积还是列向量间的内积，最终的运算结果都是一样的。\n\nu= np.array([3,5,2])\nv= np.array([1,4,7])\n# 若使用numpy库中的内积运算函数dot进行运算，传入的参数必须是用一维数组表示的行向量\nprint(np.dot(u,v)) #一维乘一维，结果还是一维\n\nuC= u[:,np.newaxis]\nvC= v[:,np.newaxis]\nprint(uC)\nprint(vC)\nprint(np.dot(uC,vC)) #行数为1的二维数组，即二维数组形式的行向量，报错\n\nu= np.array([[3,5,2]])\nv= np.array([[1,4,7]])\nprint(np.dot(u,v)) #列数为1的二维数组，即二维数组形式的行向量，报错\n\nu= np.array([[3,5,2]]).T\nv= np.array([[1,4,7]]).T\nprint(np.dot(u,v)) #列数为1的二维数组，即二维数组形式的列向量，报错\n\n二维数组形式的向量怎么进行内积运算呢？我们知道二维数组表示下得向量的本质上是矩阵，只不过是行数或列数为1的特殊矩阵。若将这种表示方法下的向量作为传入内积运算函数dot的参数，就需要依据矩阵的乘法法则来计算。\n\nu= np.array([[3,5,2]])\nv= np.array([[1,4,7]]).T\nprint(np.dot(u,v)) #二维乘二维，结果还是二维\n\n[[37]]\n\n\n向量的外积运算：这里只讨论在二维平面和三维空间中的运算情况。\n在二维平面中，与内积类似，外积也有一种表达式：u*v= |u||v|sinθ。在二维平面中，向量的外积表示两个向量张成的平行四边形的“面积”。当然，这个面积要打上引号，因为若两个向量的夹角大于180度，那么向量外积运算所得到的结果为负。\n\nu= np.array([3,5])\nv= np.array([1,4])\nprint(np.cross(u,v)) #一维乘一维，结果还是一维\n\n7\n\n\n/tmp/ipykernel_6777/2827165229.py:3: DeprecationWarning: Arrays of 2-dimensional vectors are deprecated. Use arrays of 3-dimensional vectors instead. (deprecated in NumPy 2.0)\n  print(np.cross(u,v)) #一维乘一维，结果还是一维\n\n\n而在三维空间中，外积要复杂一点，其计算所得到的结果是一个向量而不是一个数值。其最终得到的结果向量也是有明确的物理含义的，即表示u和v两个向量张成平面的法向量。\n\nx= np.array([3,3,9])\ny= np.array([1,4,12])\nprint(np.cross(x,y))\n\n[  0 -27   9]\n\n\n\n\n向量先数乘后叠加：向量的线性组合\n基于向量加法和数量乘法这两类基本运算，将其进行组合应用。针对向量u和v，先求出标量c和向量u的数量积，再求出标量d和向量v的数量积，最后再将二者进行叠加，就得到向量u和v的线性组合cu+dv，这里的标量c和d可以取任意值，包括0。\n\nu= np.array([[1,2,3]]).T\nv= np.array([[4,5,6]]).T\nw= np.array([[7,8,9]]).T\nprint(3*u + 4*v + 5*w)\nprint(u)\nu.shape\nnp.array([[1,2,3]]).shape\nprint(np.dot(u.T,v.T)) #报错，带有第二个维度无法进行向量内积\n\n进一步思考：我们知道，两个向量相加，在几何上就是将两个向量首尾依次连接，所得到的结果向量就是连接最初的起点和最终的终点的有向连线。我们假定有3个非零的三维列向量u、v和w，讨论以下几种不同的线性组合情况：\n\n第一种情况：cu的所有线性组合构成的图像\n由于标量c可以取0，因而cu的所有线性组合构成的图像可以表示为三维空间中一条穿过原点(0,0,0)的直线，包括原点本身。\n第二种情况：cu+dv的所有线性组合构成的图像\n\n当向量u和向量v不在一条直线上时，即u和v不共线\ncu+dv的所有线性组合构成的图像可以表示为三维空间中的一个通过原点(0,0,0)的二维平面。\n当向量u和向量v处在一条直线上时\n则cu+dv的所有线性组合构成的图像可以表示为三维空间中一条穿过原点(0,0,0)的直线，包括原点本身。\n\n第三种情况：cu+dv+ew的所有线性组合构成的图像\n\n当向量u、v、w不在一个平面上时，cu+dv+ew的所有线性组合构成的图像是整个三维空间。\n当向量u、v、w处在一个平面上时，cu+dv+ew的所有线性组合构成的图像是三维空间中的一个通过原点(0,0,0)的二维平面。\n当向量u、v、w处在一条直线上时，cu+dv+ew的所有线性组合构成的图像是三维空间中一条穿过原点(0,0,0)的直线，包括原点本身。\n\n\n我们发现，在讨论上述线性组合的多种不同情况时，均反复提到了共线、共面的概念。这些特殊的性质会对一组向量线性组合所得到的结果向量在空间中的位置产生重要影响，它们构成了线性代数中非常重要的概念。后面我们也将使用更加专业的词汇对其进行描述和介绍，即线性相关和线性无关。"
  }
]