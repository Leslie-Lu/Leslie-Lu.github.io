[
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "I have made contributions to neurosyphilis and cancer research, and my work has been published in journals including eClinicalMedicine, JMIR Public Health and Surveillance, among others. Besides, I have patented a clinical diagnosis system for neurological syphilis (LU504466, rank 1/1).\nIn addition to my work on neurosyphilis and cancer, I have extensive experience applying advanced statistical concepts, artificial intelligence, and causal inference methods to conduct real-world research based on electronic health records. I have conducted multiple studies in the areas of COVID-19, HPV and HIV infections, and cardiovascular disease."
  },
  {
    "objectID": "publications/index.html#journal-articles",
    "href": "publications/index.html#journal-articles",
    "title": "Publications",
    "section": "Journal articles",
    "text": "Journal articles"
  },
  {
    "objectID": "publications/index.html#working-papers",
    "href": "publications/index.html#working-papers",
    "title": "Publications",
    "section": "Working papers",
    "text": "Working papers"
  },
  {
    "objectID": "publications/index.html#book-chapters",
    "href": "publications/index.html#book-chapters",
    "title": "Publications",
    "section": "Book chapters",
    "text": "Book chapters"
  },
  {
    "objectID": "publications/index.html#reviews",
    "href": "publications/index.html#reviews",
    "title": "Publications",
    "section": "Reviews",
    "text": "Reviews"
  },
  {
    "objectID": "publications/index.html#selected-seminar-papers",
    "href": "publications/index.html#selected-seminar-papers",
    "title": "Publications",
    "section": "Selected seminar papers",
    "text": "Selected seminar papers"
  },
  {
    "objectID": "publications/index.html#translations",
    "href": "publications/index.html#translations",
    "title": "Publications",
    "section": "Translations",
    "text": "Translations"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello there!",
    "section": "",
    "text": "I am a PhD candidate in epidemiology and biostatistics at Sun Yat-sen University. I have made contributions to neurosyphilis and cancer research, and my work has been published in journals including eClinicalMedicine, JMIR Public Health and Surveillance, among others. Besides, I have patented a clinical diagnosis system for neurological syphilis (LU504466, rank 1/1).\nIn addition to my work on neurosyphilis and cancer, I have extensive experience applying advanced statistical concepts, artificial intelligence, and causal inference methods to conduct real-world research based on electronic health records. I have conducted multiple studies in the areas of COVID-19, HPV and HIV infections, and cardiovascular disease.\nTo date (May 10, 2024), I have published 25 papers in peer-reviewed journals, serving as the first author and co-first author for 10 of these."
  },
  {
    "objectID": "blog/index.html#section",
    "href": "blog/index.html#section",
    "title": "Blog",
    "section": "2024",
    "text": "2024\n\n\n    \n    \n                  \n            June 7, 2024\n        \n        \n            常用损失函数\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    machine learning\n                \n                \n                \n                    deep learning\n                \n                \n                \n                    loss function\n                \n                \n            \n            \n\n            常用损失函数介绍\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            May 17, 2024\n        \n        \n            Python中机器学习模型的校准\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    machine learning\n                \n                \n                \n                    calibration\n                \n                \n            \n            \n\n            预测模型如何校准\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            May 13, 2024\n        \n        \n            Hierarchical composite endpoints治疗效应的可视化\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    clinical trial\n                \n                \n                \n                    endpoint\n                \n                \n            \n            \n\n            复合终点治疗效应的可视化\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            May 11, 2024\n        \n        \n            最优分类阈值\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    machine learning\n                \n                \n            \n            \n\n            分类问题中阈值的选择\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-1",
    "href": "blog/index.html#section-1",
    "title": "Blog",
    "section": "2023",
    "text": "2023"
  },
  {
    "objectID": "blog/index.html#section-2",
    "href": "blog/index.html#section-2",
    "title": "Blog",
    "section": "2022",
    "text": "2022"
  },
  {
    "objectID": "blog/index.html#section-3",
    "href": "blog/index.html#section-3",
    "title": "Blog",
    "section": "2021",
    "text": "2021"
  },
  {
    "objectID": "blog/index.html#section-4",
    "href": "blog/index.html#section-4",
    "title": "Blog",
    "section": "2020",
    "text": "2020"
  },
  {
    "objectID": "blog/index.html#section-5",
    "href": "blog/index.html#section-5",
    "title": "Blog",
    "section": "2019",
    "text": "2019"
  },
  {
    "objectID": "blog/index.html#section-6",
    "href": "blog/index.html#section-6",
    "title": "Blog",
    "section": "2018",
    "text": "2018"
  },
  {
    "objectID": "blog/index.html#section-7",
    "href": "blog/index.html#section-7",
    "title": "Blog",
    "section": "2017",
    "text": "2017"
  },
  {
    "objectID": "blog/index.html#section-8",
    "href": "blog/index.html#section-8",
    "title": "Blog",
    "section": "2016",
    "text": "2016"
  },
  {
    "objectID": "blog/2024/05/17/calibration/index.html",
    "href": "blog/2024/05/17/calibration/index.html",
    "title": "Python中机器学习模型的校准",
    "section": "",
    "text": "calibration\n在我们利用机器学习模型来建模分类预测时，首要关注的指标能力当然是dircrimination，即模型的预测区分能力。常见的指标有sensitivity、specificity、AUROC等。我们在上一篇文章中介绍了如何选择最优分类阈值，这里我们接着介绍在选择了最优阈值后，如何评估模型的校准能力。\n所谓校准能力，即模型预测的概率与实际发生的概率一致。\n通俗来解释这个事情：比如说，我们模型预测某个病人患病的概率是0.8，那么，按照概率定义理解，模型预测概率为0.8时，100个人中应该有80个人最终患病，这个结果体现了模型的校准能力和稳定性。如果模型预测概率为0.8时，实际只有20个人患病，那么，模型的校准能力就不够好，你也不会信任这个模型在实际应用中的预测结果。这就是校准能力的重要性，即你的模型最终输出的概率值要准确反映出事件实际发生的概率。\n\n\n如何评价calibration\n\ncalibration plot\n\n\n\ncalibration curve\n\n\n上图是一个典型的calibration curve，也是我们在文章中常见的图。\n我们将模型预测概率cut或者quantile成5或者10个区间（bin），每个区间预测概率的均值作为x轴，每个区间实际发生的概率作为y轴，然后画出来这个曲线。这个图是评价模型校准能力的一个直观指标。python中可以轻松实现这个工作：\n\nfrom sklearn.calibration import calibration_curve\ny_means, pred_means = calibration_curve(y_true, y_pred, n_bins=10,strategy)\n\n理想情况下，所有点都在对角线上，即模型预测的概率与实际发生的概率完全一致。如果点在对角线上方，说明模型低估，反之，高估。\ncalibration level的定义有：\n\n\n\ncalibration level(Alonzo 2009)\n\n\n\n\n其他指标\n除了calibration plot，我们还可以用其他指标来评价模型的校准能力，比如说Brier score、Hosmer-Lemeshow test、calibration in the large等。这里不做详细介绍。\n我们感兴趣的是，当我们通过上述方法评价了模型的校准能力后，如果发现模型的校准能力不够好，我们应该怎么办？\n\n\n\ncalibrate model\n我们已经发现，模型输出值并不能代表概率。python中一般有predict_proba方法，即这个方法其实并不能保证输出的概率是真实的概率。\n\nfrom sklearn.ensemble import RandomForestClassifier\nmodel= RandomForestClassifier().fit(X_train, y_train)\ny_pred = model.predict_proba(X_test)[:,1]\n\n所以，我们需要对模型进行校准。\n\nPlatt scaling\nPlatt scaling是一种常见的校准方法，其原理是对模型输出的概率以及真实标签，用一个logistic regression模型来拟合，从而实现对模型输出的概率进行校准，拿到最终的概率。\n\nfrom sklearn.calibration import CalibratedClassifierCV\ncalibrated = CalibratedClassifierCV(model, method='sigmoid', cv=5)\ncalibrated.fit(X_train, y_train)\n\n\n\nIsotonic regression\nIsotonic regression是另一种校准方法，其原理是对模型输出的概率以及真实标签，用一个isotonic regression模型来拟合，从而实现对模型输出的概率进行校准。\n\nfrom sklearn.isotonic import IsotonicRegression\nir = IsotonicRegression().fit(y_pred, y_test)\n\n\n\nbayesian binning into quantiles\nBBQ是一种基于贝叶斯的校准方法，其原理是将预测概率分成若干个区间，然后在每个区间内对概率进行校准。该方法结合了分箱（binning）和贝叶斯推断的优点，可以在样本量较小时仍然保持较好的校准效果。\n还有其他方法可以供尝试。\n\n\n\ntake home message\n在利用机器学习模型进行分类预测时，我们不可忽视模型的校准能力。\n代码已经放进了星球里。\n\nDid you find this page helpful? Consider sharing it 🙌\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nAlonzo, T. A. 2009. “Clinical Prediction Models: A Practical Approach to Development, Validation, and Updating: By Ewout w. Steyerberg.” Generic. Oxford University Press.\n\nCitationBibTeX citation:@online{lu2024,\n  author = {Lu, Zhen},\n  title = {Python中机器学习模型的校准},\n  date = {2024-05-17},\n  url = {https://leslie-lu.github.io//blog/2024/05/17/calibration},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLu, Zhen. 2024. “Python中机器学习模型的校准.” May 17, 2024.\nhttps://leslie-lu.github.io//blog/2024/05/17/calibration."
  },
  {
    "objectID": "blog/2024/05/11/optimal_threshold/index.html",
    "href": "blog/2024/05/11/optimal_threshold/index.html",
    "title": "最优分类阈值",
    "section": "",
    "text": "这里我们借助scikit-learn来探讨分类问题中阈值的选择。\n\n数据准备和参数选择\n首先是数据准备：\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nnp.set_printoptions(suppress=True, precision=8, linewidth=1000)\npd.options.mode.chained_assignment = None\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\n\ndata = load_breast_cancer()\nX = data[\"data\"]\ny = data[\"target\"]\n\nXtrain, Xvalid, ytrain, yvalid = train_test_split(X, y, test_size=.20, random_state=516)\n\nprint(f\"Xtrain.shape: {Xtrain.shape}\")\nprint(f\"Xvalid.shape: {Xvalid.shape}\")\n\nXtrain.shape: (455, 30)\nXvalid.shape: (114, 30)\n\n\n模型我们这里选择随机森林。超参的选择，基于GridSearchCV，这里也不赘述。有一个点需要说明，由于使用的是肿瘤数据集，在这种情况下，我们更关注的是recall，即尽量减少假阴性的情况。因而，我们在训练模型时，也是将recall作为评价指标。\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    \"n_estimators\": [100, 150, 250],\n    \"min_samples_leaf\": [2, 3, 4],\n    \"ccp_alpha\": [0, .1, .2, .3]\n    }\n\nmdl = GridSearchCV(\n    RandomForestClassifier(random_state=516), \n    param_grid, \n    scoring=\"recall\", \n    cv=5\n    )\n\nmdl.fit(Xtrain, ytrain)\n\nprint(f\"best parameters: {mdl.best_params_}\")\n\nbest parameters: {'ccp_alpha': 0, 'min_samples_leaf': 4, 'n_estimators': 100}\n\n\n\n\n模型预测\n拿到模型后，自然我们可以开始预测：\n\nypred = mdl.predict_proba(Xvalid)[:,1]\nypred\n\narray([0.005     , 0.82743637, 0.97088095, 0.        , 0.        , 1.        , 0.98020202, 0.67380556, 0.        , 0.99333333, 0.9975    , 0.30048576, 0.9528113 , 0.99666667, 0.04102381, 0.99444444, 1.        , 0.828226  , 0.        , 0.        , 0.97916667, 1.        , 0.99607143, 0.90425163, 0.        , 0.02844156, 0.99333333, 0.98183333, 0.9975    , 0.08869769, 0.97369841, 0.        , 1.        , 0.71100866, 0.96022727, 0.        , 0.71200885, 0.06103175, 0.005     , 0.99490476, 0.1644127 , 0.        , 0.23646934, 1.        , 0.57680164, 0.64901715, 0.9975    , 0.61790818, 0.95509668, 0.99383333, 0.04570455, 0.97575758, 1.        , 0.47115815, 0.92422619, 0.77371415, 0.        , 1.        , 0.26198657, 0.        , 0.28206638, 0.95216162, 0.98761905, 0.99464286, 0.98704762, 0.85579351, 0.10036905, 0.00222222, 0.98011905, 0.99857143, 0.92285967, 0.95180556, 0.97546947, 0.84433189, 0.005     , 0.99833333, 0.83616339, 1.        , 0.9955    , 1.        , 0.99833333, 1.        ,\n       0.86399315, 0.9807381 , 0.        , 0.99833333, 0.9975    , 0.        , 0.98733333, 0.96822727, 0.23980827, 0.7914127 , 0.        , 0.98133333, 1.        , 1.        , 0.89251019, 0.9498226 , 0.18943254, 0.83494391, 0.9975    , 1.        , 0.77079113, 0.99722222, 0.30208297, 1.        , 0.92111977, 0.99428571, 0.91936508, 0.47118074, 0.98467172, 0.006     , 0.05750305, 0.96954978])\n\n\n这个时候，我们要讲的东西就来了。一般地，我们会选择0.50作为分类阈值，即大于0.50的为正类，小于0.50的为负类。\n\nypred = mdl.predict_proba(Xvalid)[:,1].reshape(-1, 1)\nyhat = mdl.predict(Xvalid).reshape(-1, 1)\npreds = np.concatenate([ypred, yhat], axis=1)\nprint(preds)\nprint(confusion_matrix(yvalid, yhat))\n\n[[0.005      0.        ]\n [0.82743637 1.        ]\n [0.97088095 1.        ]\n [0.         0.        ]\n [0.         0.        ]\n [1.         1.        ]\n [0.98020202 1.        ]\n [0.67380556 1.        ]\n [0.         0.        ]\n [0.99333333 1.        ]\n [0.9975     1.        ]\n [0.30048576 0.        ]\n [0.9528113  1.        ]\n [0.99666667 1.        ]\n [0.04102381 0.        ]\n [0.99444444 1.        ]\n [1.         1.        ]\n [0.828226   1.        ]\n [0.         0.        ]\n [0.         0.        ]\n [0.97916667 1.        ]\n [1.         1.        ]\n [0.99607143 1.        ]\n [0.90425163 1.        ]\n [0.         0.        ]\n [0.02844156 0.        ]\n [0.99333333 1.        ]\n [0.98183333 1.        ]\n [0.9975     1.        ]\n [0.08869769 0.        ]\n [0.97369841 1.        ]\n [0.         0.        ]\n [1.         1.        ]\n [0.71100866 1.        ]\n [0.96022727 1.        ]\n [0.         0.        ]\n [0.71200885 1.        ]\n [0.06103175 0.        ]\n [0.005      0.        ]\n [0.99490476 1.        ]\n [0.1644127  0.        ]\n [0.         0.        ]\n [0.23646934 0.        ]\n [1.         1.        ]\n [0.57680164 1.        ]\n [0.64901715 1.        ]\n [0.9975     1.        ]\n [0.61790818 1.        ]\n [0.95509668 1.        ]\n [0.99383333 1.        ]\n [0.04570455 0.        ]\n [0.97575758 1.        ]\n [1.         1.        ]\n [0.47115815 0.        ]\n [0.92422619 1.        ]\n [0.77371415 1.        ]\n [0.         0.        ]\n [1.         1.        ]\n [0.26198657 0.        ]\n [0.         0.        ]\n [0.28206638 0.        ]\n [0.95216162 1.        ]\n [0.98761905 1.        ]\n [0.99464286 1.        ]\n [0.98704762 1.        ]\n [0.85579351 1.        ]\n [0.10036905 0.        ]\n [0.00222222 0.        ]\n [0.98011905 1.        ]\n [0.99857143 1.        ]\n [0.92285967 1.        ]\n [0.95180556 1.        ]\n [0.97546947 1.        ]\n [0.84433189 1.        ]\n [0.005      0.        ]\n [0.99833333 1.        ]\n [0.83616339 1.        ]\n [1.         1.        ]\n [0.9955     1.        ]\n [1.         1.        ]\n [0.99833333 1.        ]\n [1.         1.        ]\n [0.86399315 1.        ]\n [0.9807381  1.        ]\n [0.         0.        ]\n [0.99833333 1.        ]\n [0.9975     1.        ]\n [0.         0.        ]\n [0.98733333 1.        ]\n [0.96822727 1.        ]\n [0.23980827 0.        ]\n [0.7914127  1.        ]\n [0.         0.        ]\n [0.98133333 1.        ]\n [1.         1.        ]\n [1.         1.        ]\n [0.89251019 1.        ]\n [0.9498226  1.        ]\n [0.18943254 0.        ]\n [0.83494391 1.        ]\n [0.9975     1.        ]\n [1.         1.        ]\n [0.77079113 1.        ]\n [0.99722222 1.        ]\n [0.30208297 0.        ]\n [1.         1.        ]\n [0.92111977 1.        ]\n [0.99428571 1.        ]\n [0.91936508 1.        ]\n [0.47118074 0.        ]\n [0.98467172 1.        ]\n [0.006      0.        ]\n [0.05750305 0.        ]\n [0.96954978 1.        ]]\n[[35  3]\n [ 1 75]]\n\n\n但是，这个阈值是可以调整的。我们可以通过调整阈值来达到不同的目的。比如，我们可以通过调整阈值来减少假阴性的情况，这在类别不平衡时尤为重要。\n\n\n阈值的选择\n我们介绍几种常用的方法。\n\n1. 阳性类别prevalance\n我们看下这个数据集中阳性类别的比例：\n\nprint(f\"Proportion of positives in training set: {ytrain.sum() / ytrain.shape[0]:.2f}\")\n\nProportion of positives in training set: 0.62\n\n\n这个toy数据集很夸张哈，达到了0.62。在实际应用中，这个比例可能只有10%或者1%。这里我们只是拿它示例哈，用这个prevalance来作为阈值。\n\nthresh = 1- ytrain.sum() / ytrain.shape[0]\nyhat = np.where(ypred &lt;= thresh, 0, 1)\nprint(confusion_matrix(yvalid, yhat))\n\n[[34  4]\n [ 0 76]]\n\n\n考虑prevalance的方法，可以在类别不平衡的情况下，减少假阴性的情况。\n\n\n2. 最优F1指数\nF1指数是precision和recall的调和平均数。我们可以通过最大F1指数来选择最优的阈值。\n\n\nThreshold using optimal f1-score: 0.471.\n\n\nF1最高为0.471，我们采用它来进行预测：\n\nthresh = .471\nyhat = np.where(ypred &lt;= thresh, 0, 1)\nprint(confusion_matrix(yvalid, yhat))\n\n[[34  4]\n [ 0 76]]\n\n\n\n\n3. ROC曲线\n我们可以通过ROC曲线来选择最优的阈值。ROC曲线下的面积AUC越大，说明模型越好。我们可以选择ROC曲线最靠近左上角的点作为最优阈值。\n\n\n\n\n\n\n\n\n\n\n\n4. PRC曲线\nPRC曲线是precision-recall曲线。相比于ROC曲线，PRC曲线更适合类别不平衡的情况。我们主要选择PRC曲线最靠近右上角的点作为最优阈值。\n\n\n\n\n\n\n\n\n\nSelected threshold using precision-recall curve: 0.674.\n\n\n\n\n5. 分别关注precision和recall\n我们可以通过调整阈值来分别关注precision和recall。比如，我们可以通过调整阈值来提高recall，减少假阴性的情况。\n\n\n\n\n\n\n\n\n\n\n代码已经放进了星球里。\n\n\nDid you find this page helpful? Consider sharing it 🙌\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{lu2024,\n  author = {Lu, Zhen},\n  title = {最优分类阈值},\n  date = {2024-05-11},\n  url = {https://leslie-lu.github.io//blog/2024/05/11/optimal_threshold},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLu, Zhen. 2024. “最优分类阈值.” May 11, 2024. https://leslie-lu.github.io//blog/2024/05/11/optimal_threshold."
  },
  {
    "objectID": "blog/2024/05/13/hierarchical_composite_endpoints/index.html",
    "href": "blog/2024/05/13/hierarchical_composite_endpoints/index.html",
    "title": "Hierarchical composite endpoints治疗效应的可视化",
    "section": "",
    "text": "复合终点\n有时，根据主要研究目的，我们很难从多个终点指标中选出其中某一个作为主要终点，此时，我们可以利用复合终点来作为主要终点。\nHierarchical composite endpoints (HCE)可以整合不同类型、不同重要性终点成一个有序终点指标，以表示患者经历的不同严重程度的终点。如，在固定随访的RCT中，outcomes of interest可以是death、hospitalization，而这两个终点存在严重程度的差异。很明显，死亡是最严重的。同样最终死亡的两个患者，生存时间更长，意味治疗效应更好；同样最终住院的两个患者，入院前时间更长，治疗效应更好；同样未住院的两个患者，某一实验室指标的change from baseline更大，效应更好。\n对于这种HCE，我们可以计算win odds(Gasparyan et al. 2021)来比较组间差异，然而，治疗效应的可视化受到复合终点的影响，不容易像单纯的生存曲线那样用合适的工具可视化出来。\n针对这一问题，AstraZeneca的Martin Karpefors等人提出了一种新的方法，即maraca plot(Karpefors, Lindholm, and Gasparyan 2023)。这种方法可以将复合终点中time to event(TTE)以及连续性终点的治疗效应可视化出来，同时也可以用来比较不同治疗组之间的差异。对应的R包可以方便地实现这一点。\n\n\nmaraca plot\nmaraca基于ggplot2，其中，对于TTE采用Kaplan-Meier曲线展示cumulative proportions，对于连续性终点可选用箱线图、violin plot以及scatter plot展示连续性分布。这种方法可以同时展示HCE的不同组成成分。\n来看一个例子。\n\nlibrary(maraca)\ndata(hce_scenario_a, package = \"maraca\")\ndata &lt;- hce_scenario_a\ndata |&gt; head()\n\n  SUBJID              GROUP GROUPN      AVAL0       AVAL    TRTP\n1      1          Outcome I      0 120.440921   120.4409  Active\n2      2 Continuous outcome  40000   3.345229 40003.3452 Control\n3      3 Continuous outcome  40000  22.802615 40022.8026  Active\n4      4          Outcome I      0 577.311386   577.3114 Control\n5      5         Outcome II  10000 781.758081 10781.7581  Active\n6      6        Outcome III  20000 985.097981 20985.0980 Control\n\n\n具体变量意义，大家可以查看?hce_scenario_a。\n可视化如下：\n\ncolumn_names &lt;- c(outcome = \"GROUP\", arm = \"TRTP\", value = \"AVAL0\")\ntte_outcomes &lt;- c(\"Outcome I\", \"Outcome II\", \"Outcome III\", \"Outcome IV\")\ncontinuous_outcome &lt;- \"Continuous outcome\"\narm_levels &lt;- c(active = \"Active\", control = \"Control\")\nmaraca_object &lt;- maraca(\n  data, tte_outcomes, continuous_outcome, arm_levels, column_names,\n  fixed_followup = 3*365, compute_win_odds = TRUE\n)\nAZ_colors &lt;- c(\"#830051\", \"#F0AB00\")\nplot(maraca_object, density_plot_type = \"default\") + theme_bw() +\n  scale_color_manual(values = AZ_colors) +\n  scale_fill_manual(values = AZ_colors)\n\n\n\n\n\n\n\n\n\n\n结果解释\n怎么看这张图？\n首先是x轴上HCE的5个组成成分，x轴上每个成分的长度大小，代表了患者达到不同成分终点的比例，可以看到，continuous outcome的比例最大，说明这个终点的患者所占比例最大。其次，cumulative percentage显示active组在四个TTE终点上是存在差异的。再然后是continuous outcome的分布，偏向x轴右侧代表change from baseline更大。而这些结合起来，就是win odds的结果，可以看到，和我们从可视化的角度看到的结果是一致的。\n代码已经放进了星球里。\n\n\n\n\n\n\n\n\n\n\nDid you find this page helpful? Consider sharing it 🙌\n\nGasparyan, S. B., E. K. Kowalewski, F. Folkvaljon, O. Bengtsson, J. Buenconsejo, J. Adler, and G. G. Koch. 2021. “Power and Sample Size Calculation for the Win Odds Test: Application to an Ordinal Endpoint in COVID-19 Trials.” Journal Article. Journal of Biopharmaceutical Statistics 31 (6): 765–87.\n\n\nKarpefors, M., D. Lindholm, and S. B. Gasparyan. 2023. “The Maraca Plot: A Novel Visualization of Hierarchical Composite Endpoints.” Journal Article. Clinical Trials (London, England) 20 (1): 84–88. https://doi.org/10.1177/17407745221134949.\n\nCitationBibTeX citation:@online{lu2024,\n  author = {Lu, Zhen},\n  title = {Hierarchical Composite {endpoints治疗效应的可视化}},\n  date = {2024-05-13},\n  url = {https://leslie-lu.github.io//blog/2024/05/13/hierarchical_composite_endpoints},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLu, Zhen. 2024. “Hierarchical Composite\nEndpoints治疗效应的可视化.” May 13, 2024. https://leslie-lu.github.io//blog/2024/05/13/hierarchical_composite_endpoints."
  },
  {
    "objectID": "blog/2024/06/07/loss_function/index.html",
    "href": "blog/2024/06/07/loss_function/index.html",
    "title": "常用损失函数",
    "section": "",
    "text": "loss function\n在机器学习/深度学习任务中，衡量模型预测值与真实值之间的差异的指标称为损失函数。损失函数是模型训练的关键组成部分，它可以帮助我们优化模型参数，使得模型的预测值更加接近真实值。预测任务的目标也是最小化损失函数，如，我们利用反向传播算法等方法，通过更新损失函数相对于模型参数的梯度来最小化损失函数，提高模型的预测能力。此外，有效的损失函数还可以帮助我们平衡模型的偏差和方差，提高模型的泛化能力。\n依据预测任务的不同，损失函数可以分为回归任务和分类任务两大类。回归任务的损失函数通常是均方误差（MSE）或平均绝对误差（MAE），而分类任务的损失函数则有交叉熵损失函数、Hinge损失函数等。本文将介绍常用的损失函数及其应用场景。\n\n均方误差（MSE）\n均方误差（Mean Squared Error，MSE）是回归任务中最常用的损失函数之一，它衡量模型预测值与真实值之间的差异。MSE的计算公式如下：\n\\[\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n\\]\n可以看到，MSE是预测值与真实值之间差值的平方和的均值，它对较大差异分配更高的惩罚。MSE非负，越小，说明模型的预测值与真实值之间的差异越小，模型的预测能力越好。MSE对异常值敏感，因为它是差值的平方和，异常值的平方会放大差异，导致模型的预测能力下降。\n其在pytorch中的实现：\n\ntorch.nn.MSELoss(reduction='mean')\n\n\n\n平均绝对误差（MAE）\n平均绝对误差（Mean Absolute Error，MAE）是回归任务中另一种常用的损失函数。MAE的计算公式如下：\n\\[\nMAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n\\]\n相比于MSE，MAE是预测值与真实值之间差值的绝对值的均值，它对异常值不敏感，因为它是差值的绝对值的和，不会对某一异常值的差异分配过高的权重。MAE的值越小，说明模型的预测值与真实值之间的差异越小，模型的预测能力越好。\n针对MAE和MSE的优缺点，我们可以根据具体的任务需求选择合适的损失函数。如果任务需要重点关注异常值，可以选择MSE，否则选择MAE。\n\ntorch.nn.L1Loss(reduction='mean')\n\n\n\nHuber loss\nHuber loss是一种结合了MSE和MAE的损失函数，它在差值较小的情况下使用MSE，差值较大的情况下使用MAE。Huber loss的计算公式如下：\n\\[\nL_{\\delta}(y, \\hat{y}) = \\left\\{\n\\begin{array}{ll}\n\\frac{1}{2}(y - \\hat{y})^2, & \\text{if } |y - \\hat{y}| \\leq \\delta \\\\\n\\delta |y - \\hat{y}| - \\frac{1}{2} \\delta^2, & \\text{otherwise}\n\\end{array}\n\\right.\n\\]\n其中，\\(\\delta\\)是一个超参数，用于控制MSE和MAE之间的平衡。Huber loss对异常值不敏感，同时保留了MSE的平滑性，是一种较为稳健的损失函数。\n\ntorch.nn.SmoothL1Loss(reduction='mean')\n\n\n\n二元交叉熵损失函数（Binary Cross Entropy Loss）\n交叉熵损失函数（Cross Entropy Loss）是二分类任务中最常用的损失函数之一，我们前面也以及介绍过。交叉熵损失函数的计算公式如下：\n\\[\nL(y, \\hat{y}) = -\\frac{1}{n} \\sum_{i=1}^{n} y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)\n\\]\n其中，\\(y_i\\)是真实标签，\\(\\hat{y}_i\\)是模型预测的概率值。交叉熵损失函数对于模型预测的概率值和真实标签之间的差异进行了惩罚，使得模型更加关注预测正确的类别。交叉熵损失函数是一种凸函数，可以通过梯度下降等方法进行优化。\n\ntorch.nn.BCELoss(weight=None, reduction='mean')\n\n\n\n多类交叉熵损失函数（Categorical Cross Entropy Loss）\n多类交叉熵损失函数是多分类任务中常用的损失函数之一，它是交叉熵损失函数的扩展。多类交叉熵损失函数的计算公式如下：\n\\[\nL(y, \\hat{y}) = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{m} y_{ij} \\log(\\hat{y}_{ij})\n\\]\n其中，\\(y_{ij}\\)是真实标签，\\(\\hat{y}_{ij}\\)是模型预测的概率值。\n\ntorch.nn.CrossEntropyLoss(weight=None,ignore_index=-100, reduction='mean')\n\n\n\nHinge损失函数\nHinge损失函数是支持向量机（SVM）中常用的损失函数之一，它适用于二分类任务。Hinge损失函数的计算公式如下：\n\\[\nL(y, \\hat{y}) = \\max(0, 1 - y \\cdot \\hat{y})\n\\]\nHinge损失函数旨在最大化决策边界的间隔，即使得正确分类的样本距离决策边界的距离尽可能大。Hinge损失函数对于误分类的样本进行了惩罚，使得模型更加关注分类边界附近的样本，从而尽可能把数据点推向远离决策边界的方向。\n代码已经放进了星球里。\n\nDid you find this page helpful? Consider sharing it 🙌"
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitæ",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "news/index.html",
    "href": "news/index.html",
    "title": "What I’m doing now",
    "section": "",
    "text": "May 11 2024: 🚀✨🎉 网站由 rmarkdown 更新为 quarto 支持。Hello World！"
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "This page contains the source code, links, and slides for various workshops, talks, lectures, and presentations I’ve given."
  },
  {
    "objectID": "talks/index.html#section",
    "href": "talks/index.html#section",
    "title": "Talks",
    "section": "2024",
    "text": "2024"
  }
]