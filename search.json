[
  {
    "objectID": "blog/2024/05/11/optimal_threshold/index.html",
    "href": "blog/2024/05/11/optimal_threshold/index.html",
    "title": "最优分类阈值",
    "section": "",
    "text": "这里我们借助scikit-learn来探讨分类问题中阈值的选择。\n\n数据准备和参数选择\n首先是数据准备：\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nnp.set_printoptions(suppress=True, precision=8, linewidth=1000)\npd.options.mode.chained_assignment = None\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\n\ndata = load_breast_cancer()\nX = data[\"data\"]\ny = data[\"target\"]\n\nXtrain, Xvalid, ytrain, yvalid = train_test_split(X, y, test_size=.20, random_state=516)\n\nprint(f\"Xtrain.shape: {Xtrain.shape}\")\nprint(f\"Xvalid.shape: {Xvalid.shape}\")\n\nXtrain.shape: (455, 30)\nXvalid.shape: (114, 30)\n\n\n模型我们这里选择随机森林。超参的选择，基于GridSearchCV，这里也不赘述。有一个点需要说明，由于使用的是肿瘤数据集，在这种情况下，我们更关注的是recall，即尽量减少假阴性的情况。因而，我们在训练模型时，也是将recall作为评价指标。\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    \"n_estimators\": [100, 150, 250],\n    \"min_samples_leaf\": [2, 3, 4],\n    \"ccp_alpha\": [0, .1, .2, .3]\n    }\n\nmdl = GridSearchCV(\n    RandomForestClassifier(random_state=516), \n    param_grid, \n    scoring=\"recall\", \n    cv=5\n    )\n\nmdl.fit(Xtrain, ytrain)\n\nprint(f\"best parameters: {mdl.best_params_}\")\n\nbest parameters: {'ccp_alpha': 0, 'min_samples_leaf': 4, 'n_estimators': 100}\n\n\n\n\n模型预测\n拿到模型后，自然我们可以开始预测：\n\nypred = mdl.predict_proba(Xvalid)[:,1]\nypred\n\narray([0.005     , 0.82743637, 0.97088095, 0.        , 0.        , 1.        , 0.98020202, 0.67380556, 0.        , 0.99333333, 0.9975    , 0.30048576, 0.9528113 , 0.99666667, 0.04102381, 0.99444444, 1.        , 0.828226  , 0.        , 0.        , 0.97916667, 1.        , 0.99607143, 0.90425163, 0.        , 0.02844156, 0.99333333, 0.98183333, 0.9975    , 0.08869769, 0.97369841, 0.        , 1.        , 0.71100866, 0.96022727, 0.        , 0.71200885, 0.06103175, 0.005     , 0.99490476, 0.1644127 , 0.        , 0.23646934, 1.        , 0.57680164, 0.64901715, 0.9975    , 0.61790818, 0.95509668, 0.99383333, 0.04570455, 0.97575758, 1.        , 0.47115815, 0.92422619, 0.77371415, 0.        , 1.        , 0.26198657, 0.        , 0.28206638, 0.95216162, 0.98761905, 0.99464286, 0.98704762, 0.85579351, 0.10036905, 0.00222222, 0.98011905, 0.99857143, 0.92285967, 0.95180556, 0.97546947, 0.84433189, 0.005     , 0.99833333, 0.83616339, 1.        , 0.9955    , 1.        , 0.99833333, 1.        ,\n       0.86399315, 0.9807381 , 0.        , 0.99833333, 0.9975    , 0.        , 0.98733333, 0.96822727, 0.23980827, 0.7914127 , 0.        , 0.98133333, 1.        , 1.        , 0.89251019, 0.9498226 , 0.18943254, 0.83494391, 0.9975    , 1.        , 0.77079113, 0.99722222, 0.30208297, 1.        , 0.92111977, 0.99428571, 0.91936508, 0.47118074, 0.98467172, 0.006     , 0.05750305, 0.96954978])\n\n\n这个时候，我们要讲的东西就来了。一般地，我们会选择0.50作为分类阈值，即大于0.50的为正类，小于0.50的为负类。\n\nypred = mdl.predict_proba(Xvalid)[:,1].reshape(-1, 1)\nyhat = mdl.predict(Xvalid).reshape(-1, 1)\npreds = np.concatenate([ypred, yhat], axis=1)\nprint(preds)\nprint(confusion_matrix(yvalid, yhat))\n\n[[0.005      0.        ]\n [0.82743637 1.        ]\n [0.97088095 1.        ]\n [0.         0.        ]\n [0.         0.        ]\n [1.         1.        ]\n [0.98020202 1.        ]\n [0.67380556 1.        ]\n [0.         0.        ]\n [0.99333333 1.        ]\n [0.9975     1.        ]\n [0.30048576 0.        ]\n [0.9528113  1.        ]\n [0.99666667 1.        ]\n [0.04102381 0.        ]\n [0.99444444 1.        ]\n [1.         1.        ]\n [0.828226   1.        ]\n [0.         0.        ]\n [0.         0.        ]\n [0.97916667 1.        ]\n [1.         1.        ]\n [0.99607143 1.        ]\n [0.90425163 1.        ]\n [0.         0.        ]\n [0.02844156 0.        ]\n [0.99333333 1.        ]\n [0.98183333 1.        ]\n [0.9975     1.        ]\n [0.08869769 0.        ]\n [0.97369841 1.        ]\n [0.         0.        ]\n [1.         1.        ]\n [0.71100866 1.        ]\n [0.96022727 1.        ]\n [0.         0.        ]\n [0.71200885 1.        ]\n [0.06103175 0.        ]\n [0.005      0.        ]\n [0.99490476 1.        ]\n [0.1644127  0.        ]\n [0.         0.        ]\n [0.23646934 0.        ]\n [1.         1.        ]\n [0.57680164 1.        ]\n [0.64901715 1.        ]\n [0.9975     1.        ]\n [0.61790818 1.        ]\n [0.95509668 1.        ]\n [0.99383333 1.        ]\n [0.04570455 0.        ]\n [0.97575758 1.        ]\n [1.         1.        ]\n [0.47115815 0.        ]\n [0.92422619 1.        ]\n [0.77371415 1.        ]\n [0.         0.        ]\n [1.         1.        ]\n [0.26198657 0.        ]\n [0.         0.        ]\n [0.28206638 0.        ]\n [0.95216162 1.        ]\n [0.98761905 1.        ]\n [0.99464286 1.        ]\n [0.98704762 1.        ]\n [0.85579351 1.        ]\n [0.10036905 0.        ]\n [0.00222222 0.        ]\n [0.98011905 1.        ]\n [0.99857143 1.        ]\n [0.92285967 1.        ]\n [0.95180556 1.        ]\n [0.97546947 1.        ]\n [0.84433189 1.        ]\n [0.005      0.        ]\n [0.99833333 1.        ]\n [0.83616339 1.        ]\n [1.         1.        ]\n [0.9955     1.        ]\n [1.         1.        ]\n [0.99833333 1.        ]\n [1.         1.        ]\n [0.86399315 1.        ]\n [0.9807381  1.        ]\n [0.         0.        ]\n [0.99833333 1.        ]\n [0.9975     1.        ]\n [0.         0.        ]\n [0.98733333 1.        ]\n [0.96822727 1.        ]\n [0.23980827 0.        ]\n [0.7914127  1.        ]\n [0.         0.        ]\n [0.98133333 1.        ]\n [1.         1.        ]\n [1.         1.        ]\n [0.89251019 1.        ]\n [0.9498226  1.        ]\n [0.18943254 0.        ]\n [0.83494391 1.        ]\n [0.9975     1.        ]\n [1.         1.        ]\n [0.77079113 1.        ]\n [0.99722222 1.        ]\n [0.30208297 0.        ]\n [1.         1.        ]\n [0.92111977 1.        ]\n [0.99428571 1.        ]\n [0.91936508 1.        ]\n [0.47118074 0.        ]\n [0.98467172 1.        ]\n [0.006      0.        ]\n [0.05750305 0.        ]\n [0.96954978 1.        ]]\n[[35  3]\n [ 1 75]]\n\n\n但是，这个阈值是可以调整的。我们可以通过调整阈值来达到不同的目的。比如，我们可以通过调整阈值来减少假阴性的情况，这在类别不平衡时尤为重要。\n\n\n阈值的选择\n我们介绍几种常用的方法。\n\n1. 阳性类别prevalance\n我们看下这个数据集中阳性类别的比例：\n\nprint(f\"Proportion of positives in training set: {ytrain.sum() / ytrain.shape[0]:.2f}\")\n\nProportion of positives in training set: 0.62\n\n\n这个toy数据集很夸张哈，达到了0.62。在实际应用中，这个比例可能只有10%或者1%。这里我们只是拿它示例哈，用这个prevalance来作为阈值。\n\nthresh = 1- ytrain.sum() / ytrain.shape[0]\nyhat = np.where(ypred &lt;= thresh, 0, 1)\nprint(confusion_matrix(yvalid, yhat))\n\n[[34  4]\n [ 0 76]]\n\n\n考虑prevalance的方法，可以在类别不平衡的情况下，减少假阴性的情况。\n\n\n2. 最优F1指数\nF1指数是precision和recall的调和平均数。我们可以通过最大F1指数来选择最优的阈值。\n\n\nThreshold using optimal f1-score: 0.471.\n\n\nF1最高为0.471，我们采用它来进行预测：\n\nthresh = .471\nyhat = np.where(ypred &lt;= thresh, 0, 1)\nprint(confusion_matrix(yvalid, yhat))\n\n[[34  4]\n [ 0 76]]\n\n\n\n\n3. ROC曲线\n我们可以通过ROC曲线来选择最优的阈值。ROC曲线下的面积AUC越大，说明模型越好。我们可以选择ROC曲线最靠近左上角的点作为最优阈值。\n\n\n\n\n\n\n\n\n\n\n\n4. PRC曲线\nPRC曲线是precision-recall曲线。相比于ROC曲线，PRC曲线更适合类别不平衡的情况。我们主要选择PRC曲线最靠近右上角的点作为最优阈值。\n\n\n\n\n\n\n\n\n\nSelected threshold using precision-recall curve: 0.674.\n\n\n\n\n5. 分别关注precision和recall\n我们可以通过调整阈值来分别关注precision和recall。比如，我们可以通过调整阈值来提高recall，减少假阴性的情况。\n\n\n\n\n\n\n\n\n\n\n代码已经放进了星球里。\n\n\nDid you find this page helpful? Consider sharing it 🙌\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{lu2024,\n  author = {Lu, Zhen},\n  title = {最优分类阈值},\n  date = {2024-05-11},\n  url = {https://leslie-lu.github.io/blog/2024/05/11/optimal_threshold/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLu, Zhen. 2024. “最优分类阈值.” May 11, 2024. https://leslie-lu.github.io/blog/2024/05/11/optimal_threshold/."
  },
  {
    "objectID": "blog/2024/05/17/calibration/index.html",
    "href": "blog/2024/05/17/calibration/index.html",
    "title": "Python中机器学习模型的校准",
    "section": "",
    "text": "calibration\n在我们利用机器学习模型来建模分类预测时，首要关注的指标能力当然是dircrimination，即模型的预测区分能力。常见的指标有sensitivity、specificity、AUROC等。我们在上一篇文章中介绍了如何选择最优分类阈值，这里我们接着介绍在选择了最优阈值后，如何评估模型的校准能力。\n所谓校准能力，即模型预测的概率与实际发生的概率一致。\n通俗来解释这个事情：比如说，我们模型预测某个病人患病的概率是0.8，那么，按照概率定义理解，模型预测概率为0.8时，100个人中应该有80个人最终患病，这个结果体现了模型的校准能力和稳定性。如果模型预测概率为0.8时，实际只有20个人患病，那么，模型的校准能力就不够好，你也不会信任这个模型在实际应用中的预测结果。这就是校准能力的重要性，即你的模型最终输出的概率值要准确反映出事件实际发生的概率。\n\n\n如何评价calibration\n\ncalibration plot\n\n\n\ncalibration curve\n\n\n上图是一个典型的calibration curve，也是我们在文章中常见的图。\n我们将模型预测概率cut或者quantile成5或者10个区间（bin），每个区间预测概率的均值作为x轴，每个区间实际发生的概率作为y轴，然后画出来这个曲线。这个图是评价模型校准能力的一个直观指标。python中可以轻松实现这个工作：\n\nfrom sklearn.calibration import calibration_curve\ny_means, pred_means = calibration_curve(y_true, y_pred, n_bins=10,strategy)\n\n理想情况下，所有点都在对角线上，即模型预测的概率与实际发生的概率完全一致。如果点在对角线上方，说明模型低估，反之，高估。\ncalibration level的定义有：\n\n\n\ncalibration level(Alonzo 2009)\n\n\n\n\n其他指标\n除了calibration plot，我们还可以用其他指标来评价模型的校准能力，比如说Brier score、Hosmer-Lemeshow test、calibration in the large等。这里不做详细介绍。\n我们感兴趣的是，当我们通过上述方法评价了模型的校准能力后，如果发现模型的校准能力不够好，我们应该怎么办？\n\n\n\ncalibrate model\n我们已经发现，模型输出值并不能代表概率。python中一般有predict_proba方法，即这个方法其实并不能保证输出的概率是真实的概率。\n\nfrom sklearn.ensemble import RandomForestClassifier\nmodel= RandomForestClassifier().fit(X_train, y_train)\ny_pred = model.predict_proba(X_test)[:,1]\n\n所以，我们需要对模型进行校准。\n\nPlatt scaling\nPlatt scaling是一种常见的校准方法，其原理是对模型输出的概率以及真实标签，用一个logistic regression模型来拟合，从而实现对模型输出的概率进行校准，拿到最终的概率。\n\nfrom sklearn.calibration import CalibratedClassifierCV\ncalibrated = CalibratedClassifierCV(model, method='sigmoid', cv=5)\ncalibrated.fit(X_train, y_train)\n\n\n\nIsotonic regression\nIsotonic regression是另一种校准方法，其原理是对模型输出的概率以及真实标签，用一个isotonic regression模型来拟合，从而实现对模型输出的概率进行校准。\n\nfrom sklearn.isotonic import IsotonicRegression\nir = IsotonicRegression().fit(y_pred, y_test)\n\n\n\nbayesian binning into quantiles\nBBQ是一种基于贝叶斯的校准方法，其原理是将预测概率分成若干个区间，然后在每个区间内对概率进行校准。该方法结合了分箱（binning）和贝叶斯推断的优点，可以在样本量较小时仍然保持较好的校准效果。\n还有其他方法可以供尝试。\n\n\n\ntake home message\n在利用机器学习模型进行分类预测时，我们不可忽视模型的校准能力。\n代码已经放进了星球里。\n\nDid you find this page helpful? Consider sharing it 🙌\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nAlonzo, T. A. 2009. “Clinical Prediction Models: A Practical Approach to Development, Validation, and Updating: By Ewout w. Steyerberg.” Generic. Oxford University Press.\n\nCitationBibTeX citation:@online{lu2024,\n  author = {Lu, Zhen},\n  title = {Python中机器学习模型的校准},\n  date = {2024-05-17},\n  url = {https://leslie-lu.github.io/blog/2024/05/17/calibration/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLu, Zhen. 2024. “Python中机器学习模型的校准.” May 17, 2024.\nhttps://leslie-lu.github.io/blog/2024/05/17/calibration/."
  },
  {
    "objectID": "blog/2020/01/06/index.html",
    "href": "blog/2020/01/06/index.html",
    "title": "Python基础要素之数值",
    "section": "",
    "text": "前面我们已经了解了如何创建、运行脚本，接下来我们了解下Python中最常用的数据类型。\n\n数值\nPython中最主要的4种数值类型分别是整数、浮点数、长整数和复数，这里只介绍整数和浮点数（即带小数点的数）。\n整数：\n\nx = 9\nprint(\"Output #4: {0}\".format(x))\nprint(\"Output #5: {0}\".format(3**4))\nprint(\"Output #6: {0}\".format(int(8.3)/int(2.7)))\n\nOutput #4: 9\nOutput #5: 81\nOutput #6: 4.0\n\n\nOutput #6演示了将数值转换成整数并进行除法运算。\n浮点数：\n\nprint(\"Output #7: {0:.3f}\".format(8.3/2.7))\ny = 2.5*4.8\nprint(\"Output #8: {0:.1f}\".format(y))\nr = 8/float(3)\nprint(\"Output #9: {0:.2f}\".format(r))\nprint(\"Output #10: {0:.4f}\".format(8.0/3))\n\nOutput #7: 3.074\nOutput #8: 12.0\nOutput #9: 2.67\nOutput #10: 2.6667\n\n\n\n#!/usr/bin/env python3\nfrom math import exp, log, sqrt\nprint(\"Output #11: {0:.4f}\".format(exp(3)))\nprint(\"Output #12: {0:.2f}\".format(log(4)))\nprint(\"Output #13: {0:.1f}\".format(sqrt(81)))\n\nOutput #11: 20.0855\nOutput #12: 1.39\nOutput #13: 9.0\n\n\n\n\n.format格式化\n\n# Add two numbers together\nx = 4\ny = 5\nz = x + y\nprint(\"Output #2: Four plus five equals {0:d}.\".format(z))\n# Add two lists together\na = [1, 2, 3, 4]\nb = [\"first\", \"second\", \"third\", \"fourth\"]\nc = a + b\nprint(\"Output #3: {0}, {1}, {2}\".format(a, b, c))\n\nOutput #2: Four plus five equals 9.\nOutput #3: [1, 2, 3, 4], ['first', 'second', 'third', 'fourth'], [1, 2, 3, 4, 'first', 'second', 'third', 'fourth']\n\n\n\n\ntype函数\nPython提供一个名为type的函数，可以对所有对象调用这个函数，来获得关于Python如何处理这个对象的更多信息。\n函数的语法非常简单：type(variable)会返回Python中的数据类型。如果你对一个数值变量调用这个函数，它会告诉你这个数值是整数还是浮点数，还会告诉你这个数值是否能当作字符串进行处理。\n此外，由于Python同样是面向对象的语言，所以你可以对Python中所有命名对象调用type函数，不仅是变量，还有函数、语句等。\n\nDid you find this page helpful? Consider sharing it 🙌"
  },
  {
    "objectID": "blog/2024/08/20/nested_layout/index.html",
    "href": "blog/2024/08/20/nested_layout/index.html",
    "title": "大图嵌小图",
    "section": "",
    "text": "由来\n星球里不断有同学问到如何在一个大图中嵌入小图，这里简单介绍一下。\n\n\n\nQ1\n\n\n\n\n\nQ2\n\n\n我们使用生存曲线及risk table作为例子，其中生存曲线是大图，risk table是小图。常见的图形为：\n\n\n\nsurvival curve\n\n\n想要把risk table嵌入到生存曲线中。\n\n\n方法一\n使用grid包，借助于grid包中的viewport函数。viewport用于定义一个绘图区域，可以在一个图形设备中创建多个独立的绘图区域，每个区域都有自己的坐标系和尺寸。\n\n# not run\nsubvp &lt;- viewport(width = 0.35, height = 0.35, x = 0.75, y = 0.75)\nggsurv$plot\nprint(ggsurv$table, vp = subvp)\n\nviewport创建了一个子视口，它定义了一个相对主视口的区域。效果如下：\n\n\n\noption 1\n\n\n\n\n方法二\n使用annotation_custom函数，它可以在图形中添加自定义的图形元素。\n\n# not run\nggsurv$plot + annotation_custom(ggplotGrob(ggsurv$table), xmin=1900, xmax=3000, ymin=0.6, ymax=1)\n\nggplotGrob将ggsurv$table转换为grob对象，以便在图形中使用。效果如下：\n\n\n\noption 2\n\n\n\n\n方法三\n使用ggpp包。\n\n# not run\nsub_plot= tibble::tibble(\n    x= .98, y= .98, plot= list(ggsurv$table)\n)\nggsurv$plot + \n    geom_plot_npc(data = sub_plot, aes(npcx = x, npcy = y, label = plot))\n\n使用geom_plot_npc函数将子图添加到主图中，label表示要添加的子图。效果如下：\n\n\n\noption 3\n\n\n完整代码已经放在了星球里，感兴趣的同学可以自行查看。\n\nDid you find this page helpful? Consider sharing it 🙌"
  },
  {
    "objectID": "news/index.html",
    "href": "news/index.html",
    "title": "What I’m doing now",
    "section": "",
    "text": "May 11 2024: 🚀✨🎉 网站由 rmarkdown 更新为 quarto 支持。Hello World！"
  },
  {
    "objectID": "blog/index.html#section",
    "href": "blog/index.html#section",
    "title": "Blog",
    "section": "2024",
    "text": "2024\n\n\n    \n    \n                  \n            August 20, 2024\n        \n        \n            大图嵌小图\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    ggplot2\n                \n                \n            \n            \n\n            调包\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            August 14, 2024\n        \n        \n            欢迎加入预测模型星球\n\n            \n            \n                \n                \n                    workshop\n                \n                \n                \n                    prediction model\n                \n                \n            \n            \n\n            Clinical Prediction Model\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            August 11, 2024\n        \n        \n            星球第二期workshop上线\n\n            \n            \n                \n                \n                    workshop\n                \n                \n                \n                    sample size\n                \n                \n                \n                    clinical research\n                \n                \n            \n            \n\n            Sample Size Calculations in Clinical Research\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            August 4, 2024\n        \n        \n            星球第一期workshop上线\n\n            \n            \n                \n                \n                    workshop\n                \n                \n                \n                    missing data\n                \n                \n                \n                    statistical methods\n                \n                \n            \n            \n\n            Statistical Methods for Analysis with Missing Data\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            August 3, 2024\n        \n        \n            倾向性评分加权\n\n            \n            \n                \n                \n                    propensity score\n                \n                \n                \n                    weighting\n                \n                \n            \n            \n\n            倾向性评分加权的具体介绍\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            July 19, 2024\n        \n        \n            预测模型领域新书推荐\n\n            \n            \n                \n                \n                    prediction model\n                \n                \n                \n                    book\n                \n                \n            \n            \n\n            临床预测模型方法与应用\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            June 23, 2024\n        \n        \n            2023年最新JCR影响因子发布\n\n            \n            \n                \n                \n                    sci\n                \n                \n                \n                    jcr\n                \n                \n            \n            \n\n            2023年最新JCR影响因子\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            June 7, 2024\n        \n        \n            常用损失函数\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    machine learning\n                \n                \n                \n                    deep learning\n                \n                \n                \n                    loss function\n                \n                \n            \n            \n\n            常用损失函数介绍\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            May 17, 2024\n        \n        \n            Python中机器学习模型的校准\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    machine learning\n                \n                \n                \n                    calibration\n                \n                \n            \n            \n\n            预测模型如何校准\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            May 13, 2024\n        \n        \n            Hierarchical composite endpoints治疗效应的可视化\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    clinical trial\n                \n                \n                \n                    endpoint\n                \n                \n            \n            \n\n            复合终点治疗效应的可视化\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            May 11, 2024\n        \n        \n            最优分类阈值\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    machine learning\n                \n                \n            \n            \n\n            分类问题中阈值的选择\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-1",
    "href": "blog/index.html#section-1",
    "title": "Blog",
    "section": "2023",
    "text": "2023"
  },
  {
    "objectID": "blog/index.html#section-2",
    "href": "blog/index.html#section-2",
    "title": "Blog",
    "section": "2022",
    "text": "2022"
  },
  {
    "objectID": "blog/index.html#section-3",
    "href": "blog/index.html#section-3",
    "title": "Blog",
    "section": "2021",
    "text": "2021"
  },
  {
    "objectID": "blog/index.html#section-4",
    "href": "blog/index.html#section-4",
    "title": "Blog",
    "section": "2020",
    "text": "2020\n\n\n    \n    \n                  \n            January 22, 2020\n        \n        \n            R语言编程入门\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    programming\n                \n                \n            \n            \n\n            R Training\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 21, 2020\n        \n        \n            上手vim编辑器\n\n            \n            \n                \n                \n                    vim\n                \n                \n                \n                    linux\n                \n                \n            \n            \n\n            vim\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 20, 2020\n        \n        \n            计算机概论3\n\n            \n            \n                \n                \n                    linux\n                \n                \n            \n            \n\n            linux\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 19, 2020\n        \n        \n            谈谈电脑的CPU\n\n            \n            \n                \n                \n                    linux\n                \n                \n            \n            \n\n            linux\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 18, 2020\n        \n        \n            Python正则表达式与模式匹配\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    regular expression\n                \n                \n                \n                    pattern matching\n                \n                \n            \n            \n\n            Python Training\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 17, 2020\n        \n        \n            你真的了解自己的电脑吗？\n\n            \n            \n                \n                \n                    linux\n                \n                \n            \n            \n\n            linux\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 16, 2020\n        \n        \n            R语言的初体验\n\n            \n            \n                \n                \n                    r\n                \n                \n            \n            \n\n            R Training\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 11, 2020\n        \n        \n            kNN改进约会网站的配对效果\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    kNN\n                \n                \n            \n            \n\n            kNN\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 10, 2020\n        \n        \n            Python基础要素之字符串\n\n            \n            \n                \n                \n                    python\n                \n                \n            \n            \n\n            Python Training\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 9, 2020\n        \n        \n            笔记\n\n            \n            \n                \n                \n                    biostatics\n                \n                \n                \n                    causal inference\n                \n                \n            \n            \n\n            causal inference\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 8, 2020\n        \n        \n            如何用Python自编k-近邻算法？\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    kNN\n                \n                \n            \n            \n\n            kNN\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 7, 2020\n        \n        \n            NumPy函数库基础\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    numpy\n                \n                \n            \n            \n\n            Python Training\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 6, 2020\n        \n        \n            Python基础要素之数值\n\n            \n            \n                \n                \n                    python\n                \n                \n            \n            \n\n            Python Training\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 5, 2020\n        \n        \n            可惜没如果——因果关系的三个层级\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    causal inference\n                \n                \n            \n            \n\n            因果推断的基本知识\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 4, 2020\n        \n        \n            Python初体验\n\n            \n            \n                \n                \n                    pycharm\n                \n                \n                \n                    python\n                \n                \n            \n            \n\n            Python Training\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 2, 2020\n        \n        \n            因果推断-2\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    causal inference\n                \n                \n            \n            \n\n            关于因果推断的基本概念\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 1, 2020\n        \n        \n            鼠年加油\n\n            \n            \n                \n                \n                    happy new year\n                \n                \n            \n            \n\n            新年快乐\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-5",
    "href": "blog/index.html#section-5",
    "title": "Blog",
    "section": "2019",
    "text": "2019\n\n\n    \n    \n                  \n            December 11, 2019\n        \n        \n            因果推断\n\n            \n            \n                \n                \n                    statistics\n                \n                \n                \n                    biostatistics\n                \n                \n                \n                    causal inference\n                \n                \n            \n            \n\n            关于因果推断的基本概念\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            November 28, 2019\n        \n        \n            统计学是干嘛的？\n\n            \n            \n                \n                \n                    statistics\n                \n                \n                \n                    biostatistics\n                \n                \n            \n            \n\n            聊一聊什么是统计学\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "I have made contributions to neurosyphilis and cancer research, and my work has been published in journals including eClinicalMedicine, JMIR Public Health and Surveillance, among others. Besides, I have patented a clinical diagnosis system for neurological syphilis (LU504466, rank 1/1).\nIn addition to my work on neurosyphilis and cancer, I have extensive experience applying advanced statistical concepts, artificial intelligence, and causal inference methods to conduct real-world research based on electronic health records. I have conducted multiple studies in the areas of COVID-19, HPV and HIV infections, and cardiovascular disease."
  },
  {
    "objectID": "publications/index.html#journal-articles",
    "href": "publications/index.html#journal-articles",
    "title": "Publications",
    "section": "Journal articles",
    "text": "Journal articles"
  },
  {
    "objectID": "publications/index.html#working-papers",
    "href": "publications/index.html#working-papers",
    "title": "Publications",
    "section": "Working papers",
    "text": "Working papers"
  },
  {
    "objectID": "publications/index.html#book-chapters",
    "href": "publications/index.html#book-chapters",
    "title": "Publications",
    "section": "Book chapters",
    "text": "Book chapters"
  },
  {
    "objectID": "publications/index.html#reviews",
    "href": "publications/index.html#reviews",
    "title": "Publications",
    "section": "Reviews",
    "text": "Reviews"
  },
  {
    "objectID": "publications/index.html#selected-seminar-papers",
    "href": "publications/index.html#selected-seminar-papers",
    "title": "Publications",
    "section": "Selected seminar papers",
    "text": "Selected seminar papers"
  },
  {
    "objectID": "publications/index.html#translations",
    "href": "publications/index.html#translations",
    "title": "Publications",
    "section": "Translations",
    "text": "Translations"
  },
  {
    "objectID": "blog/2024/08/11/workshop_002/index.html",
    "href": "blog/2024/08/11/workshop_002/index.html",
    "title": "星球第二期workshop上线",
    "section": "",
    "text": "我们星球正式上线第二期workshop啦！\n\n\n\nworkshop\n\n\n本期workshop主题是“Sample Size Calculations in Clinical Research”。本期workshop将从临床研究中不同试验设计的角度出发，介绍如何基于不同设计类型（平行设计、交叉设计、析因设计、成组序贯设计等）、比较类型（非劣效、等效、优效试验）、主要终点指标等因素进行样本量计算。\n\n\n\nscreenshot\n\n\n目前暂定的安排是每周一节一小时，直到本期workshop的全部内容结束。\n\n\n\nused files\n\n\n后续我们会根据星球中同学们的需求，不定时地开展更多主题的workshop，具体时间请关注我们的公众号和星球，我们会在这两个平台上发布最新的信息。\n\nDid you find this page helpful? Consider sharing it 🙌"
  },
  {
    "objectID": "blog/2024/08/03/propensity_score_weighting/index.html",
    "href": "blog/2024/08/03/propensity_score_weighting/index.html",
    "title": "倾向性评分加权",
    "section": "",
    "text": "背景介绍\n对于非RCT类的观察性研究，由于分组的非随机性，导致了研究偏倚的存在，致使观察到的效应很多时候往往并不可用。为了解决这个问题，研究者们提出了倾向性评分的方法，通过倾向性评分的计算，可以使得试验组和对照组之间的分布更加接近，从而减少了研究偏倚的影响。\n我们公众号以往有过五篇类似的介绍内容，分别是：\n\n倾向性评分分析\n倾向性评分分析的统计学考虑\n倾向性评分匹配的生存分析怎么做\n倾向性评分overlapping weighting的SAS实现（一）\n生存资料倾向性评分OW的SAS实现（二）\n\n其中，后面两篇文章提到了倾向性评分加权中的overlapping weighting方法，这篇文章将对倾向性评分加权的方法进行详细介绍。\n\n\n因果效应\n首先，我们来看下几种因果效应。\nATE即平均处理效应（average treatment effect），是指在试验组和对照组之间的处理效应的差异。理想情况下，随机对照试验估计出来的效应即ATE，但是在实际研究中，由于种种原因，我们往往无法进行随机对照试验。由于ATE的估计人群是试验组和对照组的总体，ATE假设两组受试者是有相同的概率/机会接受某一种处理的，然而，实际研究中，研究者往往更加关注的是ATE的局部估计，即在某一特定的人群中（一般是接受治疗的试验组），处理的效应是多少，而这个效应即为ATT（average treatment effect on the treated）。由于ATT只需要对处理组人群估计因果处理效应，对于RCT而言，潜在的治疗效果和治疗组分配是相互独立的，因此，ATT即为ATE；然而，对于非RCT类研究而言，二者是不同的。\n我们还可以计算ATC（average treatment effect on the control），即对于未接受治疗的人群，如果接受治疗，其效应是多少。此外，还有ATM（average treatment effect among the evenly matchable），即在对照组中，找到与试验组相匹配的人群，计算在这个匹配的总体人群中的治疗效应；ATO（average treatment effect among the overlap population），即在试验组和对照组的重叠人群中，计算治疗效应。相比于ATM，ATO有着更好的方差属性，由于其不像ATM那样匹配要求，转而是选择两组重叠的中间人群，因此，ATO的估计更加稳健。\n\n\n倾向性评分加权\n针对以上五种因果效应，我们可以通过倾向性评分加权的方法来进行相应的估计。这里直接给出五种权重的计算公式：\n\nATE：\\(w_{ATE} = \\frac{Z_i}{e_i} + \\frac{1 - Z_i}{1 - e_i}\\)\nATT：\\(w_{ATT} = \\frac{e_iZ_i}{e_i} + \\frac{e_i(1-Z_i)}{1-e_i}\\)\nATC：\\(w_{ATC} = \\frac{(1-e_i)Z_i}{e_i} + \\frac{(1-e_i) (1-Z_i)}{1 - e_i}\\)\nATM：\\(w_{ATC} = \\frac{(1-e_i)Z_i}{e_i} + \\frac{(1-e_i) (1-Z_i)}{1 - e_i}\\)\nATO：\\(w_{AT0} = (1-e_i)Z_i + e_i(1-Z_i)\\)\n\n以上五种加权的示例以及具体实现的全部代码，我们已经放在了星球里，感兴趣的同学可以自行查看。\n这里我们展示下加权后人群的分布情况。\n\n原始人群的ps分布\n\n\n\nps of original population\n\n\n\n\nATE\n\n\n\nATE\n\n\n\n\nATT\n\n\n\nATT\n\n\n\n\nATC\n\n\n\nATC\n\n\n\n\nATM\n\n\n\nATM\n\n\n\n\nATO\n\n\n\nATO\n\n\n\n\n\n总结\n相信通过以上可视化的展示，大家会更容易理解倾向性评分加权的方法对目标人群的选择以及治疗效应的解释。借助于合适的效应加权，我们可以估计出治疗效应并对于以上五种治疗效应的估计值。\n\nDid you find this page helpful? Consider sharing it 🙌"
  },
  {
    "objectID": "blog/2024/06/23/JCR_2023/index.html",
    "href": "blog/2024/06/23/JCR_2023/index.html",
    "title": "2023年最新JCR影响因子发布",
    "section": "",
    "text": "2023年最新JCR影响因子\n最新的影响因子前几天已经发布了，和去年一样，大家在公众号后台回复”JCR2023”，即可拿到最全的总结excel表格，包括2023年的最新影响因子，以及各个学科的排名，希望对大家有所帮助。\n\n\n关注的一些期刊\n几乎全部的期刊影响因子都回落到了几年前的水平。\n四大神刊中JAMA几近腰斩，柳叶刀系列的多个子刊也是如此。\n\n\n\n柳叶刀系列\n\n\n医工交叉领域也是普遍下滑。medical informatics数字医疗部分，lancet digital health和npj Digital Medicine分别是23.8和12.4分。\n以往动辄二三十分的盛况不再。\n\nDid you find this page helpful? Consider sharing it 🙌"
  },
  {
    "objectID": "blog/2020/01/21/index.html",
    "href": "blog/2020/01/21/index.html",
    "title": "上手vim编辑器",
    "section": "",
    "text": "我们平时已经接触了不少的程序编辑器，今天我们要上手一种命令行模式下的文本编辑器——vim编辑器。说它是文本编辑器有点小瞧它的功能，实际上，它也可以作为程序编辑器使用，且功能十分强大。\n在所有的Linux发行版上面都会有一个文本编辑器，那就是vi，vim是高级版的vi。vim不仅可以用不同的颜色显示文字内容，还能够进行诸如shell脚本、C语言等程序编辑，搭配Python也是十分的香，甚至不少人用它来作为写作的专用编辑器。由此可知它的功能有多么的强大。\n如果你在学习Linux，身边的人都会建议你：学习使用命令行模式来处理Linux系统的设置问题，而尽量少去使用图形窗口模式。在配置Linux参数文件时，我们就需要一款强大稳定的文本编辑器。而Linux在命令行模式下的文本编辑器有哪些呢？我们常听到emacs、nano、vim等，而在其中，其实vim并非是对用户最友善的文本编辑器。但是为什么这么多人推荐使用呢？原因有几点：\n\n所有的UNIX-like系统都会内置vi文本编辑器，其他的文本编辑器不一定会存在；\n很多软件的编辑接口都会主动调用vi；\nvim具有程序编辑的能力，可以主动地以字体颜色辨别语法的正确性，方便程序设计；\n编辑速度相当快速。\n\n可以说，如果不上手vim，Linux中很多命令根本无法操作。\n我们提到，vim是高级版本的vi，它可以用颜色或下划线的方式来显示一些特殊的信息，可以依据文件的扩展名或是文件内的开头信息，判断该文件的内容而自动调用该程序的语法判断样式，再以颜色来显示程序代码与一般信息，vim是程序开发者的一项非常好用的工具，就连vim的官方网站（http://www.vim.org）都认为自己是一款程序开发工具而非仅仅是文本处理软件。\n由于是命令行模式下的编辑器，当我们在编辑程序或者制作网页的时候，vim不能做到一般编辑器那样所见即所得，这是它的一个特色。vim同样也有一些非常好用的功能，如支持正则表达式的查找方式、多文件编辑、区块复制等，非常的棒。我们会在日后持续更新vim的使用分享。\n\nDid you find this page helpful? Consider sharing it 🙌"
  },
  {
    "objectID": "blog/2020/01/19/index.html",
    "href": "blog/2020/01/19/index.html",
    "title": "谈谈电脑的CPU",
    "section": "",
    "text": "前言\n前面我们已经初步了解了计算机–你真的了解自己的电脑吗？接下来我们继续深入计算机的CPU单元。\n\n\nCPU\n前面我们提过，一般我们常说的电脑指的是x86的个人电脑架构。Linux操作系统最早在发展的时候，就是依据个人电脑的架构来设计的。而在个人电脑架构中，充当“大脑”的无疑是CPU。\n由于CPU负责大量运算，因而它是电脑中具有相当高发热量的组件。现在的所谓多内核CPU，是在一块CPU封装内嵌入两个以上的运算内核，即含有两个以上的CPU单元。\n我们已经知道，CPU内部指令集的不同会导致其工作效率的高低，那么CPU性能的比较还有什么呢？答案是频率。CPU的频率就是CPU每秒钟可以进行的工作次数，频率越高表示这块CPU在单位时间内可以做更多的事情。举例来说，Intel的i7-4790CPU频率为3.6GHz，即表示这块CPU在一秒内可以进行3.6*10的九次方次工作。但是需要注意的是：只能在同款CPU间比较频率的快慢，不同CPU由于指令集、架构、使用的二级缓存及其运算机制的可能不同，单纯看频率没有可比性。\n我们可能听过“超频”这个词，它是什么意思呢？CPU在出厂时，厂商已经设置了这款CPU正常稳定工作的频率，一些电脑硬件玩家要发挥出CPU最大的性能，往往会手动将CPU的外频通过主板提供的设置功能更改成较高频率。现在Intel的CPU会主动帮你超频，以合理利用CPU以及节能。\n最常听见的还有32位与64位电脑，我们可能也一头雾水。其实这个也与CPU相关。我们将CPU每次能够处理的数据量称为字长（word size），字长依据CPU的设计而有32位与64位，而32位与64位电脑主要就是依据这个CPU所能解析的字长而来的。早期的32位CPU中，由于CPU每次解析的数据量有限，因此从内存传来的数据量就有所限制，这也导致32位的CPU最多只能支持最大到4GB的内存。目前的64位CPU统称为x86-64。\n\nDid you find this page helpful? Consider sharing it 🙌"
  },
  {
    "objectID": "blog/2020/01/17/index.html",
    "href": "blog/2020/01/17/index.html",
    "title": "你真的了解自己的电脑吗？",
    "section": "",
    "text": "前言\n我们现在基本上人手一台电脑，无论你是Mac、Windows还是Linux，你真的了解你手头上的电脑么？当你在选购新电脑或者购买部件的时候，是否了解该怎么挑选呢？接下来我们一起了解下最常见也是容易搞不懂的电脑/计算机。\n\n\n什么是计算机\n一般地，接受用户输入命令与数据，经由中央处理器的数学与逻辑单元运算处理后，以产生或存储成有用的信息的机器，我们就称之为计算机。从这个意义上讲，我们日常用的计算器、手机、GPS卫星定位系统、ATM取款机、台式电脑、笔记本电脑、iPad、Apple Watch等都是计算机。我们常说的计算机是其中的台式或笔记本电脑。\n\n\n电脑硬件\n电脑的三大部分（以台式电脑为例）包括：\n\n输入单元：包括键鼠、读卡器、扫描仪、手写板、触控屏幕等\n主机部分：即系统单元，在主机机箱内，里面含有一堆板子、CPU与内存等\n输出单元：屏幕、打印机等\n\n主机里面最重要的就是一块主板，上面安装了中央处理器（Central Processing Unit, CPU）以及内存、硬盘（或存储卡）还有一些适配卡设备。大部分智能手机是将这些组件直接焊接在主板上面而不是插卡。\n\n\nCPU\n整台主机的重点在于CPU，CPU为一个具有特定功能的芯片，里面含有指令集，如果你想要让主机进行什么操作，就得要参考这块CPU是否有相关内置的指令集才可以。由于CPU的工作主要在于管理和运算，因此在CPU内又可分为两个主要的单元，分别是算术逻辑单元和控制单元。其中算术逻辑单元主要负责程序运算与逻辑判断，控制单元则主要协调各周边组件与各单元间的工作。CPU是整个电脑系统的最重要部分。\nCPU依设计理念不同，主要分为：\n\n精简指令集（RISC）系统：ARM公司的ARM CPU系列等。我们常使用的各品牌手机、导航系统、路由器等，几乎都是使用ARM架构的CPU。\n复杂指令集（CISC）系统：AMD、Intel等x86架构的CPU。由于x86架构CPU被大量用于个人电脑，因此个人电脑常被称为x86架构电脑。64位的个人电脑CPU又被统称为x86-64架构。x86架构的称呼来源于Intel最早研发出来的CPU代号。所谓的位（bit），指的是CPU一次读取数据的最大量。64位CPU表示CPU一次可以读写64位的数据，一般32位CPU所能读写的最大数据量大概是4GB。\n\n\n\n电脑上面常用的计算单位（容量、速度等）\n\n容量单位：电脑对数据的判断主要依据有没有通电来记录信息，所以理论上对于每一个记录单位而言，电脑只认识0与1而已。0/1这个二进制的单位我们称之为位（bit，比特）。但位实在太小，因而每份数据都使用8个位来记录，8位为一字节（Byte）。同样的，字节依然太小，因而有K代表1024B，M代表1024K，G代表1024M，T代表1024G，P代表1024T，E代表1024P。一般来说，数据容量使用二进制，所以1GB的文件大小为102410241204B。\n速度单位：CPU的命令周期常使用MHz或GHz之类的单位，这个Hz是“次数/秒”的意思。而在网络传输方面，由于网络使用的是位为单位，因此网络常使用的单位为Mbit/s（每秒多少Mbit）。大家常听到的“20M/5M”光纤传输速度，如果转成数据容量的字节时，其实理论最大传输值为：每秒2.5MB/每秒625KB的下载或上传速度。\n\n假设你今天购买了一块500GB的硬盘，但是格式化完毕后只剩下460GB左右的容量，这是为什么呢？一般硬盘制造商使用十进制的单位，所以500GB代表50010001000*1000B，转成数据的容量单位时使用二进制（1024为基数），所以就成为466GB左右的容量了。并非厂商骗人，只是因为硬盘的最小物理量为512B，最小的组成单位为扇区（sector），通常硬盘容量的计算采用多少个扇区，所以才会使用十进制来处理。\n\n\n内存\nCPU读取的数据完全从内存中来（无论是程序还是一般文件数据），如果要读取硬盘中的数据，也要将数据挪到内存当中，再交由CPU来读取。内存中的数据则是从输入单元所传输进来的，而CPU处理完毕的数据也必须要先写回内存，最后数据才从内存传输到输出单元。\n这就是我们常说的，要加快系统性能，通常将内存容量加大就可以获得相当好的效果。因为所有的数据都是要经过内存的传输，所以内存的容量如果太小，数据读写性能就不足，对性能的影响相当大，尤其在Linux作为服务器操作系统的环境下。这也是为什么在买手机时，人们对可用内存（运行内存）的要求都很高的原因。\n\nDid you find this page helpful? Consider sharing it 🙌"
  },
  {
    "objectID": "blog/2020/01/11/index.html",
    "href": "blog/2020/01/11/index.html",
    "title": "kNN改进约会网站的配对效果",
    "section": "",
    "text": "前言\n前面我们已经初步了解了kNN——如何用Python自编k-近邻算法？今天我们试着进行一个实例上kNN的应用。\n海伦一直使用在线约会网站寻找自己心仪的约会对象。经过一番总结，她发现曾交往过三种类型的人：不喜欢的人、魅力一般的人和极具魅力的人。她发现自己无法直接将约会网站推荐的匹配对象归入恰当的上述类别之中，希望我们的分类软件能够更好地帮助她进行确切的分类。此外，她还收集了一些约会网站未曾记录的数据信息，提供给了我们。\n\n\n准备数据：从文本文件中解析数据\n海伦将准备的数据存放在文本文件datingTestSet2.txt中，每个样本数据占据一行，总共有1000行。每个样本主要收集了3种特征：每年飞行里程数、玩视频游戏所耗时间百分比和每周消费的冰淇淋公升数。如下：\n\n\n\ndatingTestSet2\n\n\n而在我们将上述特征数据输入到分类器之前，必须将待处理数据的格式转换为分类器可以接受的格式。我们之前在kNN.py中已经创建了kNN分类器函数，接下来我们创建用于处理输入数据格式的file2matrix函数，此函数的输入为文件名字符串，输出为训练样本矩阵和类标签向量，将文本记录转换为NumPy。\n\n\n\nfile2matrix\n\n\n首先我们以r模式（只读模式）打开要处理的文本文件，readlines函数读取整个文件所有行，保存在一个列表变量中，每行作为一个元素，我们计数文件的行数。然后创建以零填充的矩阵，我们将该矩阵的另一维度设置为固定值3。循环处理文件的每一行数据：使用strip函数截取掉所有的回车字符，使用tab字符（将上一步得到的整行数据分割成一个元素列表，选取前3个元素存储到特征矩阵中，使用索引值-1将文件的最后一列存储到向量classLabelVector中，这里，我们必须明确地通知解释器存储的的元素值为整型，否则Python会将这些元素当作字符串进行处理。\n使用函数file2matrix读取文件数据，必须确保文件存储在我们的工作目录中。重新加载kNN.py模块，以确保更新的内容可以生效，否则Python将继续使用上次加载的kNN模块。\n\n\n\nload data\n\n\n现在我们已经从文本文件导入了数据并将其格式化为想要的格式，接下来我们以图形化的方式直观地展示数据内容，以便辨识出一些数据模式。\n\n\n分析数据：使用Matplotlib创建散点图\n首先我们使用Matplotlib制作原始数据的散点图：\n\n\n\nscatter\n\n\n输出效果如下图，散点图使用特征矩阵的第二、三列数据，分别为玩视频游戏所耗时间百分比和每周消费的冰淇淋公升数。由于没有使用样本类别标签，我们很难看出有用的数据模式信息。\n\n\n\nplot\n\n\n为了更好地理解数据，我们以不同的方式来标记不同的样本类别。Matplotlib库提供的scatter函数支持个性化标记散点图上的点。重新输入上面的代码，调用scatter函数时使用下列参数，利用变量datingLabels存储的类别标签属性，在散点图上绘制色彩不等、尺寸不同的点：\n\n\n\nscatter function\n\n\n\n\n\ncolored plot\n\n\n我们基本上能够看到数据点所属三个类别的区域轮廓，但还不是十分明显，接下来我们使用特征矩阵的第一、二列属性作图：\n\n\n\nclusters\n\n\n此时我们可以看到图中清晰地标示了三个不同的样本类别区域，通过这两个特征更容易区分数据点。\n\n\n准备数据：归一化数值\n观察原始数据我们发现：每年飞行里程数的数量级远大于其余两个特征。在利用kNN计算样本之间的距离时，数值大的该特征会极大地影响最终的结果，也就是说，数量值大小会影响特征对结果影响的权重，而我们这里认为三个特征是同等重要的。\n因而在处理这种情况时，我们通常采用的方法是将数值归一化，如将取值范围处理为0到1或者-1到1之间。下面的公式可将特征值转化为0到1区间内的值：\\(newValue = \\frac{oldValue - min}{max - min}\\)，其中 \\(min\\) 和 \\(max\\) 分别是数据集中的最小特征值和最大特征值。我们需要在脚本 kNN.py 中增加一个函数 autoNorm，自动将数字特征值转化为0到1区间内的值。\n\n\n\nautoNorm\n\n\n我们将每列的最小值放在变量minVals中，每列最大值放在变量maxVals中，其中的参数0使得函数可以从列中选取最小值和最大值。然后函数计算可能的取值范围，并创建新的返回矩阵。\n正如前面给出的公式，为了归一化特征，我们使用当前值减去最小值，然后除以取值范围。而需要注意的是，特征值矩阵有1000*3个值，而minVals和maxVals的值都为1*3。为了解决这个问题，我们使用NumPy库中函数tile将变量内容复制成输入矩阵同等大小的矩阵，然后再利用具体特征值相除得到归一化后的特征矩阵。需要注意的是：对于某些数值处理软件包，/可能意味着矩阵除法，但在NumPy库中，矩阵除法需要使用函数linalg.solve(matA,matB)。\n我们重新加载kNN.py模块，执行函数autoNorm：\n\n\n\nreload autoNorm\n\n\n这里我们也可以只返回normMat矩阵，但是后面我们需要取值范围和最小值来归一化需要测试的新数据。\n\n\n测试算法：作为完整程序验证分类器\n我们已经对数据按照需求进行了处理，下面我们来测试分类器的效果。我们将已有数据的90%作为训练集来训练分类器，使用余下的10%作为测试集，检测分类器的错误率。这里由于海伦提供的数据并没有按照特定目的来排序，因而我们可随意选择10%数据而不影响测试集选择的随机性。\n\n\n\ndatingClassTest\n\n\n转换数据格式并归一化后，我们决定哪些数据用于测试，然后将训练集和测试集输入到kNN分类器classify函数中，计算错误率并输出分类结果。\n\n\n\ntest result\n\n\nkNN分类器在测试集上的错误率为5%。我们可以改变函数datingClassTest内变量hoRatio和变量k的值，看看错误率是否会发生一些变化。\n现在，海伦可以输入未知对象的特征信息，由的分类器来帮助她判定某一对象的可交往程度：讨厌、一般喜欢、非常喜欢。\n\n\n使用算法：构建完整可用系统\n我们会给海伦一小段程序，通过该程序海伦会在约会网站上找到某个人并输入他的信息，程序会给出她对对方喜欢程度的预测值。\n\n\n\nclassifyPerson\n\n\n这里使用input函数获取用户控制台的输入。我们让海伦给出她在约会网站上新找的一个人信息。由于NumPy库提供的数组操作并不支持Python自带的数组类型，因此在编写代码时要注意不要使用错误的数组类型。另外在输入新样本时注意将其归一化处理。\n\n\n\nclassify result\n\n\n这样，我们就完成了kNN对约会网站的配对效果的改进了。\n\nDid you find this page helpful? Consider sharing it 🙌"
  },
  {
    "objectID": "blog/2020/01/09/index.html",
    "href": "blog/2020/01/09/index.html",
    "title": "笔记",
    "section": "",
    "text": "表示的重要性\n表示问题必须优先于获取问题。如果缺少问题的表示方法，我们也就不知道如何存储信息以供使用。人工智能对认知研究的一个主要贡献就是确立“表示第一，获取第二”的范式。\n通常，在寻求一个好的表示方法的过程中，关于如何获取知识的洞见就会自然产生，无论这种洞见是来自数据，还是来自程序员。\n人类的大脑肯定拥有某种简洁的信息表示方式，同时还拥有某种十分有效的程序用以正确解释每个问题，并从存储的信息表示中提取正确答案。我们需要给机器装备同样高效的表示信息和提取答案的算法，因果图和因果推断就派上了用场。\n\n\n打破规则\n计算机不能理解因果关系，我们必须教会它如何打破规则，让它理解“观察到某事件”和“使某事件发生”之间的区别。\n我们需要告诉计算机：“无论何时，如果你想使某事发生，那就删除因果图中指向该事的所有箭头，之后继续根据逻辑规则进行分析。”这样做的原因很简单：使某事发生就意味着将它从所有其他影响因子中解放出来，并使它受限于唯一的影响因子——能强制其发生的那个因子。\n计算机能够进行因果推理的前提是，计算机懂得有选择地打破逻辑规则。\n\n\n概率的重要性\n构建因果模型不仅仅是画箭头，箭头背后还隐藏着概率。当我们绘制一个从X指向Y的箭头时，我们是在暗指，某些概率规则或函数具体说明了“如果X发生改变，Y将如何变化”。在某些情况下我们可能知道这个规则具体是什么，但更多时候，我们不得不根据数据来对这个规则进行估计。因果革命最有趣的特点之一就是，在许多情况下，我们可以对这些完全不确定的数学细节置之不理。通常情况下，因果图自身的结构就足够让我们推测出各种因果关系和反事实关系：简单的或复杂的、确定的或概率的、线性的或非线性的。\n\n\n概率与因果关系\n概率能将我们对静态世界的信念进行编码，而因果论则告诉我们，当世界被改变时（无论改变是通过干预还是通过想象实现的），概率是否会发生改变以及如何改变。\n\nDid you find this page helpful? Consider sharing it 🙌"
  },
  {
    "objectID": "blog/2020/01/07/index.html",
    "href": "blog/2020/01/07/index.html",
    "title": "NumPy函数库基础",
    "section": "",
    "text": "机器学习算法涉及很多线性代数知识，我们会经常使用NumPy函数库，用线性代数简化不同的数据点上执行的相同数学运算。将数据表示为矩阵形式，只需要执行简单的矩阵运算而不需要复杂的循环操作。\n在Python shell开发环境中输入下面命令：from numpy import *，将NumPy函数库中的所有模块引入到当前的命名空间，输入以下命令：\n\n\n\nNumPy\n\n\n上述命令构造了一个4*4的随机数组（随机数组在不同计算机上输出可能不同）。\n在NumPy函数库中存在两种不同的数据类型（矩阵matrix和数组array），二者都可以用于处理行列表示的数字元素。虽然看起来很相似，但在这两个数据类型上执行相同的数学运算可能得到不同的结果，其中的矩阵matrix与MATLAB中matrices等价。\n调用mat()函数可将数组转换为矩阵：\n\n\n\nmat\n\n\n如何进行矩阵求逆呢？不记得或者没学过矩阵求逆也没关系，NumPy库.I操作符可以很方便地算出矩阵的逆运算：\n\n\n\n逆矩阵\n\n\n接着执行矩阵乘法，得到矩阵与其逆矩阵相乘的结果：\n\n\n\n矩阵乘法\n\n\n结果应该是单位矩阵，除了对角线元素是1，4*4矩阵的其他元素应该全是0。上面实际输出略有不同，矩阵里还留下了很多数值非常小的元素，这是计算机处理误差产生的结果，我们来看一下误差值：\n\n\n\n误差\n\n\n函数eye(4)创建4*4的单位矩阵。\n只要能够顺利完成上面的例子，你就已经正确地安装并初步使用了NumPy函数库。后面我们会对它有更深的了解。\n\nDid you find this page helpful? Consider sharing it 🙌"
  },
  {
    "objectID": "blog/2020/01/04/index.html",
    "href": "blog/2020/01/04/index.html",
    "title": "Python初体验",
    "section": "",
    "text": "目前Python这门语言有多火也不用多说，各种公众号推送制造的焦虑让你感觉实在不学不行，接下来我们就来体验一下别人口中的这门似乎很神奇的编程语言。\n\n创建Python脚本\nPython的安装这里就不说了，当然如果你是mac用户，恭喜你的笔记本自带了Python2（前几天官方已停止对2的更新了）。如何在Python shell中简单地运行代码呢？Windows用户打开命令行窗口，mac用户打开终端，输入“python3”，按下回车键，就能看见Python提示符（&gt;&gt;&gt;）：\n\n\n\nPython提示符\n\n\n面对复杂多代码的任务，我们需要把代码都写在Python脚本上，然后运行脚本 ，提高工作效率。我们可以选择一个自己喜欢的文本编辑器，可供选择的有很多：Spyder、Pycharm、Jupyter notebook、Visual Studio code等。打开编辑器，一般我们将 #!/usr/bin/env python3 作为第一行。以井号开头的代码行为注释行，Windows系统不读取也不执行该行代码，但是像macOS这样的基于Unix的系统会根据这一行来找到执行该脚本的Python版本，加入这一行可以使你的脚本在不同操作系统之间具有可移植性。我们将上面这俩行代码放到Pycharm中，保存为first-script.py文件，这就是一个简单的Python脚本了。\n\n\n运行Python脚本\n对于在编辑器内运行，编辑器会有一个绿色三角运行按钮，点击一下即可运行输出：\n\n\n\nPycharm运行按钮\n\n\n当然，我们也可以选择在命令行或者终端中运行脚本：打开命令行或者终端，提示符会是一个具体的文件夹，即目录，如mac：/Users/luzhen。我们将脚本保存在桌面上，同时在终端中切换到桌面目录：\n\n\n\n终端切换目录\n\n\nmac上下一步就是为脚本添加执行权限，输入命令：chmod +x first-script.py。chmod是一个Unix命令，表示改变访问权限（change access mode）。+x表示在访问设置中添加执行权限，而非读、写权限。这样Python就可以执行脚本了。mac上只要你在一个脚本上运行了chmod命令，以后就可以随意运行该脚本，无需第二次执行chmod命令。\n接下来就可以运行脚本了：\n\n\n\n终端运行脚本\n\n\n可以看到终端窗口已经完成了脚本输出的打印，脚本运行成功！当然，Windows上还有一种运行方法，直接输入\\(python3 first-script.py\\)，也可成功执行脚本，mac上同样适用。\n\n\n与命令行交互的几个小技巧\n\n使用向上箭头键得到以前的命令\n\n在命令行和终端窗口中，你可以通过按向上箭头键找到以前输入的命令，可以减少每次运行Python脚本时必需的输入量，特别是当Python脚本的文件名特别长或需要在命令行上提供额外的参数（比如输入文件名或输出文件名）的时候。\n\n用Ctrl+c停止脚本\n\n我们已经学会了运行脚本，那么如何提前中断和停止Python脚本呢？Windows是Ctrl+C，mac是Control+c，就可以停止通过命令开始的进程。（进程：计算机对一系列命令的处理过程。对于一个脚本或程序，计算机将它解释成一个进程，如果这个程序非常复杂，就解释成一系列进程，这些进程可以顺序执行，也可以并发执行。）\n\n读懂出错信息并找到解决方案\n\n当窗口显示了错误信息时，我们先读懂出错信息。某些情况下，出错信息中明确指出了代码中出现错误的行，我们可以集中精力解决这一行的错误（你的文本编辑器应该设置成显示行号，可以在网上搜索一下）。出错信息也是编程的一部分，学会编程也包括学会如何有效地调试程序错误。最好的做法是将整个错误信息（至少是信息的主要部分）复制到搜索引擎上，看看别人是如何调试这种错误的。\n这样以后，接下来我们就可以来了解认识Python的语言基础要素了。人生苦短，一起学习Python。\n\nDid you find this page helpful? Consider sharing it 🙌"
  },
  {
    "objectID": "blog/2020/01/01/index.html",
    "href": "blog/2020/01/01/index.html",
    "title": "鼠年加油",
    "section": "",
    "text": "回望一幕幕送别 何尝不轻描淡写 独自出姑苏城外 流光未曾相约 他日他乡重逢 风轻柔河流缓缓\n\n\n新年快乐，鼠年加油 没有珍惜的时间，2020开始去珍惜 没有完成的事情，2020开始去完成 从来不相信鸡汤，也不制造鸡汤 等2021年再回顾这一年 希望感受到的不再是很多尴尬的空白 每到新的一年，你的朋友圈是不是也有刷屏的感慨和祝福 一年初始，心愿是美好的 如果能坚持做下去，该是多么圆满 新年少偷点懒，多学习新东西 多看看书，多写点公众号😂 多做一些分享，和小伙伴们共同进步 希望每个你新年新气象，活成你想要的样子 感谢你的关注❤️\n\n\nDid you find this page helpful? Consider sharing it 🙌"
  },
  {
    "objectID": "blog/2019/11/28/index.html",
    "href": "blog/2019/11/28/index.html",
    "title": "统计学是干嘛的？",
    "section": "",
    "text": "统计学之所以存在，关键的原因只有一个，那就是变异及由此产生的抽样误差。没有变异，没有抽样误差，就没有统计学存在的理由。当我们把多个随机结果放在一起的时候，却能发现一定的规律性。正是因为这种规律的存在，所以我们仍然可以在变异中寻找规律，这也正是统计学的主要目的：从各种看似杂乱的现象中找出潜在的规律。\n\n抽样调查\n既然是规律，那就一定要在大多数人中存在，只在一小部分人中存在的现象不是规律，而是偶然，因为更多的是大多数人没有存在该现象，这才是规律。要证明一种现象是不是真正的规律，需要在大量人群中进行验证。由于我们无法接触到理论意义上的总体，因而我们换一种思路，调查部分具有代表性的样本，然后用统计学方法将样本的结果推广到总体，这就是我们所说的抽样调查。\n\n\n统计推断与参数估计\n统计学通常利用样本数据来推断总体结果，就是我们所说的用样本统计量推断总体参数。总体参数是客观存在的，经典的频率主义学派认为，总体参数是一个客观存在且固定的数值，而贝叶斯学派认为连总体参数自身也是个随机变量，所以也需要我们去估计。样本随机，样本统计量也是随机的，用它来估计总体参数，估计结果会存在一定的误差。但科学合理的抽样调查，其推断的结果是可靠的。偏差的样本会导致偏差的结论。样本必须足够代表总体。当然还需要考虑其他因素，比如调查员的水平、总体人群的变化等影响因素。\n\n\n抽样误差\n然而，即使代表性非常好的样本，也是无法真正等同于总体的，总会存在一定的抽样误差。样本统计量之间的差异就反映了抽样误差。由于抽样误差的存在，如果用样本统计量直接估计总体参数，那么肯定会有一定的偏差。所以在估计总体参数时需要考虑到抽样误差带来的偏差，因而我们在点估计之外，用置信区间来估计总体参数。抽样误差带来的偏差是多大呢？在实际中，我们不可能通过多次抽样，计算每个样本间统计量的差异大小从而去估计偏差大小，我们只能通过一次样本计算。这种根据一次样本计算抽样误差的大小就是标准误（standard error）。标准误几乎在所有统计方法中都会出现，因为它可以提示结果的可靠性：如果标准误较小，则说明抽样误差小，这意味着样本很稳定，对总体的代表性很好，由此推论结果较为可靠；如果标准误较大，则说明抽样误差大，提示样本代表性不强，这种情况下一般需要加大样本量，否则结果不可靠。\n\nDid you find this page helpful? Consider sharing it 🙌"
  },
  {
    "objectID": "blog/2019/12/11/index.html",
    "href": "blog/2019/12/11/index.html",
    "title": "因果推断",
    "section": "",
    "text": "因果推断\n禁止言论就意味着禁止了思想，同时也扼杀了与此相关的原则、方法和工具。\n\n\n干预\ndo算子表明正在进行主动干预而非被动观察，这一概念在经典统计学中没有涉及。临床试验中使用do算子来确保观察到的病人存活期的变化能完全归因于药物本身，而没有混杂其他影响寿命长短的因素。如果不进行干预而让病人自行决定是否服用该药物，那么其他因素就可能会影响病人的决定，而服药和未服药的两组病人的存活期的差异也就无法再被仅仅归因于药物。例如，假设只有重症期的病人服用了这种药，那么两组之间的比较结果实际上反映的是其病情的严重程度，而非药物的影响。在数学上，我们把自行服药病人的生存期的观测概率称为条件概率，这里的概率是以观察到病人服用药物为条件的。【观察到】和【进行干预】是有本质的区别的。我们不认为气压计读数下降是风暴来临的原因，因为观察到气压计读数下降意味着风暴来临的概率增加，但人为使气压计读数下降并不能影响风暴来临的概率。因果推断要做的就是如何在不实际实施干预的情况下预测干预的效果。\n\n\n反事实\n假如某人在服用某种药物一个月后死亡，我们现在要关注的问题就是这种药物是否导致了他的死亡。为了回答这个问题，我们需要假设：如果他没服用这种药物，是否会避免死亡？反事实推理输出有关反事实世界的答案。\n语言会塑造思想。你无法回答一个你提不出来的问题；你也无法提出一个你的语言所不能描述的问题。\n\nDid you find this page helpful? Consider sharing it 🙌"
  },
  {
    "objectID": "blog/2020/01/02/index.html",
    "href": "blog/2020/01/02/index.html",
    "title": "因果推断-2",
    "section": "",
    "text": "统计值的不确定性意味着什么？\n统计推断利用统计方法生成一个问题答案的实际估计值，并给出对该估计值的不确定性大小的统计估计。这种不确定性反映了样本数据集的代表性以及可能存在的测量误差或数据缺失。数据永远是从理论上无限的总体中抽取的有限样本。我们无法避免根据样本测量的概率无法代表整个总体的相应概率的可能性。统计学提供了很多方法来应对这种不确定性，包括极大似然估计、倾向评分、置信区间、显著性检验等。\n\n\n相关与独立\n以因果模型的路径图(因果图)来表示的变量之间的听从模式通常会导向数据中某种显而易见的模式或者相关关系。“A和B之间没有连接路径”翻译成统计语言，就是“A和B相互独立”，即发现A的存在不会改变B发生的可能性。\n\n\n想象力与规划\n历史学家尤瓦尔·赫拉利在他的《人类简史》一书中指出，人类祖先想象不存在之物的能力是一切的关键，正是这种能力让他们得以交流得更加顺畅。在获得这种能力之前，他们只相信自己的直系亲属或者本部落的人。而此后，信任就因共同的幻想(例如信仰无形但可想象的神，信仰来世，或者信仰领袖的神性)和期许而延伸到了更大的群体。我们的智人祖先新掌握的因果想象力使他们能够通过一种被我们称为“规划”的复杂过程更有效地完成许多事情，例如他们可以通过想象和比较几个狩猎策略的结果来完成一次狩猎活动。而要做到这一点，思维主体必须具备一个可供参考并且可以自主调整的关于狩猎现实的心理模型。心理模型是施展想象力的舞台，它使我们可以通过对模型局部的自主调整修改来试验重估不同情景的概率，人类的心理模型因而具有一种模块性，其涉及预测对环境进行刻意改变后的结果，并根据预测结果选择行为方案以催生出自己所期待的结果。\n\nDid you find this page helpful? Consider sharing it 🙌"
  },
  {
    "objectID": "blog/2020/01/05/index.html",
    "href": "blog/2020/01/05/index.html",
    "title": "可惜没如果——因果关系的三个层级",
    "section": "",
    "text": "前言\n我们平时在统计中最常接触到的就是相关关系，而在因果推断中，实际上相关关系是处在因果关系三个层级的最低层级。正如我们所熟知的，相关不能说明因果，我们从观察到的数据中得到的相关对于因果的解释并不能起到直接的作用。\n\n\n因果关系的三个层级\n\n第一层级：关联\n我们一般通过观察寻找规律。如果观察到某一事件改变了观察到另一事件的可能性，我们便说这一事件与另一事件相关联。更进一步地，我们基于被动观察做出预测。\n典型的数据预测问题就是：“如果我观察到···会怎样？”例如，一家商店可能会问你：“购买牙膏的顾客同时购买牙线的可能性有多大？”这类问题是统计学的安身立命之本，统计学家主要通过收集和分析数据给出答案。\n我们可以这样解答：首先采集所有顾客购物行为的数据，筛选出购买牙膏的顾客，计算出当中购买牙线的人数比例。这个比例也就是我们所说的条件概率，用来反映(针对大数据)买牙膏和买牙线两种行为之间的关联程度，即P(牙线|牙膏)。典型常用的关联度量方法即相关分析或回归分析，具体操作就是将一条直线拟合到数据点集中，再去确定这条直线的斜率。\n我们在学习统计的时候，几乎所有老师都会和你说：“相关不代表因果。”统计学本身不能告诉我们，牙膏或牙线哪个是因，哪个是果。但是从商店的角度看，因果这件事并不重要——好的预测无需好的解释。\n当前的机器学习程序(包括应用深度神经网络的程序)几乎仍然完全是在关联模式下运行的。它们由一系列观察结果驱动，致力于拟合出一个函数，就像我们试图用点集拟合出一条直线一样。深度神经网络为拟合函数的复杂性增加了更多的层次，但其拟合过程仍然由原始数据驱动。如果无人驾驶汽车的程序设计者想让汽车在新情况下做出不同的反应，他就必须明确地在程序中添加这些新反应的描述代码，否则机器没有应对新情况的灵活性和适应性。\n\n\n第二层级：干预\n在第一层级中，我们基于被动观察发现规律，做出预测。而当我们开始寻求主动对环境做出改变时，我们就迈上了因果关系的第二层级。这一层级的一个典型问题是：“如果我们把牙膏的价格翻倍，牙线的销售额将会怎样？”问出这个问题的时候，我们实际上已经脱离了收集到的数据本身，而要对数据的环境做出干预。我们把这样的问题记作P(牙线|do(牙膏))，即“如果我们实施…行动，将会怎样”。\n毫无疑问，干预比关联更高级，因为其不仅涉及被动观察，还涉及主动改变现状。无论你的数据集有多大、神经网络有多深，只要你使用的是被动收集的数据，就无法回答有关干预的问题。从统计学中学到的任何方法都不足以让我们明确表述类似“如果价格翻倍将会发生什么”这样的问题，更谈不上回答了。\n为什么我们无法仅仅通过观察来回答牙线的问题呢？为什么不直接进入存有历史购买信息的数据库中，看看在牙膏价格翻倍的情况下对应的牙线的销售情况呢？原因在于，在历史销售信息中，牙膏涨价可能是出于完全不同的原因，如产品供不应求，其他商店也不得不涨价等。而我们只想刻意干预牙膏价格，这一结果就可能与历史上顾客在别处买不到便宜牙膏时的购买行为大相径庭。简单地说，我们只想知道单纯的牙膏涨价这个因所对应的牙线的果，而历史数据中各种影响因素完全超出了我们所提出问题的范畴，因而无法仅仅利用观察历史数据来回答干预的问题。\n因果推断则可以帮助我们解决这一问题。\n我们知道预测干预结果的一种非常直接的方法是：在严格控制的条件下进行实验。更加有趣的是，一个足够强大准确的因果模型可以在不进行实验的前提下，利用第一层级（关联）的数据来回答第二层级（干预）的问题。\n日常生活中，我们一直都在实施干预，尽管我们不会这么一本正经地称呼它。当我们服用阿司匹林试图治疗头痛时，就是在干预一个变量（人体内阿司匹林的量），以影响另一个变量（头痛的状态）。如果我们关于阿司匹林治愈头痛的因果知识是正确的，那么我们的结果变量的值将会从“头痛”变为“不头痛”。\n\n\n第三层级：反事实\n但是到此仍然不能回答所有我们感兴趣的因果问题：现在已经不头痛了，是因为我吃的阿司匹林么？是因为我吃的食物么？是因为我心情变好了么？\n正是这些问题将我们带入到因果关系的第三层级：反事实。要回答以上问题，我们就必须回到过去改变历史，“假如我们没有服用过阿司匹林，会怎样？”世界上没有哪个实验可以撤销对一个已接受过治疗的人所进行的治疗，进而比较治疗与未治疗两种条件下的结果。\n数据就是事实，而在反事实世界里，观察到的事实被完全否定。回到牙膏的例子，第三层级的问题是：“假如我们把牙膏价格翻倍，之前买了牙膏的顾客仍然选择购买的概率是多少？”在这个问题中，我们所做的就是将真实的世界（我们知道顾客以当前的价格购买了牙膏）和虚构的世界（牙膏价格翻倍）进行对比。\n“倘若那天，把该说的话好好说，该体谅的不执着，你会怎么做？”对这类问题的回答让我们得以从历史和他人的经验中获取经验教训。从想象的反事实中，我们获得了灵活性、反省能力以及改善行为的能力。\n因果关系第三层级的典型问题就是：”假如我当时做了…会怎样？“和“为什么？”两者都涉及观察到的世界与反事实世界的比较。仅靠干预实验无法回答这样的问题。\n\nDid you find this page helpful? Consider sharing it 🙌"
  },
  {
    "objectID": "blog/2020/01/08/index.html",
    "href": "blog/2020/01/08/index.html",
    "title": "如何用Python自编k-近邻算法？",
    "section": "",
    "text": "k-近邻算法概述\n简单地说，kNN依据不同特征值之间的距离进行分类。它不具有显式的学习过程，实际上是利用训练数据集对特征向量空间进行划分，并作为其分类的模型，即我们知道训练集中每一数据与所属分类的对应关系，输入没有标签的新数据后，将新数据的每个特征与训练集中数据对应的特征进行比较，然后算法提取训练集中特征最为相似数据的分类标签，选择k个最相似数据中出现次数最多的类别作为新数据的分类。\n\n\n自编kNN函数\n\n\n\nkNN\n\n\nclassify()函数有4个输入参数：待分类的输入向量inX，训练集dataSet，训练集标签向量labels，参数k为选择最近数据点个数，其中，inX维度为1xN，dataSet维度为MxN，labels维度为1xM，k为奇数。\ndataSet.shape以元组形式返回训练集维度(M, N)，dataSetsize为训练集的样本个数M。这里距离度量采用欧式距离，因而tile函数将输入数据重复M行1列（从而与训练集维度相同），分别和训练集中的每个数据点对应特征相减再平方，再按行相加，不保持其二维特性，即得输入数据与训练集中每个数据点之间的欧式距离。\n计算完距离之后，argsort函数对距离按照从小到大的次序排列，并返回排序后对应的原始索引值。使用for循环确定前k个距离最小元素所属的类别voteIlabel，使用get函数按照字典classCount键值取得相应的字典值，如果字典中存在这个键，get函数就返回对应的字典值，如果不存在，则返回0，用这种方式计数k个数据中每个标签出现的次数。因而字典classCount中键值为标签，字典值为k个标签对应的个数。\n使用sorted函数对字典classCount进行排序：items函数同时引用字典的键和值，结果是一个列表，其中包含的是键-值对形式的元组。由于字典没有隐含排序，我们可以按照字典的键或字典值来排序，这里的key就是排序的规则，关键字函数设置用于排序的关键字。使用operator模块中的itemgetter函数对列表按照每个元组第二个索引位置（即字典值，标签个数）进行排序，reverse=True对应降序。所以最后返回sortedClassCount列表中第一个元组的第一个值，即在k个标签中出现次数最多的标签，这样即完成了一个简单的kNN算法。\n\n\n创建训练集\n\n\n\n训练集\n\n\n我们创建了一个简单的训练集，有4组数据，每组数据有两个我们已知的属性/特征值。向量labels包含了每个数据的标签信息，labels包含的元素个数等于group矩阵行数。\n\n\n运行kNN\n我们需要在脚本中导入两个模块NumPy和运算符operator（kNN执行排序操作时使用到）：\n\n\n\nimport modules\n\n\n保存脚本文件，改变当前路径到存储脚本文件位置，进入Python：\n\n\n\nrun kNN\n\n\n输出结果应该是B，也可以改变输入数据运行。这样，我们已经构造了一个简单的kNN分类器。\n\nDid you find this page helpful? Consider sharing it 🙌"
  },
  {
    "objectID": "blog/2020/01/10/index.html",
    "href": "blog/2020/01/10/index.html",
    "title": "Python基础要素之字符串",
    "section": "",
    "text": "字符串是Python中的另一种基本数据类型。它通常是指人类可以阅读的文本，但更广泛地说，它是一个字符序列，并且字符只有在组成这个序列时才有意义。一些对象看上去是数值，但实际上是字符串，比如邮政编码，你对邮政编码做加减乘除是没有意义的，所以最好在代码中将其作为字符串来处理。接下来介绍用于字符串管理的一些模块、函数和操作。\n\n字符串\n字符串可以包含在单引号、双引号、3个单引号或3个双引号之间：\n\n\n\n字符串\n\n\nOutput #14展示了一个包含在单引号之间的简单字符串。\n\n\n+、*、len()\n\n\n\noperator\n\n\nOutput #18展示了使用+操作符将两个字符串相加。+操作符按照原样相加，如果你想在结果字符串中留出空格的话，就必须在原字符串中加上空格（Output #18在字母a后加了空格），Output #19中的*操作符也是这样，其将字符串重复一定的次数。Output #20展示了使用内置函数len来确定字符串中字符的数量。len函数将空格与标点符号都计入字符串长度，所以结果是23个字符。\n\n\nsplit函数\n使用split函数将一个字符串拆分成一个子字符串列表（列表中的子字符串正好可以构成原字符串）。split函数可以在括号中使用两个附加参数，第一个附加参数表示使用哪个字符进行拆分，第二个参数表示进行拆分的次数。\n\n\n\nsplit\n\n\n在Output #21中，括号中没有附加参数，所以split函数使用空格字符（默认值）对字符串进行拆分。因为这个字符串有5个空格，所以被拆分成具有6个子字符串的列表。Output #22中，第一个附加参数是用空格来拆分字符串，第二个附加参数是2，说明只想用前两个空格进行拆分，生成一个带有3个元素的列表。第二个参数会在我们解析数据的时候派上用场。举例来说，你可能会解析一个日志文件，文件中包含时间戳、错误代码和由空格分隔的错误信息。在这种情况下，可以使用前两个空格进行拆分，解析出时间戳和错误代码，但是不使用剩下的空格进行拆分，以便完整地保留错误信息。Output #23和Output #24中，括号中的附加参数是都好，split函数在出现逗号的位置拆分字符串。\n\n\njoin函数\n使用join函数将列表中的子字符串组合成一个字符串。join函数将一个参数放在join前面，表示使用这个字符（或字符串）在子字符串之间进行组合。print(\"Output #25: {0}\".format(','.join(string2_list)))这里join函数将子字符串组合成一个字符串，子字符串之间为逗号。因为列表中有6个子字符串，所以组合后有5个逗号。\n\n\nstrip函数\n使用strip、lstrip和rstrip函数从字符串两端删除不想要的字符，这三个函数都可以在括号中使用一个附加参数来设定要从字符串两端删除的字符（或字符串）。\n\n\n\nstrip\n\n\n可以看到，string3的左侧有几个空格，右侧包含制表符（、几个空格和换行符（）。在Output #26中，你会看到句子前面有空白，句子下面有一个空行，句子后面有你看不到的制表符和空格。{0:s}中的s表示传入的值应该格式化为一个字符串。下面展示了从字符串两端删除其他字符的方法，将要删除的字符作为strip函数的附加参数即可：通过将美元符号、下划线、短划线和加号作为附加参数，通知程序从字符串两端删除它们。\n\n\nreplace函数\n使用replace函数将字符串中的一个或一组字符替换为另一个或另一组字符。replace函数在括号中使用两个附加参数，第一个参数作用是在字符串中查找要替换的字符或一组字符，第二个参数是要用来替换的一个或一组字符。\n\n\n\nreplace\n\n\nOutput #32展示了使用replace函数将字符串中的空格替换为!@!。Output #33展示了使用逗号替换字符串中的空格。\n\n\nlower、upper、capitalize函数\nlower和upper函数分别用来将字符串中的字母转换为小写和大写，capitalize函数对字符串中的第一个字母应用upper函数，对其余字母应用lower函数：\n\n\n\nlower-upper-capitalize\n\n\nOutput #36对句子的首字母大写。Output #37将capitalize函数放在一个for循环中，对string8_list这个列表中的每个元素首字母大写，其余字母小写。\n\nDid you find this page helpful? Consider sharing it 🙌"
  },
  {
    "objectID": "blog/2020/01/16/index.html",
    "href": "blog/2020/01/16/index.html",
    "title": "R语言的初体验",
    "section": "",
    "text": "R语言是从起源于贝尔实验室的S统计绘图语言演变而来的。与S语言类似，R也是一种为统计计算和绘图而生的语言和环境，它是一套开源的数据分析解决方案，由一个庞大且活跃的全球性研究型社区维护。\n\nR的特点总结\n\n软件本身及程序包的源代码公开；\n涵盖了多种行业数据分析中几乎所有的方法；\n任意一个分析步骤的结果均可被轻松保存、操作，并作为进一步分析的输入；\nR拥有顶尖水准的制图功能；\nR可运行于多种平台上，包括Windows、UNIX和Mac OS X；\n可轻松地从各种类型的数据源读写数据，包括文本文件、数据库管理系统、统计软件，乃至专门的数据仓库；\n每个函数都有统一格式的帮助和运行实例。\n\n\n\nR的帮助系统\nR提供了大量的帮助功能，学会如何使用这些帮助文档有助于编程。R的内置帮助系统提供了当前已安装包中所有函数的细节、参考文献以及使用示例。帮助文档可以通过以下函数进行查看。\n\nhelp.start()：打开帮助文档首页\nhelp(foo)或?foo：查看函数foo的描述说明等帮助信息(如返回值)\nhelp.search(‘foo’)或??foo：以foo为关键词搜索本地帮助文档\nRSiteSearch(‘foo’)：以foo为关键词搜索在线文档和邮件列表存档\napropos(‘foo’, mode=‘function’)：列出名称中含有foo的所有可用函数，在只知道函数的部分名称时搜索可用\nexample(foo)：查看函数foo的使用范例\ndata() 列出当前已加载包中所含的所有可用示例数据集\nvignette() 列出当前已安装包中所有可用的vignette文档\nvignette(‘foo’) 为主题foo显示指定的vignette文档\n\n\n\n工作空间和目录\n工作空间（workspace）是当前R的工作环境，存储着所有你定义的对象（向量、矩阵、函数、数据框和列表）。在一个R会话结束时，你可以将当前工作空间保存到一个镜像中，以便在下次启动R时自动载入它。当前的工作目录（working directory）是R用来读取文件和保存结果的默认目录。\n用于管理工作空间和目录的部分标准命令如下：\n\ngetwd()：查看当前工作目录\nsetwd()：重新设定当前工作目录。如果需要读入一个不在当前工作目录下的文件，需要在调用语句中写明文件的完整路径。setwd()命令的路径中使用正斜杠/。R将反斜杠。即使在Windows平台上运行R，在路径中也要使用正斜杠。\nls()：列出当前工作空间中的对象\nrm(objectlist)：删除一个或多个对象\noptions()：显示或设置当前选项\nhistory(#)：显示最近使用的#个命令(默认值为25)\nsavehistory(‘myfile’) 保存命令历史到文件myfile.Rhistory中\nloadhistory(‘myfile’) 载入命令历史文件myfile.Rhistory\nsave.image(‘myfile’) 保存工作空间到文件myfile.RData中\nload(‘myfile’) 读取工作空间myfile.RData到当前会话中\nsave(objectlist, file=‘myfile’) 保存指定对象到一个文件中\nq()：结束对话退出R，并询问是否保存工作空间\n\n\n\nR包\nR提供了大量备用功能，通过可选模块的下载和安装来实现。目前有15364个称为包的用户贡献模块可从https://cran.r-project.org/web/packages下载。这些包提供了横跨各种领域、数量庞大的功能，包括分析地理数据、处理蛋白质质谱，甚至是心理测验分析的功能。\nR包是R函数、数据、预编译代码以一种定义完善的格式组成的集合，具有详细的说明和示例。计算机上存储包的目录称为库（library）。.libPaths()显示库所在位置，library()则可以显示库中包。\n第一次安装一个包，使用命令install.packages()即可，在括号中输入要安装的包名称，一个包仅需安装一次。update.packages()更新已安装的包。installed.packages()列出已安装的包的相关信息(如版本号、依赖关系等)。Windows下的R包是经过编译的zip文件，安装时不要解压缩。安装路径为“Pacakges&gt;install packages from local files”，选择本地磁盘上存储zip包的文件夹。\n包的安装是指从某个CRAN镜像站点下载它并将其放入库中的过程。安装好以后，必须被载入到会话中才能使用包，需要使用library()函数载入该包。在一次应用中，包只需载入一次，如果需要，我们可以自定义启动环境以自动载入会频繁使用的包。search()显示已加载并可使用的包。help(‘package_name’)输出某个包的简短描述以及包中可用的函数名称和数据集名称的列表，help()查看包中任意函数或数据集的描述，R的帮助系统包含了每个函数的一个描述（同时带有示例），每个数据集的信息也被包括其中。\n\n\nR的使用\nR是面向对象的，区分大小写的解释型数组编程语言。R中多数功能是由程序内置函数、用户自编函数和对对象的创建和操作所实现的。一次交互式会话期间的所有数据对象都被保存在内存中。R语句由函数和赋值构成，R使用 -&gt; 而非 = 作为赋值符号。R也允许使用 = 为对象赋值，但是它不是标准语法，某些情况下会出现问题。R具有完备的数据存取、管理、分析和显示等功能，将数据处理和统计分析融为一体。以后我们继续学习R语言。\n\nDid you find this page helpful? Consider sharing it 🙌"
  },
  {
    "objectID": "blog/2020/01/18/index.html",
    "href": "blog/2020/01/18/index.html",
    "title": "Python正则表达式与模式匹配",
    "section": "",
    "text": "很多商业分析都依赖模式匹配，也称为正则表达式（regular expression）。举例来说，我们可能需要分析一下来自深圳的所有订单。此时，你需要识别的模式就是“深圳”这个词。同样，你可能需要分析来自某个供应商的商品质量，此时你要识别的模式就是供应商的名字。\nPython包含了re模块，它提供了在文本中搜索特定模式/正则表达式的强大功能。要在脚本中使用re模块提供的功能，我们需要在脚本上方加入代码import re。\n导入re模块后，可以使用一大波函数和元字符来创建和搜索任意复杂的模式。元字符（metacharacter）是正则表达式中具有特殊意义的字符，使正则表达式能够匹配特定的字符串。\n常用的元字符包括 |、()、[]、.、*、+、?、^、$和(?P&lt;name&gt;)。如果你在正则表达式中见到这些字符，要知道程序不是要搜索这些字符本身，而是要搜索它们要描述的东西。\nre模块包含了很多有用的函数，用于创建和搜索特定的模式。一起来看一个示例代码：\n\n\n\nsample code\n\n\n第一行赋值字符串变量string，下一行将字符串拆分为列表，列表中的每个元素都是一个单词。使用re.compile和re.I函数以及用r表示的原始字符串，创建一个名为pattern的正则表达式。re.compile函数将文本形式的模式编译成为正则表达式（正则表达式不是必须编译的，但编译是个好习惯，因为这样可以显著地提高程序运行速度），re.I函数确保模式是不区分大小写的，即能同时在字符串中匹配“The”和“the”，原始字符串标志r可确保Python不处理字符串中的转义字符（如、），这样在进行模式匹配是，字符串中的转义字符和正则表达式中的元字符就不会有意外的冲突。利用for循环在列表变量sring_list的各个元素之间进行迭代，取出列表中所有的单词，使用re.search函数将每个单词与正则表达式进行比较，如果相匹配，那么count的值就加1。print语句打印出正则表达式在字符串汇总找到模式“The”（不区分大小写）的次数。\n再看另一个示例：\n\n\n\nsample code 2\n\n\n这个示例想要打印出相匹配的字符串，而不是相匹配的次数，这里使用到了(?P&lt;name&gt;)元字符和group函数。(?P&lt;name&gt;)元字符使匹配的字符串可以在后面的程序中通过组名符号name来引用，这里称为match_word。后面if语句中使用了group函数获取分段截获的字符串，如果相匹配，那么就在search函数返回的数据结构中找出match_word组合中的值，并打印出来。\n最后一个示例：\n\n\n\nsample code 3\n\n\n我们演示了使用re.sub函数在文本中用一种模式替换另一种模式。将正则表达式赋给变量string_to_find不是必需的，但若正则表达式特别长或复杂的话，将它赋给一个变量，然后传入re.compile函数有助于理解。最后使用re.sub函数以不区分大小写的方式在变量string中寻找模式，将发现的每个模式替换成a。\n\nDid you find this page helpful? Consider sharing it 🙌"
  },
  {
    "objectID": "blog/2020/01/20/index.html",
    "href": "blog/2020/01/20/index.html",
    "title": "计算机概论3",
    "section": "",
    "text": "内存\n前面提到CPU所使用的数据都是来自内存（Main Memory），不论是软件程序还是文件数据，都必须要读入内存后CPU才能利用。个人电脑的内存主要组件为动态随机存取内存（Dynamic Random Access Memory，DRAM），随机存取内存只有在通电时才能记录与使用，断电之后数据就消失，因此我们也称这种RAM为挥发性内存。\nDRAM根据技术的更新又分好几代，而使用上较广泛的有所谓的SDRAM与DDR SDRAM两种。新一代的PC大多使用DDR内存。\n在某种意义上，内存的容量有时比CPU的速度还要重要。如果内存容量不够大的话将会导致某些大容量数据无法被完整地加载，此时已存在内存当中但暂时没有被使用到的数据就必须要先被释放，使得可用内存容量大于该数据，那份新数据才能够被加载。所以。通常越大的内存代表越快速的系统，这是因为系统不用常常释放一些内存中的数据。\n\n\nCPU的二级高速缓存\n除了内存外，个人电脑中还有许多类似内存的存储结构存在，最为我们所熟知的还有CPU内的二级高速缓存。我们现在知道CPU的数据都由内存提供，但CPU到内存之间还是得要通过内存控制器。如果某些很常用的程序或数据可以放置到CPU内存的话，那么CPU数据的读取就不需要跑到内存重新读取，这对于性能来说是一个很大的提升，这就是二级缓存的设计理念。新一代的CPU都有内置容量不等的L2缓存在CPU内部，以加快CPU的运行性能。\n\n\n只读存储器\n还记得你的电脑在开机的时候可以按下[Del]按键来进入一个名为BIOS（Basic Input Output System）的界面吧？BIOS是一个程序，这个程序是写死到主板上面的一个存储芯片中的，这个存储芯片在没有通电时也能够记录数据，这就是只读存储器（Read Only Memory，ROM）。ROM是一种非易失性的存储。\nBIOS对于个人电脑来说是非常重要的，它是系统在启动的时候首先会去读取的一个小程序。另外，固件（firmware）很多也是使用ROM来进行软件的写入（固件：固定在硬件上面的控制软件）。BIOS就是个固件，控制着启动时各项硬件参数的获取与启动设备的选择。\n\nDid you find this page helpful? Consider sharing it 🙌"
  },
  {
    "objectID": "blog/2020/01/22/index.html",
    "href": "blog/2020/01/22/index.html",
    "title": "R语言编程入门",
    "section": "",
    "text": "前言\n类似其他计算机高级语言，R用户只需要熟悉其命令、语句及简单的语法规则，就可以做数据管理和分析处理工作。R把大部分常用的复杂数据计算的算法作为标准函数调用，用户仅需要指出函数名及其必要的参数即可，这一特点使得R编程十分简单。\nR是面向对象的、区分大小写的解释型数组编程语言，输入后可直接给出结果。R中功能靠函数实现。R的函数分为“高级”和“低级”函数，高级函数可调用低级函数，这里的高级函数习惯上称为泛型函数。plot()就是泛型函数，可以根据数据的类型调用底层的函数，应用相应的方法绘制相应的图形。这就是面向对象编程的思想。\n\n\n数据集\n创建含有研究信息的数据集，这是任何数据分析的第一步。数据集通常是由数据构成的一个矩形数组，行表示观测，列表是变量。\nR可以处理的数据类型包括数值型（如100）、字符型（如“流光相约”）、逻辑型（TRUE/FALSE）、复数型（如2+3i）和因子型（表示不同类别）。R中有许多用于存储数据的结构，包括标量、向量、矩阵、数组、数据框和列表。多样化数据结构赋予了R极其灵活的数据处理能力。\n\n\n标量与向量\n标量可以看成是只含一个元素的向量，用于保存常量，如f=3、g=’US’和h=TRUE。\n向量是一系列元素的组合，用于存储数值型、字符型或逻辑型数据的一维数组。执行组合功能的函数c()可用来创建向量：\n\n\n\nvector\n\n\n提示：单个向量中的数据必须是相同的类型或模式（数值型、字符型或逻辑型），同一向量中不可混杂不同类型数据。\n通过在方括号中给定元素所处位置的数值访问向量中的元素，如a[c(2, 4)]用于访问向量a中第2个和第4个元素。\n\n\n矩阵\n矩阵是一个二维数组，和向量类似，其中元素必须类型相同，即一个矩阵中只能包含一种数据类型（数值型、字符型或逻辑型），可通过函数matrix()创建矩阵：\nmymatrix= matrix(vector,nrow = ,ncol = ,byrow = ,dimnames = list())\n其中，vector包含了矩阵的元素，nrow和ncol用于制定行和列的维数，dimnames包含了可选的、以字符型向量表示的行名和列名，byrow表明矩阵应当按行填充（byrow=T）还是按列填充（byrow=F），默认按列填充。\n\n\n\nmatrix\n\n\n我们可以使用下标和方括号来选择矩阵中的行、列或元素。X[i,]表示矩阵X中的第i行，X[,j]表示第j列，X[i, j]表示第i行第j个元素。\n\n\n数组\n矩阵都是二维的，仅能包含一种数据类型，当维度超过2时，需要使用数组，可通过函数array()创建：\nmyarray= array(vector,dimensions,dimnames)\n其中，vector包含了数组中的数据，dimensions是一个数值型向量，给出了各个维度下标的最大值，dimnames是可选的、各维度名称标签的列表。\n\n\n\narray\n\n\n数组是矩阵的一个自然推广，从数组中选取元素的方式与矩阵相同。\n\n\n数据框\n与通常在SAS、SPSS和STATA中看到的数据集类似，不同的列可以包含不同类型的数据。数据框是R中最常处理的数据结构，可使用函数data.frame()创建：\nmydata=data.frame(col1,col2,col3,...)\n其中，列向量col1，col2，col3等可谓任何类型（如字符型、数值型或逻辑型）。\n选取数据框中元素的方式比较多，既可以使用下标记号，也可以直接指定列名，如patientdata\\(age，其中\\)被用来选取一个给定数据框中的某个特定变量。\n在每个变量名前都输入一次数据框名可能十分麻烦，我们可以联合使用函数attach()和detach()，或单独使用函数with()来简化代码。函数attach()将数据框添加到R的搜索路径中，R在遇到一个变量名后，将检查搜索路径中的数据框，以定位到这个变量，如\n\n\n\ndataframe\n\n\n函数detach()将数据框从搜索路径中移除，不会影响数据框本身。注意：函数attach()和detach()最好是分析一个单独的数据框，并且不太可能有多个同名对象。当同名时，原始对象将进行优先运算。另一种方式是使用函数with()，如\n\n\n\nwith function\n\n\n花括号{}之间的语句都针对该数据框执行，无需担心名称冲突；若仅有一条语句，花括号可以省略。函数with()的局限性在于赋值仅在此函数的括号内有效。若需要创建在with()结构外依然存在的对象，使用特殊赋值符号 -&gt;&gt; ，即可保存对象到全局环境中。\n\n\n列表\n列表是一些对象（成分）的有序集合，允许整合若干对象到单个对象名下，其中的对象可以是任何数据结构，如某个列表可以是若干向量、矩阵、数据框甚至其他列表的组合，使用函数list()创建列表：\nmylist=list(object1,object2,...)\n可以通过在双重方括号中指明代表某个成分的数字或名称来访问列表中的元素，如mylist[[4]]。由于列表允许以一种简单的方式组织和重新调用可能不相干的信息，且许多R函数的运行结果都是以列表的形式返回的，由分析人员决定需要取出其中哪些成分，列表是R中的重要数据结构。\n\nDid you find this page helpful? Consider sharing it 🙌"
  },
  {
    "objectID": "blog/2024/07/19/new_book/index.html",
    "href": "blog/2024/07/19/new_book/index.html",
    "title": "预测模型领域新书推荐",
    "section": "",
    "text": "首先，今天这篇不是软文哦。\n\n\n\n\n临床预测模型方法与应用\n\n\n非常高兴向大家推荐和我们课题组一直保持良好合作的荷兰 Utrecht University 王俊峰教授参与主编的新书《临床预测模型方法与应用》。\n王老师是临床预测模型领域内的专家，大家感兴趣的可以去看王老师的google scolar。在和王老师合作做项目的过程中，我也是收获很多，扫除了一些知识上的盲点和疑区。因而，对于关注我的同学们而言，如果有对临床预测模型感兴趣的，我也是非常推荐这本书。\n这本书由南京医科大学公共卫生学院的陈峰教授作序，主编人员都是在预测模型、生物统计领域内有着丰富经验和深刻见解的科研人员，王老师在我们合作的项目文章里也给与了我悉心的指导，北京天坛医院谷鸿秋教授也是刚刚作为一作发表了NEJM，这本书可以说是大咖云集了。\n\n\n\n序言\n\n\n\n\n\n序言\n\n\n相信很多做科研的同学，一直想找一本这个方向领域的权威且全面的中文书，这本书应该是一个不错的选择。如果是对预测模型感兴趣的小伙伴可以直接下单预定啦，也可以关注下8月份王老师在北大、复旦的讲座。这本书8月份会正式上市，目前可以扫码下图进行预定。\n\n\n\n预定\n\n\n\nDid you find this page helpful? Consider sharing it 🙌"
  },
  {
    "objectID": "blog/2024/08/04/workshop_001/index.html",
    "href": "blog/2024/08/04/workshop_001/index.html",
    "title": "星球第一期workshop上线",
    "section": "",
    "text": "我们星球正式上线第一期workshop啦！\n\n\n\nworkshop\n\n\n本期workshop主题是“Statistical Methods for Analysis with Missing Data”。本期workshop将从缺失数据的概念、缺失数据的类型、缺失数据的机制、缺失数据的影响、缺失数据的处理方法等方面展开讲解，帮助大家更好地理解缺失数据的问题，掌握缺失数据的处理方法。\n\n\n\nscreenshot\n\n\n目前暂定的安排是每周一节一小时，直到本期workshop的全部内容结束。\n\n\n\nused files\n\n\n后续我们会根据星球中同学们的需求，不定时地开展更多主题的workshop，具体时间请关注我们的公众号和星球，我们会在这两个平台上发布最新的信息。\n\nDid you find this page helpful? Consider sharing it 🙌"
  },
  {
    "objectID": "blog/2024/08/14/clinical_prediction_model/index.html",
    "href": "blog/2024/08/14/clinical_prediction_model/index.html",
    "title": "欢迎加入预测模型星球",
    "section": "",
    "text": "由来\n前面我们给大家推荐了这本几位非常厉害的教授老师主编的《临床预测模型方法与应用》，陆陆续续地，在各个平台上，大家都反馈已经收到了这本书，并且这本书还很大很厚，涵盖了方法学、操作、专题以及案例。\n\n\n\nfeedback\n\n\n那这时候，就有同学和我说，按照以往看书的习惯，收到书的前一俩周，还是可以翻翻，但是后面就会慢慢地放在那里，然后就不了了之了。\n所以，我们就想，能不能有一个平台，让大家一起学习这本书，一起讨论、交流，一起进步呢？\n\n\n预测模型星球\n\n\n\n知识星球\n\n\n向大家完整介绍下这个星球，也鼓励更多想要讨论交流的小伙伴进入到我的星球里面，大家一起愉快地学习。\n\n首先，你要有实体书，不然我们每周讨论学习某一个章节的时候，你可能会有点懵。\n\n购买的二维码链接在这里。\n\n知识星球的目的是和大家一起营造一个国内高质量的预测模型类研究的知识圈，让星球里的人能够受益。\n\n来到这里你会遇到一群志同道合的人，一起学习、相互交流、共同促进。这个交流圈高度专注于预测类研究领域。\n\n\n\ncontent\n\n\n\n我们会每周一起学习讨论这本书的一个章节，每周一次，每次一个小时或一个章节，讨论的内容会包括这一章的重点内容、难点、案例分析等。\n除了这本书，如果未来人数足够，我们还会不定期地邀请一些同样从事预测类研究的博士生、学者，来和大家分享他们的研究成果、经验、心得等。\n\n后续我们会根据星球中同学们的需求，不定时地开展更多主题的workshop，具体时间请关注我们的公众号和星球，我们会在这两个平台上发布最新的信息。\n\nDid you find this page helpful? Consider sharing it 🙌"
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "This page contains the source code, links, and slides for various workshops, talks, lectures, and presentations I’ve given."
  },
  {
    "objectID": "talks/index.html#section",
    "href": "talks/index.html#section",
    "title": "Talks",
    "section": "2024",
    "text": "2024"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello there!",
    "section": "",
    "text": "I am a PhD candidate in epidemiology and biostatistics at Sun Yat-sen University. I have made contributions to neurosyphilis and cancer research, and my work has been published in journals including eClinicalMedicine, JMIR Public Health and Surveillance, among others. Besides, I have patented a clinical diagnosis system for neurological syphilis (LU504466, rank 1/1).\nIn addition to my work on neurosyphilis and cancer, I have extensive experience applying advanced statistical concepts, artificial intelligence, and causal inference methods to conduct real-world research based on electronic health records. I have conducted multiple studies in the areas of COVID-19, HPV and HIV infections, and cardiovascular disease.\nTo date (May 10, 2024), I have published 25 papers in peer-reviewed journals, serving as the first author and co-first author for 10 of these."
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitæ",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "blog/2024/05/13/hierarchical_composite_endpoints/index.html",
    "href": "blog/2024/05/13/hierarchical_composite_endpoints/index.html",
    "title": "Hierarchical composite endpoints治疗效应的可视化",
    "section": "",
    "text": "复合终点\n有时，根据主要研究目的，我们很难从多个终点指标中选出其中某一个作为主要终点，此时，我们可以利用复合终点来作为主要终点。\nHierarchical composite endpoints (HCE)可以整合不同类型、不同重要性终点成一个有序终点指标，以表示患者经历的不同严重程度的终点。如，在固定随访的RCT中，outcomes of interest可以是death、hospitalization，而这两个终点存在严重程度的差异。很明显，死亡是最严重的。同样最终死亡的两个患者，生存时间更长，意味治疗效应更好；同样最终住院的两个患者，入院前时间更长，治疗效应更好；同样未住院的两个患者，某一实验室指标的change from baseline更大，效应更好。\n对于这种HCE，我们可以计算win odds(Gasparyan et al. 2021)来比较组间差异，然而，治疗效应的可视化受到复合终点的影响，不容易像单纯的生存曲线那样用合适的工具可视化出来。\n针对这一问题，AstraZeneca的Martin Karpefors等人提出了一种新的方法，即maraca plot(Karpefors, Lindholm, and Gasparyan 2023)。这种方法可以将复合终点中time to event(TTE)以及连续性终点的治疗效应可视化出来，同时也可以用来比较不同治疗组之间的差异。对应的R包可以方便地实现这一点。\n\n\nmaraca plot\nmaraca基于ggplot2，其中，对于TTE采用Kaplan-Meier曲线展示cumulative proportions，对于连续性终点可选用箱线图、violin plot以及scatter plot展示连续性分布。这种方法可以同时展示HCE的不同组成成分。\n来看一个例子。\n\nlibrary(maraca)\ndata(hce_scenario_a, package = \"maraca\")\ndata &lt;- hce_scenario_a\ndata |&gt; head()\n\n  SUBJID              GROUP GROUPN      AVAL0       AVAL    TRTP\n1      1          Outcome I      0 120.440921   120.4409  Active\n2      2 Continuous outcome  40000   3.345229 40003.3452 Control\n3      3 Continuous outcome  40000  22.802615 40022.8026  Active\n4      4          Outcome I      0 577.311386   577.3114 Control\n5      5         Outcome II  10000 781.758081 10781.7581  Active\n6      6        Outcome III  20000 985.097981 20985.0980 Control\n\n\n具体变量意义，大家可以查看?hce_scenario_a。\n可视化如下：\n\ncolumn_names &lt;- c(outcome = \"GROUP\", arm = \"TRTP\", value = \"AVAL0\")\ntte_outcomes &lt;- c(\"Outcome I\", \"Outcome II\", \"Outcome III\", \"Outcome IV\")\ncontinuous_outcome &lt;- \"Continuous outcome\"\narm_levels &lt;- c(active = \"Active\", control = \"Control\")\nmaraca_object &lt;- maraca(\n  data, tte_outcomes, continuous_outcome, arm_levels, column_names,\n  fixed_followup = 3*365, compute_win_odds = TRUE\n)\nAZ_colors &lt;- c(\"#830051\", \"#F0AB00\")\nplot(maraca_object, density_plot_type = \"default\") + theme_bw() +\n  scale_color_manual(values = AZ_colors) +\n  scale_fill_manual(values = AZ_colors)\n\n\n\n\n\n\n\n\n\n\n结果解释\n怎么看这张图？\n首先是x轴上HCE的5个组成成分，x轴上每个成分的长度大小，代表了患者达到不同成分终点的比例，可以看到，continuous outcome的比例最大，说明这个终点的患者所占比例最大。其次，cumulative percentage显示active组在四个TTE终点上是存在差异的。再然后是continuous outcome的分布，偏向x轴右侧代表change from baseline更大。而这些结合起来，就是win odds的结果，可以看到，和我们从可视化的角度看到的结果是一致的。\n代码已经放进了星球里。\n\n\n\n\n\n\n\n\n\n\nDid you find this page helpful? Consider sharing it 🙌\n\nGasparyan, S. B., E. K. Kowalewski, F. Folkvaljon, O. Bengtsson, J. Buenconsejo, J. Adler, and G. G. Koch. 2021. “Power and Sample Size Calculation for the Win Odds Test: Application to an Ordinal Endpoint in COVID-19 Trials.” Journal Article. Journal of Biopharmaceutical Statistics 31 (6): 765–87.\n\n\nKarpefors, M., D. Lindholm, and S. B. Gasparyan. 2023. “The Maraca Plot: A Novel Visualization of Hierarchical Composite Endpoints.” Journal Article. Clinical Trials (London, England) 20 (1): 84–88. https://doi.org/10.1177/17407745221134949.\n\nCitationBibTeX citation:@online{lu2024,\n  author = {Lu, Zhen},\n  title = {Hierarchical Composite {endpoints治疗效应的可视化}},\n  date = {2024-05-13},\n  url = {https://leslie-lu.github.io/blog/2024/05/13/hierarchical_composite_endpoints/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLu, Zhen. 2024. “Hierarchical Composite\nEndpoints治疗效应的可视化.” May 13, 2024. https://leslie-lu.github.io/blog/2024/05/13/hierarchical_composite_endpoints/."
  },
  {
    "objectID": "blog/2024/06/07/loss_function/index.html",
    "href": "blog/2024/06/07/loss_function/index.html",
    "title": "常用损失函数",
    "section": "",
    "text": "loss function\n在机器学习/深度学习任务中，衡量模型预测值与真实值之间的差异的指标称为损失函数。损失函数是模型训练的关键组成部分，它可以帮助我们优化模型参数，使得模型的预测值更加接近真实值。预测任务的目标也是最小化损失函数，如，我们利用反向传播算法等方法，通过更新损失函数相对于模型参数的梯度来最小化损失函数，提高模型的预测能力。此外，有效的损失函数还可以帮助我们平衡模型的偏差和方差，提高模型的泛化能力。\n依据预测任务的不同，损失函数可以分为回归任务和分类任务两大类。回归任务的损失函数通常是均方误差（MSE）或平均绝对误差（MAE），而分类任务的损失函数则有交叉熵损失函数、Hinge损失函数等。本文将介绍常用的损失函数及其应用场景。\n\n均方误差（MSE）\n均方误差（Mean Squared Error，MSE）是回归任务中最常用的损失函数之一，它衡量模型预测值与真实值之间的差异。MSE的计算公式如下：\n\\[\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n\\]\n可以看到，MSE是预测值与真实值之间差值的平方和的均值，它对较大差异分配更高的惩罚。MSE非负，越小，说明模型的预测值与真实值之间的差异越小，模型的预测能力越好。MSE对异常值敏感，因为它是差值的平方和，异常值的平方会放大差异，导致模型的预测能力下降。\n其在pytorch中的实现：\n\ntorch.nn.MSELoss(reduction='mean')\n\n\n\n平均绝对误差（MAE）\n平均绝对误差（Mean Absolute Error，MAE）是回归任务中另一种常用的损失函数。MAE的计算公式如下：\n\\[\nMAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n\\]\n相比于MSE，MAE是预测值与真实值之间差值的绝对值的均值，它对异常值不敏感，因为它是差值的绝对值的和，不会对某一异常值的差异分配过高的权重。MAE的值越小，说明模型的预测值与真实值之间的差异越小，模型的预测能力越好。\n针对MAE和MSE的优缺点，我们可以根据具体的任务需求选择合适的损失函数。如果任务需要重点关注异常值，可以选择MSE，否则选择MAE。\n\ntorch.nn.L1Loss(reduction='mean')\n\n\n\nHuber loss\nHuber loss是一种结合了MSE和MAE的损失函数，它在差值较小的情况下使用MSE，差值较大的情况下使用MAE。Huber loss的计算公式如下：\n\\[\nL_{\\delta}(y, \\hat{y}) = \\left\\{\n\\begin{array}{ll}\n\\frac{1}{2}(y - \\hat{y})^2, & \\text{if } |y - \\hat{y}| \\leq \\delta \\\\\n\\delta |y - \\hat{y}| - \\frac{1}{2} \\delta^2, & \\text{otherwise}\n\\end{array}\n\\right.\n\\]\n其中，\\(\\delta\\)是一个超参数，用于控制MSE和MAE之间的平衡。Huber loss对异常值不敏感，同时保留了MSE的平滑性，是一种较为稳健的损失函数。\n\ntorch.nn.SmoothL1Loss(reduction='mean')\n\n\n\n二元交叉熵损失函数（Binary Cross Entropy Loss）\n交叉熵损失函数（Cross Entropy Loss）是二分类任务中最常用的损失函数之一，我们前面也以及介绍过。交叉熵损失函数的计算公式如下：\n\\[\nL(y, \\hat{y}) = -\\frac{1}{n} \\sum_{i=1}^{n} y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)\n\\]\n其中，\\(y_i\\)是真实标签，\\(\\hat{y}_i\\)是模型预测的概率值。交叉熵损失函数对于模型预测的概率值和真实标签之间的差异进行了惩罚，使得模型更加关注预测正确的类别。交叉熵损失函数是一种凸函数，可以通过梯度下降等方法进行优化。\n\ntorch.nn.BCELoss(weight=None, reduction='mean')\n\n\n\n多类交叉熵损失函数（Categorical Cross Entropy Loss）\n多类交叉熵损失函数是多分类任务中常用的损失函数之一，它是交叉熵损失函数的扩展。多类交叉熵损失函数的计算公式如下：\n\\[\nL(y, \\hat{y}) = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{m} y_{ij} \\log(\\hat{y}_{ij})\n\\]\n其中，\\(y_{ij}\\)是真实标签，\\(\\hat{y}_{ij}\\)是模型预测的概率值。\n\ntorch.nn.CrossEntropyLoss(weight=None,ignore_index=-100, reduction='mean')\n\n\n\nHinge损失函数\nHinge损失函数是支持向量机（SVM）中常用的损失函数之一，它适用于二分类任务。Hinge损失函数的计算公式如下：\n\\[\nL(y, \\hat{y}) = \\max(0, 1 - y \\cdot \\hat{y})\n\\]\nHinge损失函数旨在最大化决策边界的间隔，即使得正确分类的样本距离决策边界的距离尽可能大。Hinge损失函数对于误分类的样本进行了惩罚，使得模型更加关注分类边界附近的样本，从而尽可能把数据点推向远离决策边界的方向。\n代码已经放进了星球里。\n\nDid you find this page helpful? Consider sharing it 🙌"
  }
]