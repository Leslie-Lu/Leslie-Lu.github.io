[
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "My research spans neurosyphilis, viral infections (COVID-19 and HPV), and cancers (HNSCC and cervical cancer)."
  },
  {
    "objectID": "publications/index.html#journal-articles",
    "href": "publications/index.html#journal-articles",
    "title": "Publications",
    "section": "Journal articles",
    "text": "Journal articles"
  },
  {
    "objectID": "publications/index.html#working-papers",
    "href": "publications/index.html#working-papers",
    "title": "Publications",
    "section": "Working papers",
    "text": "Working papers"
  },
  {
    "objectID": "publications/index.html#book-chapters",
    "href": "publications/index.html#book-chapters",
    "title": "Publications",
    "section": "Book chapters",
    "text": "Book chapters"
  },
  {
    "objectID": "publications/index.html#reviews",
    "href": "publications/index.html#reviews",
    "title": "Publications",
    "section": "Reviews",
    "text": "Reviews"
  },
  {
    "objectID": "publications/index.html#selected-seminar-papers",
    "href": "publications/index.html#selected-seminar-papers",
    "title": "Publications",
    "section": "Selected seminar papers",
    "text": "Selected seminar papers"
  },
  {
    "objectID": "publications/index.html#translations",
    "href": "publications/index.html#translations",
    "title": "Publications",
    "section": "Translations",
    "text": "Translations"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello there!",
    "section": "",
    "text": "I am a PhD candidate in epidemiology and biostatistics at Sun Yat-sen University. I have made contributions to neurosyphilis and viral infection research, and my work has been published in journals including eClinicalMedicine, JMIR Public Health and Surveillance, The Lancet Regional Health - Western Pacific, among others. Besides, I have patented a clinical diagnosis system for neurological syphilis (LU504466, rank 1/1).\nIn addition to my work on neurosyphilis and viral infection, I have extensive experience applying advanced statistical concepts, artificial intelligence, and causal inference methods to conduct real-world research based on electronic health records.\nTo date (May 22, 2025), I have published 33 papers in peer-reviewed journals, serving as the first author and co-first author for 13 of these."
  },
  {
    "objectID": "blog/index.html#section",
    "href": "blog/index.html#section",
    "title": "Blog",
    "section": "2025",
    "text": "2025\n\n\n    \n    \n                  \n            June 30, 2025\n        \n        \n            换行符不一致导致 comm 结果异常\n\n            \n            \n                \n                \n                    Linux\n                \n                \n                \n                    Command Line\n                \n                \n                \n                    Troubleshooting\n                \n                \n                \n                    Text Processing\n                \n                \n                \n                    File Formats\n                \n                \n                \n                    Cross-Platform Development\n                \n                \n            \n            \n\n            Issues with comm command due to inconsistent line endings in text files\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            June 24, 2025\n        \n        \n            离线跑 VEP 必备：Reference FASTA 文件下载与预处理\n\n            \n            \n                \n                \n                    VEP\n                \n                \n                \n                    reference genome\n                \n                \n                \n                    bioinformatics\n                \n                \n                \n                    variant annotation\n                \n                \n                \n                    genomics\n                \n                \n            \n            \n\n            Run VEP offline with local reference FASTA\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            June 18, 2025\n        \n        \n            2024年度期刊影响因子 JCR2024 公布\n\n            \n            \n                \n                \n                    impact-factor\n                \n                \n                \n                    JCR\n                \n                \n                \n                    journal\n                \n                \n            \n            \n\n            JCR2024 is released just now\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            June 17, 2025\n        \n        \n            色素痣就诊\n\n            \n            \n                \n                \n                    healthcare\n                \n                \n                \n                    dermatology\n                \n                \n            \n            \n\n            Visit for mole evaluation and management\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            June 9, 2025\n        \n        \n            在DNAnexus平台进行生信分析的省钱攻略\n\n            \n            \n                \n                \n                    UKB\n                \n                \n                \n                    UK Biobank\n                \n                \n                \n                    DNAnexus\n                \n                \n                \n                    Bioinformatics\n                \n                \n                \n                    Genomics\n                \n                \n                \n                    Cost-saving Tips\n                \n                \n            \n            \n\n            Cost-saving tips and strategies for bioinformatics analysis on the DNAnexus platform\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            May 23, 2025\n        \n        \n            序列基序（Motifs）解析\n\n            \n            \n                \n                \n                    bioinformatics\n                \n                \n                \n                    algorithms\n                \n                \n                \n                    motifs\n                \n                \n                \n                    biology\n                \n                \n                \n                    biostatistics\n                \n                \n            \n            \n\n            Motifs\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            May 7, 2025\n        \n        \n            PE0004: Largest Palindrome Product\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    project euler\n                \n                \n                \n                    algorithm\n                \n                \n                \n                    math\n                \n                \n                \n                    programming\n                \n                \n                \n                    palindrome\n                \n                \n                \n                    product\n                \n                \n            \n            \n\n            Project Euler Problem 4: Largest Palindrome Product\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            May 4, 2025\n        \n        \n            手写 Transformer\n\n            \n            \n                \n                \n                    Transformer\n                \n                \n                \n                    NLP\n                \n                \n                \n                    Deep Learning\n                \n                \n                \n                    Machine Learning\n                \n                \n                \n                    AI\n                \n                \n                \n                    Python\n                \n                \n                \n                    PyTorch\n                \n                \n            \n            \n\n            Annotated Transformer\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            April 22, 2025\n        \n        \n            脱发就诊\n\n            \n            \n                \n                \n                    daily life\n                \n                \n                \n                    health\n                \n                \n            \n            \n\n            Hair Loss Diagnosis\n            \n            \n        \n        \n    \n    \n    \n                  \n            April 17, 2025\n        \n        \n            Conventional Commits 规范详解\n\n            \n            \n                \n                \n                    git\n                \n                \n                \n                    git-commit\n                \n                \n                \n                    conventional-commits\n                \n                \n                \n                    git-flow\n                \n                \n            \n            \n\n            Conventional Commits\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            April 16, 2025\n        \n        \n            大模型翻译个人博客post\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    llm\n                \n                \n                \n                    translator\n                \n                \n                \n                    qmd\n                \n                \n                \n                    large-language-model\n                \n                \n            \n            \n\n            LLM Translator\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            April 9, 2025\n        \n        \n            PE0003: Largest Prime Factor\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    project euler\n                \n                \n                \n                    algorithm\n                \n                \n                \n                    math\n                \n                \n                \n                    programming\n                \n                \n                \n                    prime\n                \n                \n                \n                    factor\n                \n                \n            \n            \n\n            Project Euler Problem 2: Largest Prime Factor\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            April 4, 2025\n        \n        \n            如何科学选择DNAnexus平台的计算实例\n\n            \n            \n                \n                \n                    wgs\n                \n                \n                \n                    genomics\n                \n                \n                \n                    analysis\n                \n                \n                \n                    dnanexus\n                \n                \n                \n                    instance\n                \n                \n            \n            \n\n            DNAnexus instance\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            April 2, 2025\n        \n        \n            WGS: GATK vs GraphTyper vs DRAGEN\n\n            \n            \n                \n                \n                    wgs\n                \n                \n                \n                    genomics\n                \n                \n                \n                    analysis\n                \n                \n                \n                    gatk\n                \n                \n                \n                    graph-typer\n                \n                \n                \n                    dragen\n                \n                \n            \n            \n\n            DRAGEN WGS\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            April 1, 2025\n        \n        \n            PE0002: Even Fibonacci Numbers\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    project euler\n                \n                \n                \n                    algorithm\n                \n                \n                \n                    math\n                \n                \n                \n                    programming\n                \n                \n                \n                    fibonacci\n                \n                \n            \n            \n\n            Project Euler Problem 2: Even Fibonacci Numbers\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            March 31, 2025\n        \n        \n            PE0001: Multiples of 3 and 5\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    project euler\n                \n                \n                \n                    algorithm\n                \n                \n                \n                    math\n                \n                \n                \n                    programming\n                \n                \n            \n            \n\n            Project Euler Problem 1: Multiples of 3 and 5\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            March 25, 2025\n        \n        \n            NumPy 广播机制详解\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    numpy\n                \n                \n                \n                    broadcasting\n                \n                \n            \n            \n\n            broadcasting in numpy\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            March 22, 2025\n        \n        \n            来拿2025年中科院最新分区Excel表\n\n            \n            \n                \n                \n                    SCI\n                \n                \n            \n            \n\n            SCI journal partitions\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            March 11, 2025\n        \n        \n            全基因组测序GWAS vs. 传统GWAS\n\n            \n            \n                \n                \n                    GWAS\n                \n                \n                \n                    bioinformatics\n                \n                \n                \n                    genetics\n                \n                \n                \n                    WGS\n                \n                \n            \n            \n\n            WGS-GWAS vs. traditional GWAS\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            March 10, 2025\n        \n        \n            证明平衡设计与非平衡设计的统计学效率\n\n            \n            \n                \n                \n                    RCT\n                \n                \n                \n                    Statistics\n                \n                \n                \n                    Methodology\n                \n                \n                \n                    balanced design\n                \n                \n                \n                    power analysis\n                \n                \n                \n                    allocation\n                \n                \n            \n            \n\n            balanced design and unequal allocation\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            March 10, 2025\n        \n        \n            优化有效人数比例\n\n            \n            \n                \n                \n                    RCT\n                \n                \n                \n                    Statistics\n                \n                \n                \n                    Methodology\n                \n                \n                \n                    allocation\n                \n                \n                \n                    optimization\n                \n                \n            \n            \n\n            Optimizing the Number of Responders\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            March 8, 2025\n        \n        \n            证明随机化和盲法消除向均值回归偏倚\n\n            \n            \n                \n                \n                    RCT\n                \n                \n                \n                    Statistics\n                \n                \n                \n                    Methodology\n                \n                \n                \n                    randomization\n                \n                \n                \n                    blindness\n                \n                \n                \n                    regression to the mean\n                \n                \n            \n            \n\n            randomization and blindness\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            March 7, 2025\n        \n        \n            配对样本t检验的统计学效率\n\n            \n            \n                \n                \n                    RCT\n                \n                \n                \n                    Statistics\n                \n                \n                \n                    Hypothesis Testing\n                \n                \n                \n                    t-test\n                \n                \n                \n                    power analysis\n                \n                \n            \n            \n\n            power of paired sample t test\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            March 3, 2025\n        \n        \n            临床试验中基线数据p值真的有意义吗？\n\n            \n            \n                \n                \n                    RCT\n                \n                \n                \n                    Table 1\n                \n                \n                \n                    Baseline\n                \n                \n                \n                    p value\n                \n                \n            \n            \n\n            Table 1 in RCT\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 17, 2025\n        \n        \n            Zotero + OneDrive + DeepSeek：构建个人文献阅读管理系统\n\n            \n            \n                \n                \n                    zotero\n                \n                \n                \n                    onedrive\n                \n                \n                \n                    deepseek\n                \n                \n                \n                    tools\n                \n                \n            \n            \n\n            Zotero + OneDrive + DeepSeek\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 13, 2025\n        \n        \n            SAS 生存分析模拟\n\n            \n            \n                \n                \n                    sas\n                \n                \n                \n                    cox\n                \n                \n                \n                    ph\n                \n                \n                \n                    survival\n                \n                \n                \n                    simulation\n                \n                \n            \n            \n\n            proportional hazards model\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 11, 2025\n        \n        \n            一文读懂 Dosage 文件\n\n            \n            \n                \n                \n                    genetics\n                \n                \n                \n                    bioinformatics\n                \n                \n                \n                    dosage\n                \n                \n                \n                    saige\n                \n                \n            \n            \n\n            dosage file used in genetics\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 28, 2025\n        \n        \n            祝大家新年快乐！\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tools\n                \n                \n                \n                    red packet\n                \n                \n                \n                    happy new year\n                \n                \n            \n            \n\n            happy lunar new year of the snake\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 21, 2025\n        \n        \n            公众号智能回复功能终于上线\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tools\n                \n                \n                \n                    red packet\n                \n                \n            \n            \n\n            tencent finnaly released the intelligent reply function\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 20, 2025\n        \n        \n            新年红包封面来了\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tools\n                \n                \n                \n                    red packet\n                \n                \n            \n            \n\n            making the WeChat red packet cover with r\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 19, 2025\n        \n        \n            R 中项目环境管理\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tools\n                \n                \n                \n                    version control\n                \n                \n            \n            \n\n            using renv to manage R packages\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 7, 2025\n        \n        \n            Tracy-Widom Statistics\n\n            \n            \n                \n                \n                    bioinformatics\n                \n                \n                \n                    biostatistics\n                \n                \n                \n                    pca\n                \n                \n            \n            \n\n            twstats\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 6, 2025\n        \n        \n            10 个常用终端命令\n\n            \n            \n                \n                \n                    bioinformatics\n                \n                \n                \n                    tools\n                \n                \n                \n                    programming\n                \n                \n                \n                    linux\n                \n                \n                \n                    terminal\n                \n                \n            \n            \n\n            每个开发者都应该知道的终端命令\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 2, 2025\n        \n        \n            探索 AWK\n\n            \n            \n                \n                \n                    bioinformatics\n                \n                \n                \n                    tools\n                \n                \n                \n                    programming\n                \n                \n                \n                    awk\n                \n                \n                \n                    linux\n                \n                \n            \n            \n\n            强大的文本处理工具\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 1, 2025\n        \n        \n            使用 Hail 输出 PLINK 文件：一步到位\n\n            \n            \n                \n                \n                    hail\n                \n                \n                \n                    bioinformatics\n                \n                \n                \n                    dnanexus\n                \n                \n                \n                    genomics\n                \n                \n                \n                    plink\n                \n                \n                \n                    vcf\n                \n                \n            \n            \n\n            轻松处理基因数据\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-1",
    "href": "blog/index.html#section-1",
    "title": "Blog",
    "section": "2024",
    "text": "2024\n\n\n    \n    \n                  \n            December 28, 2024\n        \n        \n            掌握 Hail\n\n            \n            \n                \n                \n                    hail\n                \n                \n                \n                    bioinformatics\n                \n                \n                \n                    dnanexus\n                \n                \n                \n                    genomics\n                \n                \n            \n            \n\n            在 DNAnexus 平台上高效处理基因组数据\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            December 26, 2024\n        \n        \n            使用 Eigensoft 中的 smartPCA 进行 PCA 分析\n\n            \n            \n                \n                \n                    plink\n                \n                \n                \n                    gwas\n                \n                \n                \n                    bioinformatics\n                \n                \n                \n                    pca\n                \n                \n                \n                    eigensoft\n                \n                \n                \n                    smartpca\n                \n                \n            \n            \n\n            Eigensoft\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            December 25, 2024\n        \n        \n            GWAS 前 PCA 步骤详解\n\n            \n            \n                \n                \n                    plink\n                \n                \n                \n                    gwas\n                \n                \n                \n                    bioinformatics\n                \n                \n                \n                    pca\n                \n                \n            \n            \n\n            a typical genomic PCA analysis\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            December 24, 2024\n        \n        \n            预 GWAS 阶段的基因型数据 QC 流程\n\n            \n            \n                \n                \n                    plink\n                \n                \n                \n                    gwas\n                \n                \n                \n                    bioinformatics\n                \n                \n                \n                    genotype data\n                \n                \n                \n                    qc\n                \n                \n            \n            \n\n            genotype data QC\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            December 23, 2024\n        \n        \n            PLINK 常用命令介绍\n\n            \n            \n                \n                \n                    plink\n                \n                \n                \n                    gwas\n                \n                \n                \n                    bioinformatics\n                \n                \n            \n            \n\n            plink\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            December 21, 2024\n        \n        \n            理解 PLINK格式\n\n            \n            \n                \n                \n                    plink\n                \n                \n                \n                    gwas\n                \n                \n                \n                    bioinformatics\n                \n                \n            \n            \n\n            基因型数据存储的基础\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            December 20, 2024\n        \n        \n            从基因到疾病：等位基因频率与效应大小的关系\n\n            \n            \n                \n                \n                    maf\n                \n                \n                \n                    gwas\n                \n                \n                \n                    mendelian disease\n                \n                \n            \n            \n\n            MAF\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            December 19, 2024\n        \n        \n            深入了解 VCF/VCF.GZ/VCF.GZ.TBI 文件\n\n            \n            \n                \n                \n                    vcf\n                \n                \n                \n                    bioinformatics\n                \n                \n                \n                    genetics\n                \n                \n            \n            \n\n            Variant Call Format\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            December 16, 2024\n        \n        \n            感知机：人工神经网络的起点\n\n            \n            \n                \n                \n                    perceptron\n                \n                \n                \n                    artificial neural network\n                \n                \n                \n                    machine learning\n                \n                \n                \n                    deep learning\n                \n                \n            \n            \n\n            It all started with a Perceptron\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            December 11, 2024\n        \n        \n            DNAnexus 表型数据提取\n\n            \n            \n                \n                \n                    dnanexus\n                \n                \n                \n                    phenotype\n                \n                \n                \n                    table exporter\n                \n                \n            \n            \n\n            Table exporter\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            December 4, 2024\n        \n        \n            Ubuntu 20.04 上安装 R\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    ubuntu\n                \n                \n                \n                    linux\n                \n                \n            \n            \n\n            详细指南\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            December 3, 2024\n        \n        \n            Swiss Army Knife 工具运行 R 的 log 记录\n\n            \n            \n                \n                \n                    dnanexus\n                \n                \n                \n                    r\n                \n                \n                \n                    log\n                \n                \n                \n                    swiss army knife\n                \n                \n            \n            \n\n            Swiss Army Knife\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            December 2, 2024\n        \n        \n            使用 SAIGE 进行 GWAS 分析的详细流程\n\n            \n            \n                \n                \n                    saige\n                \n                \n                \n                    gwas\n                \n                \n                \n                    genomics\n                \n                \n            \n            \n\n            一种用于大规模基因组关联研究的高效工具\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            November 29, 2024\n        \n        \n            星球 JC | 解读 TRIPOD-AI\n\n            \n            \n                \n                \n                    journal club\n                \n                \n                \n                    tripod\n                \n                \n                \n                    ai\n                \n                \n            \n            \n\n            TRIPOD-AI：updated guidance for reporting clinical prediction models that use regression or machine learning methods\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            November 28, 2024\n        \n        \n            SAIGE\n\n            \n            \n                \n                \n                    saige\n                \n                \n                \n                    gwas\n                \n                \n                \n                    genomics\n                \n                \n            \n            \n\n            一种用于大规模基因组关联研究的高效工具\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            November 26, 2024\n        \n        \n            连锁不平衡\n\n            \n            \n                \n                \n                    linkage disequilibrium\n                \n                \n                \n                    genetics\n                \n                \n            \n            \n\n            Linkage Disequilibrium, LD\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            November 22, 2024\n        \n        \n            累积风险图（Cumulative Hazard Plot）中的 Y 轴含义\n\n            \n            \n                \n                \n                    surival analysis\n                \n                \n                \n                    cumulative hazard plot\n                \n                \n                \n                    hazard function\n                \n                \n            \n            \n\n            cumulative hazard plot\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            October 2, 2024\n        \n        \n            R 中面向对象编程系统选择\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    object oriented programming\n                \n                \n            \n            \n\n            object-oriented-programming\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            September 25, 2024\n        \n        \n            星球JC | 高尿酸血症及相关代谢疾病风险模型\n\n            \n            \n                \n                \n                    journal club\n                \n                \n            \n            \n\n            预测模型星球Journal Club\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            September 16, 2024\n        \n        \n            解析 Beta 分布与贝叶斯\n\n            \n            \n                \n                \n                    beta distribution\n                \n                \n                \n                    bayesian\n                \n                \n                \n                    probability\n                \n                \n                \n                    biostatistics\n                \n                \n            \n            \n\n            beta 分布\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            September 11, 2024\n        \n        \n            Software 2.0\n\n            \n            \n                \n                \n                    deep learning\n                \n                \n                \n                    software\n                \n                \n            \n            \n\n            人工智能时代的新编程范式\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            September 9, 2024\n        \n        \n            爱因斯坦求和\n\n            \n            \n                \n                \n                    linear algebra\n                \n                \n                \n                    deep learning\n                \n                \n                \n                    tensor\n                \n                \n            \n            \n\n            Einstein Summation\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            September 8, 2024\n        \n        \n            2023年中科院期刊分区表升级版\n\n            \n            \n                \n                \n                    journal category\n                \n                \n            \n            \n\n            爬分区表\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            September 6, 2024\n        \n        \n            星球JC | 心脏功能障碍诊断模型\n\n            \n            \n                \n                \n                    journal club\n                \n                \n            \n            \n\n            预测模型星球Journal Club\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            September 4, 2024\n        \n        \n            从随意推断（casual inference）到因果推断（causal inference）\n\n            \n            \n                \n                \n                    causal inference\n                \n                \n            \n            \n\n            因果推断\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            August 30, 2024\n        \n        \n            星球JC | 胃癌早期筛查工具\n\n            \n            \n                \n                \n                    journal club\n                \n                \n            \n            \n\n            预测模型星球Journal Club\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            August 29, 2024\n        \n        \n            github actions使用docker渲染quarto文档\n\n            \n            \n                \n                \n                    github\n                \n                \n                \n                    quarto\n                \n                \n                \n                    docker\n                \n                \n            \n            \n\n            Dockerfiles\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            August 20, 2024\n        \n        \n            大图嵌小图\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    ggplot2\n                \n                \n            \n            \n\n            调包\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            August 14, 2024\n        \n        \n            欢迎加入预测模型星球\n\n            \n            \n                \n                \n                    workshop\n                \n                \n                \n                    prediction model\n                \n                \n            \n            \n\n            Clinical Prediction Model\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            August 11, 2024\n        \n        \n            星球第二期workshop上线\n\n            \n            \n                \n                \n                    workshop\n                \n                \n                \n                    sample size\n                \n                \n                \n                    clinical research\n                \n                \n            \n            \n\n            Sample Size Calculations in Clinical Research\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            August 4, 2024\n        \n        \n            星球第一期workshop上线\n\n            \n            \n                \n                \n                    workshop\n                \n                \n                \n                    missing data\n                \n                \n                \n                    statistical methods\n                \n                \n            \n            \n\n            Statistical Methods for Analysis with Missing Data\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            August 3, 2024\n        \n        \n            倾向性评分加权\n\n            \n            \n                \n                \n                    propensity score\n                \n                \n                \n                    weighting\n                \n                \n            \n            \n\n            倾向性评分加权的具体介绍\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            July 19, 2024\n        \n        \n            预测模型领域新书推荐\n\n            \n            \n                \n                \n                    prediction model\n                \n                \n                \n                    book\n                \n                \n            \n            \n\n            临床预测模型方法与应用\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            June 23, 2024\n        \n        \n            2023年最新JCR影响因子发布\n\n            \n            \n                \n                \n                    sci\n                \n                \n                \n                    jcr\n                \n                \n            \n            \n\n            2023年最新JCR影响因子\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            June 7, 2024\n        \n        \n            常用损失函数\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    machine learning\n                \n                \n                \n                    deep learning\n                \n                \n                \n                    loss function\n                \n                \n            \n            \n\n            常用损失函数介绍\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            May 17, 2024\n        \n        \n            Python中机器学习模型的校准\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    machine learning\n                \n                \n                \n                    calibration\n                \n                \n            \n            \n\n            预测模型如何校准\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            May 13, 2024\n        \n        \n            Hierarchical composite endpoints治疗效应的可视化\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    clinical trial\n                \n                \n                \n                    endpoint\n                \n                \n            \n            \n\n            复合终点治疗效应的可视化\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            May 11, 2024\n        \n        \n            最优分类阈值\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    machine learning\n                \n                \n            \n            \n\n            分类问题中阈值的选择\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-2",
    "href": "blog/index.html#section-2",
    "title": "Blog",
    "section": "2023",
    "text": "2023"
  },
  {
    "objectID": "blog/index.html#section-3",
    "href": "blog/index.html#section-3",
    "title": "Blog",
    "section": "2022",
    "text": "2022"
  },
  {
    "objectID": "blog/index.html#section-4",
    "href": "blog/index.html#section-4",
    "title": "Blog",
    "section": "2021",
    "text": "2021"
  },
  {
    "objectID": "blog/index.html#section-5",
    "href": "blog/index.html#section-5",
    "title": "Blog",
    "section": "2020",
    "text": "2020\n\n\n    \n    \n                  \n            April 30, 2020\n        \n        \n            一文搞懂如何选择卡方检验\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    chi-square test\n                \n                \n            \n            \n\n            chi-square test\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            April 29, 2020\n        \n        \n            矩阵乘向量的新视角：空间映射\n\n            \n            \n                \n                \n                    linear algebra\n                \n                \n                \n                    matrix\n                \n                \n            \n            \n\n            linear algebra\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            April 28, 2020\n        \n        \n            如何理解残差与误差\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    residual error\n                \n                \n            \n            \n\n            residual error\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            April 27, 2020\n        \n        \n            矩阵的运算\n\n            \n            \n                \n                \n                    linear algebra\n                \n                \n                \n                    matrix\n                \n                \n                \n                    python\n                \n                \n            \n            \n\n            linear algebra\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            April 26, 2020\n        \n        \n            Python与矩阵\n\n            \n            \n                \n                \n                    linear algebra\n                \n                \n                \n                    matrix\n                \n                \n                \n                    python\n                \n                \n            \n            \n\n            linear algebra\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            April 25, 2020\n        \n        \n            R语言4.0发布上线\n\n            \n            \n                \n                \n                    r\n                \n                \n            \n            \n\n            R version update\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            March 22, 2020\n        \n        \n            概率密度与累积分布\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    probability\n                \n                \n            \n            \n\n            probability\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            March 12, 2020\n        \n        \n            空间中的向量\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    linear algebra\n                \n                \n            \n            \n\n            linear algebra\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            March 11, 2020\n        \n        \n            Python与向量\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    linear algebra\n                \n                \n                \n                    python\n                \n                \n            \n            \n\n            linear algebra\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            March 10, 2020\n        \n        \n            线性代数之向量\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    linear algebra\n                \n                \n            \n            \n\n            linear algebra\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 23, 2020\n        \n        \n            协方差分析\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    ancova\n                \n                \n            \n            \n\n            ANCOVA\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 22, 2020\n        \n        \n            试验设计与方差分析（4）\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    anova\n                \n                \n            \n            \n\n            ANOVA\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 20, 2020\n        \n        \n            试验设计与方差分析（3）\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    anova\n                \n                \n            \n            \n\n            ANOVA\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 18, 2020\n        \n        \n            试验设计与方差分析（2）\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    anova\n                \n                \n            \n            \n\n            ANOVA\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 17, 2020\n        \n        \n            试验设计与方差分析（1）\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    anova\n                \n                \n            \n            \n\n            ANOVA\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 15, 2020\n        \n        \n            方差分析\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    anova\n                \n                \n            \n            \n\n            ANOVA\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 6, 2020\n        \n        \n            R语言数据处理分析实例\n\n            \n            \n                \n                \n                    r\n                \n                \n            \n            \n\n            R Training\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 5, 2020\n        \n        \n            计算机概论7--操作系统\n\n            \n            \n                \n                \n                    linux\n                \n                \n            \n            \n\n            linux\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 4, 2020\n        \n        \n            样本量和检验效能的估计问题\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    sample size\n                \n                \n                \n                    power\n                \n                \n            \n            \n\n            sample size and power estimation\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 2, 2020\n        \n        \n            计算机概论6\n\n            \n            \n                \n                \n                    linux\n                \n                \n            \n            \n\n            linux\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 1, 2020\n        \n        \n            R语言基础--数据的输入\n\n            \n            \n                \n                \n                    r\n                \n                \n            \n            \n\n            R Training\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 31, 2020\n        \n        \n            计算机概论5\n\n            \n            \n                \n                \n                    linux\n                \n                \n            \n            \n\n            linux\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 30, 2020\n        \n        \n            R语言基础--函数\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    programming\n                \n                \n            \n            \n\n            R Training\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 29, 2020\n        \n        \n            R语言基础--运算符\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    programming\n                \n                \n            \n            \n\n            R Training\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 28, 2020\n        \n        \n            R语言基础--因子\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    programming\n                \n                \n            \n            \n\n            R Training\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 27, 2020\n        \n        \n            计算机概论4\n\n            \n            \n                \n                \n                    linux\n                \n                \n            \n            \n\n            linux\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 22, 2020\n        \n        \n            R语言编程入门\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    programming\n                \n                \n            \n            \n\n            R Training\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 21, 2020\n        \n        \n            上手vim编辑器\n\n            \n            \n                \n                \n                    vim\n                \n                \n                \n                    linux\n                \n                \n            \n            \n\n            vim\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 20, 2020\n        \n        \n            计算机概论3\n\n            \n            \n                \n                \n                    linux\n                \n                \n            \n            \n\n            linux\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 19, 2020\n        \n        \n            谈谈电脑的CPU\n\n            \n            \n                \n                \n                    linux\n                \n                \n            \n            \n\n            linux\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 18, 2020\n        \n        \n            Python正则表达式与模式匹配\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    regular expression\n                \n                \n                \n                    pattern matching\n                \n                \n            \n            \n\n            Python Training\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 17, 2020\n        \n        \n            你真的了解自己的电脑吗？\n\n            \n            \n                \n                \n                    linux\n                \n                \n            \n            \n\n            linux\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 16, 2020\n        \n        \n            R语言的初体验\n\n            \n            \n                \n                \n                    r\n                \n                \n            \n            \n\n            R Training\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 11, 2020\n        \n        \n            kNN改进约会网站的配对效果\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    knn\n                \n                \n            \n            \n\n            kNN\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 10, 2020\n        \n        \n            Python基础要素之字符串\n\n            \n            \n                \n                \n                    python\n                \n                \n            \n            \n\n            Python Training\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 9, 2020\n        \n        \n            笔记\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    causal inference\n                \n                \n            \n            \n\n            causal inference\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 8, 2020\n        \n        \n            如何用Python自编k-近邻算法？\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    knn\n                \n                \n            \n            \n\n            kNN\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 7, 2020\n        \n        \n            NumPy函数库基础\n\n            \n            \n                \n                \n                    python\n                \n                \n                \n                    numpy\n                \n                \n            \n            \n\n            Python Training\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 6, 2020\n        \n        \n            Python基础要素之数值\n\n            \n            \n                \n                \n                    python\n                \n                \n            \n            \n\n            Python Training\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 5, 2020\n        \n        \n            可惜没如果——因果关系的三个层级\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    causal inference\n                \n                \n            \n            \n\n            因果推断的基本知识\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 4, 2020\n        \n        \n            Python初体验\n\n            \n            \n                \n                \n                    pycharm\n                \n                \n                \n                    python\n                \n                \n            \n            \n\n            Python Training\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 2, 2020\n        \n        \n            因果推断-2\n\n            \n            \n                \n                \n                    biostatistics\n                \n                \n                \n                    causal inference\n                \n                \n            \n            \n\n            关于因果推断的基本概念\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 1, 2020\n        \n        \n            鼠年加油\n\n            \n            \n                \n                \n                    happy new year\n                \n                \n            \n            \n\n            新年快乐\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-6",
    "href": "blog/index.html#section-6",
    "title": "Blog",
    "section": "2019",
    "text": "2019\n\n\n    \n    \n                  \n            December 11, 2019\n        \n        \n            因果推断\n\n            \n            \n                \n                \n                    statistics\n                \n                \n                \n                    biostatistics\n                \n                \n                \n                    causal inference\n                \n                \n            \n            \n\n            关于因果推断的基本概念\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            November 28, 2019\n        \n        \n            统计学是干嘛的？\n\n            \n            \n                \n                \n                    statistics\n                \n                \n                \n                    biostatistics\n                \n                \n            \n            \n\n            聊一聊什么是统计学\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "blog/2025/06/24/vep/index.html",
    "href": "blog/2025/06/24/vep/index.html",
    "title": "离线跑 VEP 必备：Reference FASTA 文件下载与预处理",
    "section": "",
    "text": "这里，我们将详细介绍如何为 Ensembl Variant Effect Predictor (VEP) 离线模式准备本地参考基因组 FASTA，以及为什么要 先解压、再 bgzip 压缩。看完就能举一反三，为任何物种配置 VEP 参考序列。\n\n1. 为什么一定要用 Ensembl 官方 FASTA？\n\n\n\n\n\n\n\n\n原因\n\n\n\n\n坐标一致\nVEP 只能 100% 保证与同版本 Ensembl 发布的注释（GTF/GFF、缓存）完全匹配。用其他来源 → 坐标或 HGVS 解析可能错位\n\n\n目录统一\nEnsembl FTP 采用统一的 …/release-114/fasta//dna/ 结构，自动化脚本好写\n\n\n长期维护\n每个 release 都保留完整历史，且提供 MD5 校验，便于版本溯源\n\n\n\n简言之：想省心，就用 Ensembl FASTA ，且版本号要与当前 VEP 保持一致。\n\n\n2. 选哪个文件最合适？\n进入 …/dna/ 目录后，你会看到一堆后缀类似的压缩包：\n\n\n\n\n\n\n\n\n文件名片段\n含义\n是否推荐做 VEP 参考？\n\n\n\n\ndna.primary_assembly.fa.gz\n仅含正式染色体 (24 条常染 + XY + M) 及少量 decoy\n✅ 推荐\n\n\ndna.toplevel.fa.gz\nprimary_assembly + alt contigs + patch\n体积大，注释慢\n\n\n_sm. / _rm.\nsoft-mask / repeat-mask\n⛔ 不要（大小写或 N 会影响插件）\n\n\n\n拿 unmasked + primary_assembly 即可。以人类 GRCh38 为例：Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz。\n\n\n3. 下载 ➜ 解压 ➜ bgzip 三步走\n#| eval: false\n# 下载记得确认 release 号\ncurl -O https://ftp.ensembl.org/pub/release-114/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz\n# 先完全解压；bgzip 不能直接二次压缩 .gz\ngzip -d Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz\n# 用 bgzip 重新块压缩，生成可随机索引的 .fa.gz\nbgzip -@ 8 Homo_sapiens.GRCh38.dna.primary_assembly.fa   # -@ 8 = 8 线程\n# （可选）提前生成索引，VEP 第一次跑时也会自动创建\nsamtools faidx Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz\n\n为什么要这么折腾？\n\n\n\n\n\n\n\n\n\n特点\n适用场景\n\n\n\n\n纯文本 .fa\n读取最快；占用磁盘大\n本地磁盘富余\n\n\n普通 gzip .fa.gz\n空间省，但无法随机访问 ➜ VEP 巨慢\n❌ 不要\n\n\nbgzip .fa.gz\n空间省，又可随机定位 (Bio::DB::HTS::Faidx)\n✅ 推荐\n\n\n\nbgzip 和 samtools 均来自 htslib，conda 安装：conda install -c bioconda htslib samtools。\n其他物种同理，路径里换成对应英文学名即可。\n\n\n\n4. 常见报错与解决方案\n\n\n\n\n\n\n\n\n报错\n原因\n解决\n\n\n\n\nCan't locate Bio/DB/HTS/Faidx.pm\nperl-htslib 缺失\nconda install -c bioconda perl-htslib\n\n\nCould not find index file\n.fai / .gzi 损坏或丢失\n手动 samtools faidx *.fa.gz 重新索引\n\n\n运行速度奇慢 + 单核\n用了普通 gzip 文件\n按上文改用 bgzip\n\n\n内存占用飙升\n开太多 --fork 线程\n减少 --fork 或增加 --buffer_size\n\n\n\n\n\nTake Home Message\n下载 unmasked primary_assembly → 解压 → bgzip，然后把 .fa.gz 路径传给 –fasta，VEP 离线跑得又快又稳，磁盘占用还能少一半以上。"
  },
  {
    "objectID": "blog/2025/06/17/mole/index.html",
    "href": "blog/2025/06/17/mole/index.html",
    "title": "色素痣就诊",
    "section": "",
    "text": "2025 年 6 月 15 日，去医院美容皮肤科就诊。\n\n\n\n皮肤镜检查报告单\n\n\n目前无治疗，等待以后复查观察。"
  },
  {
    "objectID": "blog/2025/05/23/motif/index.html",
    "href": "blog/2025/05/23/motif/index.html",
    "title": "序列基序（Motifs）解析",
    "section": "",
    "text": "在基因组学和生物信息学研究中，序列基序（Motifs）的识别是解码基因调控机制的核心任务之一。这些短而保守的 DNA 或蛋白质序列片段，如同生命密码中的关键字符，在转录调控、蛋白相互作用和进化分析中扮演着重要角色。\n\n什么是序列基序？\n人类基因组包含约 30 亿个碱基对，仅靠人工分析无法应对。高通量测序技术（如 Illumina 测序）每天可生成数 TB 的数据，生物信息学通过算法和模型提取关键信息。\n序列基序则是一段在进化或功能选择压力下高度保守的核苷酸或氨基酸序列模式，长度通常在 5-20 个碱基（DNA）或 3-10 个残基（蛋白质）之间。该短片吨在生物序列中频繁出现，通常与特定的生物学功能相关联。它们可能是DNA中的调控序列，也可能是蛋白质中的功能域。例如，DNA上的转录因子结合位点（如 TATA 盒）、RNA 剪接信号或蛋白质结构域中的特征性模式均可视为 Motifs。\nDNA motif 多为转录因子结合位点（TFBS），与基因表达调控直接相关。如，TATA盒，一个经典 motif，序列为 TATAAA，位于真核基因启动子区域，约 80% 的真核基因依赖其启动转录。。而 TFBS 的突变可能导致疾病。例如，癌症中常见的 P53 基因突变常影响其结合 motif，扰乱基因表达。蛋白质 motif 通常与结构或功能相关。如，锌指结构（Zinc Finger）。\n\n\nMotifs 识别的算法挑战\n识别 Motifs 的本质是从大量序列中寻找统计显著的保守模式，其复杂性源于以下挑战：\n\n模糊性：Motifs 可能存在碱基变异（如单核苷酸多态性），需容忍一定错配。\n背景噪声：基因组中随机出现的相似序列可能干扰检测。\n计算复杂度：穷举所有可能的 Motifs 组合在计算上不可行（NP 难问题）。\n\n\n\n常用算法\n\n1. 枚举\n早期的 Motif 识别算法主要基于枚举方法，如 WINNOWER 和 MITRA。这些算法通过穷举所有可能的 k-mer（长度为 k 的子序列）进行比对，虽然能保证全局最优解，但仅适用于短 Motifs（k&lt;15）。随着基因组数据量的增加，这种方法逐渐被更高效的算法所取代。\n\n\n2. 概率优化\n概率优化算法是当前 Motif 识别的主流方法。它们基于统计模型，通过迭代优化 Motif 的位置权重矩阵（PWM）来寻找最优解。常用的概率优化算法包括：\n\n期望最大化（EM）算法：以隐马尔可夫模型为基础，迭代优化 PWM 参数。该方法在理论上收敛稳定，但易陷入局部最优解。\nGibbs 抽样：通过马尔可夫链蒙特卡洛（MCMC）随机采样，逐步逼近最优解。该方法灵活，适用于长 Motifs 或大数据集，但结果依赖初始条件。\n基于进化算法的创新：如遗传算法（GA）模拟自然选择过程，通过交叉、突变操作优化候选 Motifs，平衡全局搜索与计算效率。\n基于图的算法：如基于图的 Motif 识别方法，通过构建序列图谱，利用图论算法寻找 Motifs。这些方法在处理大规模数据时表现出色。\n深度学习方法：近年来，深度学习在 Motif 识别中展现出强大的潜力。通过卷积神经网络（CNN）等模型，自动提取序列特征，显著提高了识别精度。"
  },
  {
    "objectID": "blog/2025/05/04/transformer/index.html",
    "href": "blog/2025/05/04/transformer/index.html",
    "title": "手写 Transformer",
    "section": "",
    "text": "引言\nTransfomer 是一种用于自然语言处理（NLP）任务的深度学习模型架构。它在2017年由 Vaswani 等人提出，并在论文《Attention is All You Need》中首次介绍。Transformer 模型的核心思想是使用自注意力机制来捕捉输入序列中不同位置之间的依赖关系，而不依赖于传统的循环神经网络（RNN）或卷积神经网络（CNN）。\n\n\n\nAttention is All You Need\n\n\n这里，我们将手动搭建并实现该论文中的 Transformer 模型。我们将使用 PyTorch 框架来实现该模型，并逐步解释每个组件的功能和实现细节。\n\n本文代码实现参考哈佛大学 The Annotated Transformer，特别致谢 Ashish Vaswani 等原论文作者。\n\n\n\n模型架构全景\n\n\n\nTransfomer\n\n\n\n\n核心模块\n\n1. 多头注意力机制（Multi-Head Attention）\n\n\n\nMulti-Head Attention\n\n\n注意力公式为：\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\\]\n代码实现：\n\nclass MultiHeaderAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        \"Take in model size and number of heads.\"\n        super().__init__()\n        assert d_model % h == 0\n        # Assume d_v always equals d_k\n        self.d_k= d_model // h\n        self.h= h\n        self.linears= clones(nn.Linear(d_model, d_model), 4)\n        self.attn= None\n        self.dropout= nn.Dropout(p=dropout)\n\n    def forward(self, query, key, value, mask=None):\n        if mask is not None:\n            # Same mask applied to all h heads.\n            mask= mask.unsqueeze(1)\n        nbatches= query.size(0)\n\n        # 1) Do all the linear projections in batch from d_model =&gt; h x d_k\n        query, key, value= [\n            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1,2)\n            for lin, x in zip(self.linears, (query, key, value))\n        ]\n\n        # 2) Apply attention on all the prokjected vectors in batch.\n        x, self.attn= attention(\n            query, key, value, mask= mask, dropout= self.dropout\n        )\n\n        # 3) Concat using a view and apply a final linear.\n        x= (\n            x.transpose(1,2)\n            .contiguous()\n            .view(nbatches, -1, self.h * self.d_k)\n        )\n        del query\n        del key\n        del value\n        return self.linears[-1](x)\n\n其中，h=8 表示头数，为原文默认设置，d_model=512 表示模型的维度。\n\n\n\nAnnotation\n\n\n\n\n2. 位置编码（Positional Encoding）\n数学公式：\n\\[\n\\text{PE}_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n\\]\n\\[\n\\text{PE}_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n\\]\n代码实现：\n\nclass PositionalEncoding(nn.Module):\n    \"Implement the PE function.\"\n\n    def __init__(self, d_model, dropout, max_len=5000):\n        super().__init__()\n        self.dropout= nn.Dropout(p=dropout)\n\n        # Compute the positional endcodings once in log sapce.\n        pe= torch.zeros(max_len, d_model)\n        position= torch.arange(0, max_len).unsqueeze(1)\n        div_term= torch.exp(\n            torch.arange(0, d_model, 2) * -(math.log(1000.0)/d_model)\n        )\n        pe[:,0::2]= torch.sin(position * div_term)\n        pe[:,1::2]= torch.cos(position * div_term)\n        pe= pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x= x+ self.pe[:,:x.size(1)].requires_grad_(False)\n        return self.dropout(x)\n\n\n\n3. 编码器层（Encoder Layer）\n架构组成：\n\n自注意力子层\n前馈神经网络子层\n残差连接和层归一化\n\n\n\n\nEncoder block\n\n\n代码实现：\n\nclass EncoderLayer(nn.Module):\n    \"Encoder is made up of self-attn and feed forward.\"\n\n    def __init__(self, size, self_attn, feed_forward, dropout):\n        super().__init__()\n        self.self_attn= self_attn\n        self.feed_forward= feed_forward\n        self.sublayer= clones(SublayerConnection(size, dropout), 2)\n        self.size= size\n\n    def forward(self, x, mask):\n        x= self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n        return self.sublayer[1](x, self.feed_forward)\n\n\n\n4. 解码器层（Decoder Layer）\n架构组成：\n\n带掩码的自注意力子层\n编码器-解码器注意力子层\n前馈神经网络子层\n\n\n\n\nDecoder block\n\n\n代码实现：\n\nclass DecoderLayer(nn.Module):\n    \"Decoder is made of self-attn, src-attn, and feed forward\"\n\n    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n        super().__init__()\n        self.size= size\n        self.self_attn= self_attn\n        self.src_attn= src_attn\n        self.feed_forward= feed_forward\n        self.sublayer= clones(SublayerConnection(size, dropout), 3)\n\n    def forward(self, x, memory, src_mask, tgt_mask):\n        m= memory\n        x= self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n        x= self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n        return self.sublayer[2](x, self.feed_forward)\n\n其中，掩码生成函数如下：\n\ndef subsequent_mask(size):\n    \"Mask out subsequent positions.\"\n    attn_shape= (1, size, size)\n    subsequent_mask= torch.triu(torch.ones(attn_shape, dtype=torch.bool), diagonal= 1)\n    return ~subsequent_mask\n\n借助于 subsequent_mask 函数，我们可以生成一个上三角矩阵，用于掩码掉后续位置的注意力权重。\n\n\n5. 模型初始化与参数设置\n\ndef make_model(\n    src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1\n):\n    \"Helper: Construct a model from hyperparameters.\"\n    c= copy.deepcopy\n    attn= MultiHeaderAttention(h, d_model)\n    ff= PositionWiseFeedForward(d_model, d_ff, dropout)\n    position= PositionalEncoding(d_model, dropout)\n    model= EncoderDecoder(\n        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n        Generator(d_model, tgt_vocab)\n    )\n\n    # Initiailize parameters with Glorot / fan_avg.\n    for p in model.parameters():\n        if p.dim() &gt; 1:\n            nn.init.xavier_uniform_(p)\n    \n    return model\n\n\n\n\n训练过程\n\n1. 动态学习率调整\n公式：\n\\[\n\\text{lr} = \\text{d_model}^{-0.5} \\cdot \\text{min}(\\text{step}^{-0.5}, \\text{step} \\cdot \\text{warmup}^{-1.5})\n\\]\n代码实现：\n\ndef rate(step, model_size, factor, warmup):\n    \"\"\"\n    We have to default the step to 1 for LambdaLR function to avoid zero rasing to negative power.\n    \"\"\"\n    if step == 0:\n        step = 1\n    return factor * (model_size ** -0.5) * min(step ** -0.5, step * warmup ** -1.5)\n\n学习率变化曲线：\n\n\n\nlr\n\n\n前4000步线性增长，之后指数衰减。\n\n\n2. 标签平滑（Label Smoothing）\n标签平滑是一种正则化技术，用于缓解模型过拟合和提高泛化能力。它通过将目标标签的概率分布进行平滑处理，使得模型在训练时不会过于自信地预测某个特定的标签。\n代码实现：\n\nclass LabelSmoothing(nn.Module):\n    \"\"\"\n    Implement label smoothing.\n    \"\"\"\n\n    def __init__(self, size, padding_idx, smoothing=0.0):\n        super().__init__()\n        self.criterion= nn.KLDivLoss(reduction='sum')\n        self.padding_idx= padding_idx\n        self.confidence= 1.0 - smoothing\n        self.smoothing= smoothing\n        self.size= size\n        self.true_dist= None\n\n    def forward(self, x, target):\n        assert x.size(1) == self.size # vocab_size\n        true_dist= x.data.clone() # clone the data to avoid in-place operation\n        true_dist.fill_(self.smoothing / (self.size - 2))\n        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        true_dist[:, self.padding_idx]= 0 # ignore the category of padding_idx\n        mask= torch.nonzero(target == self.padding_idx)\n        if mask.dim() &gt; 0:\n            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n        self.true_dist= true_dist\n        return self.criterion(x, true_dist.clone().detach())\n\n\n\n3. 分布式训练加速\n多 GPU 训练：\n\ndef train_worker(\n    gpu,\n    ngpus_per_node,\n    vocab_src,\n    vocab_tgt,\n    spacy_de,\n    spacy_en,\n    config,\n    is_distributed= True\n):\n    print(f'Train worker process using GPU: {gpu} for training', flush=True)\n    torch.cuda.set_device(gpu)\n\n    pad_idx= vocab_tgt[\"&lt;blank&gt;\"]\n    d_model= 512\n    model= make_model(len(vocab_src), len(vocab_tgt), N=6)\n    model.cuda(gpu)\n    module= model\n\n    is_main_process= True\n    if is_distributed:\n        dist.init_process_group(\n            backend='nccl',\n            init_method='env://',\n            rank=gpu,\n            world_size=ngpus_per_node\n        )\n        model= DDP(model, device_ids=[gpu])\n        module= model.module\n        is_main_process= gpu == 0\n    \n    criterion= LabelSmoothing(\n        size=len(vocab_tgt),\n        padding_idx=pad_idx,\n        smoothing=0.1\n    )\n    criterion.cuda(gpu)\n\n    train_dataloader, valid_dataloader= create_dataloaders(\n        gpu,\n        vocab_src,\n        vocab_tgt,\n        spacy_de,\n        spacy_en,\n        batch_size=config['batch_size'] // ngpus_per_node,\n        max_padding=config['max_padding'],\n        is_distributed=is_distributed\n    )\n\n    optimizer= torch.optim.Adam(\n        model.parameters(),\n        lr=config['base_lr'],\n        betas=(0.9, 0.98),\n        eps=1e-9\n    )\n    lr_scheduler= LambdaLR(\n        optimizer= optimizer,\n        lr_lambda= lambda step: rate(\n            step, model_size=d_model,\n            factor=1, warmup=config['warmup']\n        )\n    )\n    train_state= TrainState()\n\n    for epoch in range(config['num_epochs']):\n        if is_distributed:\n            train_dataloader.sampler.set_epoch(epoch)\n            valid_dataloader.sampler.set_epoch(epoch)\n\n        model.train()\n        print(f'[GPU {gpu}] Epoch {epoch} Training ====', flush=True)\n        _, train_state= run_epoch(\n            (Batch(b[0], b[1], pad_idx) for b in train_dataloader),\n            model,\n            SimpleLossCompute(module.generator, criterion),\n            optimizer,\n            lr_scheduler,\n            mode='train+log',\n            accum_iter=config['accum_iter'],\n            train_state=train_state\n        )\n\n        GPUtil.showUtilization()\n        if is_main_process:\n            file_path= '%s%.2d.pt' % (config['file_prefix'], epoch)\n            torch.save(module.state_dict(), file_path)\n        torch.cuda.empty_cache()\n\n        print(f'[GPU {gpu}] Epoch {epoch} Validation ====', flush=True)\n        model.eval()\n        sloss= run_epoch(\n            (Batch(b[0], b[1], pad_idx) for b in valid_dataloader),\n            model,\n            SimpleLossCompute(module.generator, criterion),\n            DummyOptimizer,\n            DummyScheduler,\n            mode='eval'\n        )[0]\n        print(f'[GPU {gpu}] Epoch {epoch} Validation Loss: {sloss}', flush=True)\n        torch.cuda.empty_cache()\n\n    if is_main_process:\n        file_path= '%sfinal.pt' % (config['file_prefix'])\n        print(f'Training finished.\\nModel saved to {file_path}.pt.', flush=True)\n        torch.save(\n            module.state_dict(),\n            file_path\n        )\n\n\n\n\n实例\n我们使用 WMT 2014 英德翻译数据集进行训练，完成德语到英语的翻译任务。\n\n\n\nTraining\n\n\n完整注释的 python 代码已经放进了星球里。"
  },
  {
    "objectID": "blog/2025/04/17/Conventional_Commits/index.html",
    "href": "blog/2025/04/17/Conventional_Commits/index.html",
    "title": "Conventional Commits 规范详解",
    "section": "",
    "text": "背景\n在软件开发中，版本控制是一个至关重要的环节。Git 是最流行的版本控制系统之一，而 Git 提交信息（commit message）则是记录代码变更的重要方式。而我们经常会遇见：\n\n查看项目 Git 日志时，提交信息杂乱无章，完全看不懂每个提交的意图。\n手动编写 CHANGELOG 耗时费力，还容易遗漏重要变更。\n团队协作中，成员提交风格不一，代码审查效率低下\n\n为了提高 Git 提交信息的可读性和一致性，Conventional Commits 规范（地址：https://www.conventionalcommits.org/en/v1.0.0/）应运而生。这是一套被 Angular、Vue 等顶级开源项目广泛采用的 Git 提交规范，能将提交信息从混乱的草稿变为清晰的文档。\n这里，我们将详细介绍 Conventional Commits 规范的定义、格式、使用场景以及一些最佳实践。\n\n\n\nConventional Commits\n\n\n\n\nConventional Commits 规范\nConventional Commits 是一套基于 Git 提交消息的轻量级约定，旨在通过结构化格式提升提交信息的可读性和自动化处理能力。它的核心目标包括：\n\n人类可读：清晰描述提交的意图和影响范围。\n机器友好：支持自动化生成 CHANGELOG 和语义化版本（SemVer）。\n团队协作：统一提交风格，减少沟通成本。\n\n\n\n格式结构\n一条规范的提交消息需包含以下部分（部分可选）：\n&lt;类型&gt;[可选范围]: &lt;简短描述&gt;\n\n[可选正文]\n\n[可选脚注]\n\n1. 类型（type）\n类型是提交的核心部分，表示提交的目的和性质。常见的类型包括：\n\n\n\n类型\n用途\n对应 SemVer\n\n\n\n\nfeat\n新增功能\nMINOR（次版本）\n\n\nfix\n修复 Bug\nPATCH（补丁版本）\n\n\nBREAKING CHANGE\n破坏性变更（如 API 不兼容）\nMAJOR（主版本）\n\n\ndocs\n文档更新\n-（不影响版本）\n\n\nstyle\n代码格式调整（如缩进、空格）\n-（不影响版本）\n\n\nrefactor\n代码重构（不新增功能或修复 Bug）\n-（不影响版本）\n\n\n\n其他推荐类型有：build（构建系统）、ci（持续集成）、test（测试用例）等。\n这里我们插一句，对于版本号的控制，可以参考以下规则：\n\n\n\n版本号控制\n\n\n\n\n2. 可选范围（scope）\n可选范围用于指定提交影响的模块或功能区域。它可以是一个具体的模块名、文件名或其他标识符。范围有助于快速定位问题和理解变更的上下文。\n例如：feat(auth): add JWT authentication 表示在 auth 模块中新增了 JWT 身份验证功能。\n\n\n3. 简短描述（subject）\n简短描述是对提交内容的简要说明，通常不超过 72 个字符。它应以小写字母开头，并使用祈使句（imperative mood）来描述变更的目的。\n例如：fix: correct typo in README 表示修复了 README 文件中的拼写错误。\n\n\n4. 可选正文（body）\n可选正文用于详细描述提交的内容和背景信息。它可以包含多行文本，通常用于解释为什么要进行此更改、如何实现等。正文应与简短描述之间空一行。\n例如：\nfeat: 支持多语言切换  \n  \n新增中英文切换按钮，默认跟随系统语言。  \n依赖第三方库 `i18n-utils`，需运行 `npm install` 安装。  \n\n\n5. 可选脚注（footer）\n可选脚注用于记录与提交相关的其他信息，如关联的任务、问题或破坏性变更。它通常以 BREAKING CHANGE: 开头，后跟详细说明。脚注也可以包含引用的 issue 编号或链接。\n例如：\nBREAKING CHANGE:\n- 更新了 API 接口，删除了 `getUser` 方法，改为 `fetchUser`。\n- 需要更新客户端代码以适应新接口。\nCloses #123\nReview by @zhenlu\n\n\n\n为什么推荐使用\n\n1. 自动化工具支持\n\n生成 CHANGELOG：工具（如 standard-version）可自动从提交历史提取内容生成变更日志。\n语义化版本控制：根据提交类型自动升级版本号（feat → MINOR，fix → PATCH，BREAKING CHANGE → MAJOR）。\n代码审查加速：通过类型快速定位提交意图，减少沟通成本。\n\n\n\n2. 团队协作效率提升\n\n统一提交风格：团队成员遵循相同的提交规范，减少沟通障碍，避免风格混乱。\n清晰的历史记录：通过结构化的提交信息，团队成员可以快速了解项目进展和变更。\n问题追溯：通过关联的 issue 编号和脚注，快速定位问题和变更历史。\n\n\n\n3. 开源项目友好性\n\n开源项目通常需要清晰的文档和变更记录，Conventional Commits 规范可以帮助维护者和用户快速了解项目的演变。\n社区贡献者可以通过遵循规范的提交信息，轻松参与项目开发，降低贡献门槛。\n\n\n\n\nTakeaway\nConventional Commits 不仅是技术规范，更是团队协作的沟通协议。通过标准化提交信息，它能将 Git 日志从杂乱的历史记录升级为项目演进的清晰路线图。无论是个人项目还是大型团队，尽早引入这一规范都将显著提升开发效率和代码质量。"
  },
  {
    "objectID": "blog/2025/04/09/pe0003/index.html",
    "href": "blog/2025/04/09/pe0003/index.html",
    "title": "PE0003: Largest Prime Factor",
    "section": "",
    "text": "题目\nThe prime factors of 13195 are 5, 7, 13, and 29. What is the largest prime factor of the number 600851475143?\n\n\n问题描述\n题目要求：给定一个合数（如600851475143），找到它的最大质因数。 示例：13195的质因数为5、7、13、29，最大质因数是29。\n\n\n解答\n\n方法一：基础试除法（适合小数据）\n思路：从2开始逐个试除，记录能整除的最大质数。若剩余数&gt;1，则其本身为质数，直接作为最大质因数。\n\nimport math\n\ndef find_largest_prime_factor(n):\n    assert n &gt; 1, \"输入必须大于1\"\n    largest_factor = 1\n    for i in range(2, math.isqrt(n) + 1):  # 只需检查到sqrt(n)\n        while n % i == 0:                  # 完全除尽当前质因数\n            largest_factor = i\n            n //= i\n    if n &gt; 1:  # 剩余n本身是质数\n        largest_factor = n\n    return largest_factor\n\nresult = find_largest_prime_factor(600851475143)\nprint(result)  # 输出：6857\n\n6857\n\n\n优点：逻辑简单，适合理解质因数分解原理。 缺点：未优化时需遍历所有数，大数效率低。\n\n\n优化试除法（效率翻倍）\n核心优化：单独处理2，2是唯一的偶质数，先除尽2，后续只需检查奇数。\n\ndef optimized_largest_prime_factor(n):\n    if n % 2 == 0:\n        largest_factor = 2\n        while n % 2 == 0:\n            n //= 2\n    else:\n        largest_factor = 1\n    \n    factor = 3\n    max_factor = math.isqrt(n)\n    while n &gt; 1 and factor &lt;= max_factor:\n        if n % factor == 0:\n            largest_factor = factor\n            while n % factor == 0:\n                n //= factor\n            max_factor = math.isqrt(n)  # 更新平方根上限\n        factor += 2  # 只检查奇数\n    \n    if n &gt; 1:\n        largest_factor = n\n    return largest_factor\n\nresult = optimized_largest_prime_factor(600851475143)\nprint(result)  # 输出：6857\n\n6857\n\n\n优点：时间复杂度降低50%，适合千万级数据。\n\n\n方法三：numpy\n思路：利用 numpy的向量化运算，快速筛选质因数。\n\nimport numpy as np\ndef s3(n):\n    assert n &gt; 1, \"Input must be greater than 1\"\n    ranges= np.array(range(2, math.isqrt(n) + 1))\n    factors= ranges[n % ranges == 0]\n    while True:\n        if np.any(factors[-1] % factors[:-1] == 0):\n            factors= factors[:-1]\n        else:\n            break\n\n    return int(factors[-1])\n\nn= 600_851_475_143\ns3(n)\n\n6857\n\n\n\n\n\n答案\n答案为6857。"
  },
  {
    "objectID": "blog/2025/04/02/dragen_wgs/index.html",
    "href": "blog/2025/04/02/dragen_wgs/index.html",
    "title": "WGS: GATK vs GraphTyper vs DRAGEN",
    "section": "",
    "text": "引言\n随着高通量测序技术的飞速发展，全基因组测序（WGS）已成为精准医学和基因组学研究的重要支柱。然而，WGS 数据分析是一项复杂任务，尤其是变异检测环节，需要兼顾准确性、速度和易用性。GATK、GraphTyper 和 DRAGEN 正是这一领域的代表性工具，它们通过不同的技术手段，满足了不同的分析需求。\n这里我们将深入探讨这三种工具的区别与联系，帮助更好地理解它们在 WGS 数据分析中的应用。\n\n\nGATK：业界标准\nGATK（Genome Analysis Toolkit）是由 Broad Institute 开发的一套开源工具集，专为高通量测序数据分析设计。它在变异检测方面表现尤为突出，能够精准识别单核苷酸变异（SNV）和插入缺失（Indel）。作为 WGS 数据分析的业界标准，GATK 被广泛应用于人类基因组研究。GATK 提供从原始数据（BAM）到变异结果（VCF）的全流程解决方案，以严谨的最佳实践著称，适合需要高度定制化分析的研究项目，尤其是在追求准确性和灵活性时。其通过多步骤流程（包括 reads 比对、变异调用和过滤）确保结果可靠。\n\n\nGraphTyper：基于图的变异检测\nGraphTyper 是一种创新的变异检测工具，采用变异感知的基因组图（variation-aware genome graph）技术。与传统基于线性参考基因组的方法不同，它通过构建包含变异信息的图结构来比对测序 reads，从而减少参考等位基因偏倚。GraphTyper 特别适用于基因组复杂区域的变异检测，或需要提升检测灵敏度的研究，尤其在 SNV 和 Indel 检测中表现出色，因而适合处理大规模 WGS 数据，广泛用于群体遗传学研究。\n\n\nDRAGEN：硬件加速的快速分析平台\nDRAGEN（Dynamic Read Analysis for GENomics）是 Illumina 推出的硬件加速平台，利用 FPGA（现场可编程门阵列）技术实现超快速数据处理。它能够从原始 reads 到变异检测的全流程分析，效率远超传统软件工具。对于 30X 覆盖度的 WGS 数据，DRAGEN可在约30分钟内完成分析，极大地缩短了数据处理时间。除了 SNV 和 Indel，还能检测短串联重复（STR）、结构变异（SV）和拷贝数变异（CNV）。"
  },
  {
    "objectID": "blog/2025/03/31/pe0001/index.html",
    "href": "blog/2025/03/31/pe0001/index.html",
    "title": "PE0001: Multiples of 3 and 5",
    "section": "",
    "text": "题目\nIf we list all the natural numbers below 10 that are multiples of 3 or 5, we get 3, 5, 6, 9. The sum of these multiples is 23. Find the sum of all the multiples of 3 or 5 below 1000.\n\n\n问题描述\n题目要求：找出所有小于1000的自然数中，3或5的倍数的总和。 例如，小于10的符合条件的数有3、5、6、9，它们的和为23。\n\n\n解答\n\n暴力遍历法（适合小数据）\n思路：遍历1到999的所有数，逐个检查是否为3或5的倍数，累加符合条件的数。\n\n# Solution to Project Euler Problem 1\ndef sum_of_multiples(limit):\n    return sum(x for x in range(limit) if x % 3 == 0 or x % 5 == 0)\n\nresult = sum_of_multiples(10)\nresult\nresult = sum_of_multiples(1000)\nresult\n\n233168\n\n\n优点：简单直观，适合编程新手。 缺点：当数据量极大（如十亿）时，遍历耗时极长。\n\n\n方法二：数学公式法\n思路：利用等差数列求和公式，避免重复计算。分别计算3、5的倍数总和，减去15的倍数总和（因为15的倍数被重复计算了两次）。\n\n# solution using a more efficient method\ndef SumDivisibleBy(n, limit) -&gt; int:\n    \"\"\"\n    Returns the sum of all numbers less than limit that are divisible by n.\n    \"\"\"\n    # Find the largest number less than limit that is divisible by n\n    p = (limit - 1) // n\n    # Use the formula for the sum of an arithmetic series to calculate the sum\n    return n * p * (p + 1) // 2\n\nSumDivisibleBy(3, 10) + SumDivisibleBy(5, 10) - SumDivisibleBy(15, 10)\nSumDivisibleBy(3, 1000) + SumDivisibleBy(5, 1000) - SumDivisibleBy(15, 1000)\n\n233168\n\n\n优点：时间复杂度仅为 O(1)，十亿级数据也能秒出结果。\n\n\n\n答案\n答案为233168。"
  },
  {
    "objectID": "blog/2025/03/22/SCI_journal_partitions/index.html",
    "href": "blog/2025/03/22/SCI_journal_partitions/index.html",
    "title": "来拿2025年中科院最新分区Excel表",
    "section": "",
    "text": "3 月 20 日，期刊分区表官方正式发布了 2025 年升级版《2025 年中国科学院文献情报中心期刊分区表》。\n\n\n\n期刊分区表官方\n\n\n我们也是第一时间将最新的中科院分区表医学大类的期刊数据爬出来整理成 Excel 表格，供大家科研投稿参考。\n大家公众号后台回复2025中科院分区即可获取表格链接。\n注意：直接复制回复2025中科院分区这几个字即可，不要打错了哦。"
  },
  {
    "objectID": "blog/2025/03/10/balanced_design/02_index.html",
    "href": "blog/2025/03/10/balanced_design/02_index.html",
    "title": "优化有效人数比例",
    "section": "",
    "text": "引言\n我们前面讨论了平衡设计和非平衡设计的统计学效率，这里我们讨论，如何在总样本量固定为 \\(N = n_1 + n_2\\) 以及 \\(\\text{Var}(\\bar{x}_1 - \\bar{x}_2) = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}=K\\) 的情况下，追求最大的预期有效人数比例。\n需要强调的是，我们这里的目标是处于伦理的考虑最小化非响应者总数，而不是追求统计学效率。\n问题描述\n我们假设，对于某一个高优指标，超过某一阈值 \\(c\\) 的患者被认为是治疗有效。那么，自然地，对于组别 \\(X\\) 而言，某一受试者治疗无效的概率为：\n\\[\nP(X&lt;c)=\\Phi\\left(\\frac{c - \\mu_X}{\\sigma_X}\\right)\n\\]\n其中：\n\n\\(\\Phi(\\cdot)\\) 是标准正态分布的累积分布函数。\n\\(\\Phi\\left(\\frac{c - \\mu_X}{\\sigma_X}\\right)\\) 和 \\(\\Phi\\left(\\frac{c - \\mu_Y}{\\sigma_Y}\\right)\\) 分别表示组 \\(X\\) 和组 \\(Y\\) 中非响应者的概率。\n\\(c\\) 是响应阈值，\\(\\mu_X, \\mu_Y\\) 是组 \\(X\\) 和 \\(Y\\) 的均值，\\(\\sigma_X, \\sigma_Y\\) 是标准差。\n优化问题\n我们希望追求最大的预期有效人数比例，即，目标是最小化非响应者总数： \\[\n\\min \\left\\{ n_1 \\Phi\\left(\\frac{c - \\mu_X}{\\sigma_X}\\right) + n_2 \\Phi\\left(\\frac{c - \\mu_Y}{\\sigma_Y}\\right) \\right\\},\n\\]\n约束条件为：\n\n\\(n_1 + n_2 = N\\)（总样本量固定）。\n\\(\\frac{\\sigma_X^2}{n_1} + \\frac{\\sigma_Y^2}{n_2} = K\\)（\\(Z\\) 统计量的分母固定，以保证检验的特定功率）。\n公式推导\n1. 构造拉格朗日函数\n在优化问题中，拉格朗日乘数法用于将有约束的极值问题转化为无约束问题，其核心是：将目标函数与约束条件结合，通过引入乘数（\\(\\lambda\\) 和 \\(\\mu\\)）将约束条件融入优化过程。\n\\[\n\\mathcal{L} = n_1 \\Phi_X + n_2 \\Phi_Y + \\lambda (N - n_1 - n_2) + \\mu \\left( K - \\frac{\\sigma_X^2}{n_1} - \\frac{\\sigma_Y^2}{n_2} \\right)\n\\]\n其中 \\(\\Phi_X = \\Phi\\left(\\frac{c-\\mu_X}{\\sigma_X}\\right)\\), \\(\\Phi_Y = \\Phi\\left(\\frac{c-\\mu_Y}{\\sigma_Y}\\right)\\)，\\(n_1 \\Phi_X + n_2 \\Phi_Y\\) 是目标函数，\\(\\lambda (N - n_1 - n_2)\\) 是总样本量约束，\\(\\mu \\left( K - \\frac{\\sigma_X^2}{n_1} - \\frac{\\sigma_Y^2}{n_2} \\right)\\) 是方差约束。\n2. 求偏导并令其为零\n通过对 \\(n_1\\) 和 \\(n_2\\) 求偏导并令其为零，可以得到极值条件：\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial n_1} = \\Phi_X - \\lambda + \\mu \\frac{\\sigma_X^2}{n_1^2} = 0 \\quad \\Rightarrow \\quad \\lambda = \\Phi_X + \\mu \\frac{\\sigma_X^2}{n_1^2}\n\\]\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial n_2} = \\Phi_Y - \\lambda + \\mu \\frac{\\sigma_Y^2}{n_2^2} = 0 \\quad \\Rightarrow \\quad \\lambda = \\Phi_Y + \\mu \\frac{\\sigma_Y^2}{n_2^2}\n\\]\n通过联立对 \\(n_1\\) 和 \\(n_2\\) 的偏导方程，消去 \\(\\lambda\\) 后得到：\n\\[\n\\Phi_X + \\mu \\frac{\\sigma_X^2}{n_1^2} = \\Phi_Y + \\mu \\frac{\\sigma_Y^2}{n_2^2}\n\\]\n\\[\n\\mu \\left( \\frac{\\sigma_X^2}{n_1^2} - \\frac{\\sigma_Y^2}{n_2^2} \\right) = \\Phi_Y - \\Phi_X \\tag{1}\n\\]\n设 \\(g = \\frac{n_1}{N}\\)，\\(n_1 = N g\\)，\\(n_2 = N (1 - g)\\)，则：\n\\[\n\\frac{\\sigma_X^2}{g} + \\frac{\\sigma_Y^2}{1-g} = KN \\tag{2}\n\\]\n\\[\n\\mu \\left( \\frac{\\sigma_X^2}{(gN)^2} - \\frac{\\sigma_Y^2}{[(1-g)N]^2} \\right) = \\Phi_Y - \\Phi_X\n\\]\n求解 \\((1)\\) 和 \\((2)\\)，得到：\n\\[\n\\frac{\\sigma_X}{g} \\sqrt{\\Phi_Y} = \\frac{\\sigma_Y}{1-g} \\sqrt{\\Phi_X} \\quad \\Rightarrow \\quad g = \\frac{\\sigma_X \\sqrt{\\Phi_Y}}{\\sigma_X \\sqrt{\\Phi_Y} + \\sigma_Y \\sqrt{\\Phi_X}}\n\\]\n\\[\ng = \\frac{\\sigma_X \\sqrt{\\Phi\\left(\\frac{c-\\mu_Y}{\\sigma_Y}\\right)}}{\\sigma_X \\sqrt{\\Phi\\left(\\frac{c-\\mu_Y}{\\sigma_Y}\\right)} + \\sigma_Y \\sqrt{\\Phi\\left(\\frac{c-\\mu_X}{\\sigma_X}\\right)}}\n\\]\n3. 关键结论\n在 \\(\\mu \\neq 0\\) 的情况下，推导过程表明：\n\n最优分配比例 \\(g\\) 同时依赖于两组的标准差和非响应概率。\n若某组的非响应概率更低（即 \\(\\Phi_X\\) 或 \\(\\Phi_Y\\) 更小），应分配更多样本给该组以降低总非响应数。\n若某组方差更大（\\(\\sigma_X\\) 或 \\(\\sigma_Y\\) 更大），需权衡其非响应概率与方差对统计功效的影响。\n\n通过 \\(g\\) 的计算，可以确定最优的样本分配比例，以最小化非响应者总数，成功平衡了伦理与效率的双重目标。在实际中，我们可以先根据一些初始值来进行试验设计，然后通过 \\(g\\) 的计算再来优化后续试验的样本分配比例。这也是一种反应-适应性设计的思路。\n示例\n以为 R 代码示例，计算 \\(g\\) 值：\n\nmu_X &lt;- 2    \nmu_Y &lt;- 1.5 \nsigma_X &lt;- 1 \nsigma_Y &lt;- 1.2 \nc &lt;- 2      \n\nprob_X &lt;- pnorm((c - mu_X) / sigma_X)\nprob_Y &lt;- pnorm((c - mu_Y) / sigma_Y)\ng &lt;- (sigma_X * sqrt(prob_Y)) / (sigma_X * sqrt(prob_Y) + sigma_Y * sqrt(prob_X))\n\ncat(\"非响应概率 (X):\", prob_X, \"\\n\")\n\n非响应概率 (X): 0.5 \n\ncat(\"非响应概率 (Y):\", prob_Y, \"\\n\")\n\n非响应概率 (Y): 0.6615389 \n\ncat(\"最优分配比例 g =\", round(g, 4), \"\\n\")\n\n最优分配比例 g = 0.4894"
  },
  {
    "objectID": "blog/2025/03/08/randomizationandBlindness/index.html",
    "href": "blog/2025/03/08/randomizationandBlindness/index.html",
    "title": "证明随机化和盲法消除向均值回归偏倚",
    "section": "",
    "text": "引言\n讨论治疗分配时采用随机化和评估处理效应时采用盲法将如何帮助消除向均值回归所导致的偏倚。\n\n\n背景\n我们假设 \\(X_1\\) 是处理组的基线测量值，\\(X_2\\) 是同一组的随访测量值，且配对测量值 \\((X_1, X_2)\\) 服从二元正态分布。向均值回归（regression to the mean）是一种统计现象，是仅对那些具有 极端 初始测量值的个体进行第二次测量时发生的现象，指基线测量值 \\(X_1\\) 较高（低）的个体在随访测量中 \\(X_2\\) 回归到总体均值 \\(\\mu\\) 的现象。\n在临床试验中，这种现象可能被误认为是治疗效果。这里我们试着使用线性回归模型证明，随机化和盲法可以消除这种向均值回归的偏倚。\n\n\n线性回归模型\n设处理组的 \\(X_1\\) 和 \\(X_2\\) 二元正态分布的均值向量和协方差矩阵为：\n\\[\n\\begin{pmatrix} X_1 \\\\ X_2 \\end{pmatrix} \\sim \\text{N} \\left( \\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\end{pmatrix}, \\begin{pmatrix} \\sigma_1^2 & \\rho\\sigma_1\\sigma_2 \\\\ \\rho\\sigma_1\\sigma_2 & \\sigma_2^2 \\end{pmatrix} \\right)\n\\]\n其中，\n\\(\\rho = \\frac{\\text{Cov}(X_1, X_2)}{\\sigma_1 \\sigma_2}\\) 是 \\(X_1\\) 和 \\(X_2\\) 之间的相关系数。\n我们使用简单的线性回归模型来描述 \\(X_1\\) 和 \\(X_2\\) 之间的关系：\n\\[\nE(X_2|X_1) = \\alpha + \\beta X_1= \\mu_2 + \\rho \\frac{\\sigma_2}{\\sigma_1} (X_1 - \\mu_1)\n\\]\n其中：\n\n\\(\\alpha = \\mu_2- \\beta \\mu_1\\) ，\n\\(\\beta= \\rho \\frac{\\sigma_2}{\\sigma_1}\\) 。\n\n\n\n向均值回归的影响\n我们假设该处理组无治疗效果，即 \\(\\mu_1=\\mu_2=\\mu\\) 且 \\(\\sigma_1=\\sigma_2=\\sigma\\) ，则对于特定值 \\(X_1=x_1\\)，有：\n\\[\nE(X_2|X_1=x_1) = \\mu + \\rho (x_1-\\mu)\n\\]\n在等号两侧各减去 \\(\\mu\\) 并取绝对值后，有以下不等式成立：\n\\[\n|E(X_2|X_1=x_1) - \\mu| = |\\rho| |(x_1-\\mu)| \\leq |(x_1-\\mu)|\n\\]\n如此一来，我们可以看到，当 \\(x_1\\) 取极端值时，\\(X_2\\) 与 \\(\\mu\\) 之间的差异大体上始终会减小，\\(X_2\\) 的期望值会向 \\(\\mu\\) 靠拢，这也就是著名的向均值回归现象。而在临床试验中，这种自然变化可能被误认为是治疗效果（因为在无处理效应时却看似产生了效果），从而带来偏倚。\n上面式子也可以表示为:\n\\[\n|E(X_2|X_1=x_1)-x_1| = |(\\rho-1)(x_1-\\mu)|\n\\]\n可以更加清楚地看到，对于特定的 \\(X_1=x_1\\)，公式左侧向均值回归所产生的影响通常不为零。只有当 \\(\\rho=1\\) 或者 \\(x_1\\) 是一个等于总体均值 \\(\\mu\\) 的完美样本时，向均值回归的影响消失为零。\n\n\n随机化的作用\n接下来，我们开始讨论随机化如何帮助消除向均值回归的偏倚。\n由于随机化，使得处理组和对照组的基线测量值的分布相同，即均值均为 \\(\\mu\\)，方差相同。对照组模型为：\n\\[\n|E(Y_2|Y_1=y_1)-y_1| = |(\\rho-1)(y_1-\\mu)|\n\\]\n处理组和对照组均会发生向均值回归，治疗效果则通常通过组间差异来评估：\n\\[\n\\Delta = |E(X_2|X_1=x_1)-x_1| - |E(Y_2|Y_1=y_1)-y_1|\n\\]\n因为随机化使得 \\(|E(X_2|X_1=x_1)-x_1| \\approx |E(Y_2|Y_1=y_1)-y_1|\\)，向均值回归的影响在 \\(\\Delta\\) 中抵消。若存在处理效应，则经过随机化之后得到的 \\(\\Delta\\) 会更接近于真实的治疗效果，而排除了向均值回归得偏倚。\n\n\n盲法的作用\n而对于盲法帮助消除向均值回归的偏倚，主要在于盲法确保在不知道治疗分配情况下的 \\(x_2\\) 和 \\(y_2\\) 测量的客观性，避免人为放大或掩盖向均值回归的自然变化，从而进一步防止向均值回归被误解为治疗效应。\n\n\n结论\n随机化通过均衡基线消除向均值回归的组间差异，盲法通过客观测量避免主观偏倚，两者共同确保治疗效果估计无偏。"
  },
  {
    "objectID": "blog/2025/03/03/Table1inRCT/index.html",
    "href": "blog/2025/03/03/Table1inRCT/index.html",
    "title": "临床试验中基线数据p值真的有意义吗？",
    "section": "",
    "text": "在临床试验的结果表格中，你是否经常见到患者人口学与基线特征的统计表，附带一堆 p 值？这些 p 值到底在验证什么？是证明随机化的成功，还是暴露试验设计的漏洞？\n这里，我们试图用一篇文章来讨论下这个问题。\n\n随机化的完美幻觉：基线表格里的 p 值\n临床试验的核心是随机化分配（Randomization），它试图通过不可预测的分组消除混杂因素。然而，许多论文在结果部分展示基线数据时，会针对性别、年龄等变量计算治疗组间的 p 值。这看似严谨的操作，实则暗藏争议。\n我们争议的焦点在于：\n\n1. 无意义论\n如果试验已声明进行了随机化，则所有基线差异（无处理效应）的 p 值本质上反映的是随机误差，而非真实效应。任何由极小 p 值提示的统计显著性都是第一类错误，毫无意义。例如，在 100 次基线比较中，即使完全随机，按 5% 显著性水平也会出现 5 次假阳性。\n\n\n2. 验真派\n认为 p 值可用于验证随机化是否真实执行。比如，若基线变量显示系统性差异（如年龄显著不均衡），可能暗示分配过程存在人为干扰。\n\n\n\n当组间均衡性遭遇挑战：何时该调整协变量？\n随机化的理想情况是治疗组间基线完全均衡，但现实往往骨感。当某个预后因素（如疾病严重程度）不均衡时，该如何处理？这里，我们一般的逻辑是：\n\n1. 不调整也能成立\n即使基线不均衡，对于真正的随机化试验，我们仍然可以相信在没有纳入任何协变量的情况下，处理效应主分析的直接结果。无论观察到的处理与协变量之间的关联性如何，主效应分析的结果仍然是有效的。从这个角度看，基线表格里的 p 值是毫无必要的。\n\n\n2. 调整会更高效\n而另一方面，若某变量与结局强相关且组间不均衡，纳入协变量分析（如 ANCOVA 模型）可减少误差、提升统计效能。\n\n\n实战指南：如何科学设计基线表格？\n\n\n\nNEJM\n\n\n这里，我们给研究者的具体建议是：\n\n简化p值展示：随机化试验中，基线p值无必要，直接呈现变量分布即可。\n关注临床意义：若关键变量（如年龄、疾病分期）组间差异超过10%，需讨论是否调整分析模型。\n\n\n\n\n结语：回归医学研究的本质\n临床试验的终极目标是评估治疗效应，而非追求表格的 完美对齐。基线可比性应靠科学设计保障，而非事后修补。当一篇论文用大量 p 值自证清白时，我们或许更该追问：它的随机化是否真正无懈可击？分配过程是否足够盲态？毕竟，在医学进步的征途上，随机化需要的是技术硬核，而非统计学 p 值。"
  },
  {
    "objectID": "blog/2025/02/13/simulating_survival_models/index.html",
    "href": "blog/2025/02/13/simulating_survival_models/index.html",
    "title": "SAS 生存分析模拟",
    "section": "",
    "text": "生存分析（Survival Analysis）是统计学中用于分析事件发生时间的研究方法。在生存分析中，常常使用比例风险模型（Proportional Hazards Models），例如 Cox 回归模型，来探索影响生存时间的因素。\n这里，我们将深入解析如何使用 SAS 进行生存分析模拟，特别是如何生成符合特定假设的生存数据，并使用 Cox 回归模型进行分析。\n\n1. 生存数据模拟\n在生存分析中，数据的模拟是研究者进行仿真实验、理解模型行为以及验证方法的重要步骤。以下代码展示了如何使用 SAS 模拟符合指数分布（Exponential Distribution）和 Weibull 分布的生存数据。\n\n指数分布与 Cox 回归模型\n指数分布是生存数据中常见的一种分布模型，尤其适用于模拟没有明显时间变化的基线风险（constant baseline hazard），其形式为：\n\\[\n\\lambda(t) = \\lambda_0\n\\]\n其中，\\(\\lambda_0\\) 是基线风险，它在整个时间范围内保持不变。对于 Cox 回归模型来说，基线风险函数并未指定，而是通过比例风险的假设，估计与协变量相关的风险比（hazard ratio）。\n%macro RandExp(sigma);\n    ((&sigma) * rand(\"Exponential\"))\n%mend;\n在这段代码中，我们定义了一个宏 RandExp，它用于生成指数分布的随机数。rand(\"Exponential\") 生成一个符合指数分布的随机数，而 &sigma 为分布的尺度参数。这里我们通过宏将其封装，方便后续的调用。\n接下来，我们创建一个包含100个观测值的数据集 PHData，该数据集模拟了一个包含固定效应（例如协变量 x1 和 x2）和随机事件时间（t）的数据集。\ndo i = 1 to &N;\n        xx1{i} = rand(\"Normal\"); xx2{i} = rand(\"Normal\");\n    end;\n    baseHazardRate = 0.002; /* 事件发生的基线风险 */\n    censorRate = 0.001; /* 被删失（censoring）的风险 */\n    do i = 1 to &N;\n        x1 = xx1{i}; x2 = xx2{i};\n        eta = -2*x1 + 1*x2; /* 线性预测变量 */\n        tEvent = %RandExp( 1/(baseHazardRate * exp(eta)) ); /* 根据线性预测模拟事件时间 */\n        c = %RandExp( 1/censorRate ); /* 被删失时间 */\n        t = min(tEvent, c); /* 事件时间或删失时间 */\n        censored = (c &lt; tEvent); /* 是否删失的指示变量 */\n        output;\n    end;\n在这部分代码中，我们首先通过 rand(“Normal”) 生成了两个标准正态分布的随机变量 x1 和 x2 作为协变量。然后，我们使用这些协变量和基线风险来计算事件的时间（tEvent）和删失时间（c）。每个观察值的事件时间 t 是事件时间和删失时间中的较小值，而 censored 变量则指示该观测是否为删失。\n\n\n\n2. 使用Cox回归模型进行分析\n生成生存数据后，我们可以使用 SAS中的 PHREG 拟合比例风险模型。在我们的例子中，我们将使用 Cox 回归模型来估计协变量 x1 和 x2 对生存时间的影响。\nods graphics on;\nproc phreg data=PHData plots(overlay CL)= (survival);\n    model t*censored(1)= x1-x2;\n    ods select CensoredSummary ParameterEstimates\n        ReferenceSet SurvivalPlot;\nrun;\n这里运行结果将提供Cox回归模型的参数估计，包括协变量的风险比（Hazard Ratios），以及不同协变量条件下的生存函数图。\n\n\n\n参数估计\n\n\n\n\n\n生存曲线\n\n\n完整代码已经上传至星球，感兴趣的同学可以自行查看。"
  },
  {
    "objectID": "blog/2025/01/28/red_packet/index.html",
    "href": "blog/2025/01/28/red_packet/index.html",
    "title": "祝大家新年快乐！",
    "section": "",
    "text": "大家除夕快乐，感谢大家一直以来对我们公众号的关注和支持！\n新的一年，希望我们继续一起加油进步，共同深度求索更多有价值的内容！"
  },
  {
    "objectID": "blog/2025/01/20/red_packet/index.html",
    "href": "blog/2025/01/20/red_packet/index.html",
    "title": "新年红包封面来了",
    "section": "",
    "text": "一年一度的事情，用 R 几行代码简单画个微信红包封面，一共发放 6000 个，感谢大家一直以来对我们公众号的关注和支持！\n请进入公众号文章领取。\n\n\n\n2025_happy_newyear\n\n\n用了 magick，ggplot2，showtext 几个包，代码直接已经放进了星球里，感兴趣的同学可以去看看。"
  },
  {
    "objectID": "blog/2025/01/07/tracy_widom_test/index.html",
    "href": "blog/2025/01/07/tracy_widom_test/index.html",
    "title": "Tracy-Widom Statistics",
    "section": "",
    "text": "在现代基因组学和统计学中，Tracy-Widom 统计量（Tracy-Widom statistics）是一个广泛应用于分析主成分分析（PCA）结果的重要工具。尤其在遗传学、群体学和数据科学等领域，Tracy-Widom 统计量可以帮助研究者评估主成分的统计显著性，为后续的研究分析提供强有力的支持。\n这里，我们将带大家深入了解 Tracy-Widom 统计量的概念、应用以及如何通过 twstats 程序进行计算。\n\n什么是 Tracy-Widom 统计量\nTracy-Widom 统计量来源于随机矩阵理论，它用于描述随机矩阵的特征值分布，尤其是最大特征值的行为。在大规模数据分析中，Tracy-Widom 分布成为评估主成分显著性的标准工具。\n具体来说，Tracy-Widom统计量用于检验 PCA 中主成分的显著性。我们前面讲过使用 Eigensoft 中的 smartPCA 进行 PCA 分析。PCA 通常会计算各主成分的方差，并根据这些方差来筛选重要的主成分。然而，如何判断哪些主成分是由于数据的真实结构而非随机噪声引起的呢？这正是Tracy-Widom统计量能够发挥作用的地方，Tracy-Widom 统计量可以帮助我们判断哪些主成分是统计显著的。\nTracy-Widom分布的核心思想是：如果我们从一个高维随机矩阵中提取主成分，那么最大主成分的值将服从特定的统计分布。通过计算 Tracy-Widom统计量，研究者可以对 PCA 中的每个主成分进行显著性检验，评估其是否真的反映了数据的结构，而非偶然性噪声。\n\n\ntwstats 程序与 Tracy-Widom 统计量\nEigensoft 中的 twstats 程序可以用来计算 Tracy-Widom 统计量。twstats 程序通过计算给定数据集的特征值分布，具体来说，twstats 程序会根据 Tracy-Widom 分布为每个主成分计算一个 p 值，从而帮助研究者判断哪些主成分是值得关注的。\ntwstats 适用于随机标记数据，但不适用于含有祖先信息标记（ancestry-informative markers）的数据。它假设数据集是由随机标记构成，这意味着在分析过程中不应该包含任何可能揭示祖先信息的标记（如遗传标记）。因为在包含祖先信息的标记数据中，可能会出现基因组混合 LD（连锁不平衡），这会违背Tracy-Widom统计量的基本假设。如果数据集含有祖先信息标记，Tracy-Widom 统计量的结果可能会不准确。\nEigensoft 中的 twstats 程序示例如下：\n#!/usr/bin/perl\n\n$command = \"../bin/twstats\";\n$command .= \" -t twtable \";\n$command .= \" -i twexample.eval \";\n$command .= \" -o twexample.out\";\nprint(\"$command\\n\");\nsystem(\"$command\");"
  },
  {
    "objectID": "blog/2025/01/02/awk/index.html",
    "href": "blog/2025/01/02/awk/index.html",
    "title": "探索 AWK",
    "section": "",
    "text": "在大数据分析和生物信息学领域，处理文本数据是一项常见的任务。awk 是一种功能强大的文本处理工具，它允许用户对文本文件进行灵活的模式匹配、过滤、计算和格式化。\n这里，我们将介绍 awk 的基本概念、常用命令及其在实际数据处理中的应用。\n\n什么是 AWK\nawk 是一种编程语言，用于处理和分析文本文件，尤其是在数据处理和报告生成方面非常有用。其名称来源于其三位创造者的姓氏首字母：Alfred Aho、Peter Weinberger 和 Brian Kernighan。\nawk 处理文本文件时，会将文件的每一行视为一个记录，每一行中的字段（由空格或制表符分隔）作为字段。通过指定模式和动作，awk 可以对文件内容进行筛选、处理并输出结果。\n\n\nAWK 的基本语法\nawk 命令的基本语法如下：\nawk [OPTION] 'CONDITION {PROCESS}' FILENAME\n在 AWK 中，有一些内建的变量非常实用： - $0 : 表示当前行的所有字段。 - $n : 表示第n列字段。比如 $1 表示第一列，$4 表示第四列。 - NR : 当前处理的行号（记录号）。 - NF : 当前行的字段数。\n例如，以下命令会打印出文件中所有的行：\nawk '{ print }' filename\n如果我们只想打印文件中第二列的数据，可以这样写：\nawk '{ print $2 }' filename\n其中，$2 代表第二列，$1 代表第一列，依此类推。\n\n\n常用 AWK 命令\n\n1. 打印特定字段\n打印某一列的内容是 awk 最常见的用途之一。选择染色体 2 上的变异（保留表头）：\nawk 'NR==1 || $1==2 {print $0}' sumstats.txt | head\nNR==1 如果是第一行（通常是表头），打印。$1==2 如果第一列是染色体号为 2 的变异，打印这一行。这段代码保留了表头，并筛选出染色体 2 上的所有变异。\n\n\n2. 使用条件语句筛选数据\n我们可以使用 if 语句来根据特定条件筛选数据。例如，选择全基因组显著的变异（p 值 &lt; 5e-8）：\nawk 'NR==1 || $13&lt;5e-8 {print $0}' sumstats.txt | head\n$13&lt;5e-8 选择第 13 列（P 值列）小于 5e-8 的行。这个命令用于筛选全基因组显著的变异数据。\n\n\n3. 计算和统计\nawk 还可以用于计算数据，比如求和、平均值等。假设我们有一个文件，每行代表一个学生的成绩，我们想计算总成绩：\nawk '{ sum += $2 } END { print sum }' grades.txt\n在这个例子中，$2 是成绩列，sum += $2 会累加第二列的所有成绩，END 是在所有数据处理完后执行的操作。\n\n\n4. 格式化输出\nawk 还支持格式化输出，类似于 C 语言中的 printf。例如，想要将数据格式化为固定宽度，可以这样做：\nawk '{ printf \"%-10s %-5s %-8s\\n\", $1, $2, $3 }' data.txt\n这会将每一行的三列数据按指定格式输出，其中 %s 表示字符串，-10 表示左对齐并占用 10 个字符宽度。\n\n\n5. 多文件处理\nawk 也可以同时处理多个文件，甚至将不同文件的内容组合起来。例如：\nawk '{ print FILENAME \": \" $0 }' file1.txt file2.txt\n这里，FILENAME 是 awk 内置的变量，表示当前处理的文件名。$0 表示当前行的内容（即整行数据）。print FILENAME \": \" $0 这部分的作用是打印文件名后跟一个冒号和当前行的内容。\n\n\n\nAWK 在生物信息学中的应用\n在生物信息学数据分析中，awk 被广泛应用于处理基因组学数据、GWAS 数据、RNA-Seq 数据等。这些数据通常包含数百万行，我们可以使用 awk 来快速筛选、过滤、提取特定信息。例如，以下是一些常见的应用场景：\n\n1. 从 VCF 文件中提取信息\nVCF（Variant Call Format）文件是基因组变异的标准存储格式，通常包含变异位点的各种信息。如果我们想提取 VCF 文件中所有发生变异的基因，可以使用以下命令：\nawk '$1 !~ /^#/ { print $1, $2, $4, $5 }' variants.vcf\n这条命令会跳过以 # 开头的注释行，输出变异位点的染色体位置、参考碱基和变异碱基。\n\n\n2. 清洗 RNA-Seq 数据\nRNA-Seq 数据的处理通常需要将表达量数据中的低表达基因或特定条件下的数据进行筛选。比如，删除表达量小于某一阈值的基因：\nawk '$2 &gt; 10 { print $1, $2 }' gene_expression.txt\n这里，$2 &gt; 10 表示筛选出表达量大于 10 的基因。"
  },
  {
    "objectID": "blog/2024/12/28/hail/index.html",
    "href": "blog/2024/12/28/hail/index.html",
    "title": "掌握 Hail",
    "section": "",
    "text": "在生物信息学领域，处理大规模基因组数据尤其是 VCF（Variant Call Format）格式的数据常常是一个挑战。传统的处理方法可能面临性能瓶颈，而 Hail 作为一个专为大规模基因组数据分析设计的工具，其高效性和可扩展性使其在基因组学界得到了广泛应用。\n这里，我们将介绍 Hail 的基本使用技巧，并探讨如何在 DNAnexus 平台上导入和处理 pVCF 数据。\n\n什么是Hail？\nHail 是一个开源的、专为基因组数据分析而设计的 Python 库，特别适合用于处理大规模的 VCF 文件。它在处理和分析基因组数据时，提供了比传统工具（如 GATK）更高效的性能，尤其在进行大规模 GWAS（基因组全关联研究）、变异注释和基因型分析时，它能够显著提高计算效率。\nHail 的核心优势在于其对分布式计算的支持，能够利用 Spark 集群进行大数据的并行处理。此外，Hail 也与很多常见的生物信息学工具兼容，比如 VCF 工具、Plink 等，使得它能够轻松集成到现有的生物信息学分析管道中。\n\n\nHail 在 DNAnexus 平台上的应用\n在 DNAnexus 平台上，我们可以直接使用 Hail 进行大规模基因组数据分析。平台提供了一个强大的计算环境，可以通过 Hail 轻松地读取、处理和分析 VCF 文件。以下是一些常见的应用场景和技巧：\n\n1. 使用Hail加载和处理VCF文件\n在 DNAnexus 平台上，VCF 文件通常存储在项目中，用户可以直接加载到 Hail 中进行处理。Hail 提供了简便的 API 来读取 VCF 文件，下面是一个简单的示例，展示如何在 Hail 中加载 VCF 文件：\n\nimport hail as hl\n\n# 加载VCF文件\nvcf_file = 'gs://my_bucket/my_data.vcf.bgz'\nmt = hl.import_vcf(vcf_file)\n\n在这个示例中，我们使用 hl.import_vcf() 方法来加载 VCF 文件，gs:// 是 Google Cloud Storage（GCS）路径格式，在 DNAnexus 平台上，我们也可以使用相应的路径来引用存储在平台上的 VCF 文件。\n\n\n2. 使用 Hail 进行数据质量控制\n在基因组数据分析中，数据质量控制（QC）是不可或缺的一部分，Hail 为此提供了多种功能，例如去除低质量的样本或变异、过滤变异的深度或基因型质量等。\n以下是一个常见的 QC 操作示例，过滤掉低质量的变异：\n\n# 过滤低质量变异\nmt = mt.filter_rows(mt.qual &gt; 30)\n\n通过这样的操作，我们可以去除低质量的变异，保证后续分析结果的准确性。\n\n\n3. 基于 Hail 进行群体遗传学分析\nHail 还广泛应用于群体遗传学分析，例如计算群体间的变异频率、构建群体的基因型矩阵、进行 GWAS 分析等。Hail 的高效数据处理能力使其能够轻松处理海量数据，并进行并行计算，极大地提升了数据分析的效率。\n\n\n\npVCF 数据导入教程\n在基因组学中，VCF 文件是常见的变异数据格式，而 pVCF（partitioned VCF）格式则在处理极大规模数据时显得尤为重要。pVCF 将数据分割成多个小文件，避免了单个文件过大导致的内存问题，并使得数据处理更加灵活高效。\n在 DNAnexus 平台上，我们可以利用 Hail 来导入 pVCF 数据，具体步骤如下：\n\n1. 启动 Hail 应用\n在 DNAnexus 平台上，首先需要启动一个 Hail 集群环境。我们可以使用 dx 命令行工具或通过平台的应用界面启动 Hail 应用。\n\n\n2. 加载 pVCF 文件\n使用 Hail 的 import_vcf 方法加载 pVCF 文件。需要注意的是，pVCF 文件通常由多个分区组成，因此在导入时需要确保指定正确的路径。\n\nimport hail as hl\n\n# 导入pVCF文件\nmt = hl.import_vcf('gs://my_bucket/my_pvcf_file.part*')\n\n\n\n3. 数据分析与处理\n加载 pVCF 数据后，我们可以利用 Hail 提供的各种功能对数据进行分析。常见的分析操作包括数据过滤、变异注释和群体遗传学分析等。\n\n小技巧：在 DNAnexus 平台上，我们可以利用分布式计算资源加速分析过程。Hail 与 Apache Spark 紧密集成，可以在集群上并行处理数据，从而大大缩短分析时间。"
  },
  {
    "objectID": "blog/2024/12/25/pca/index.html",
    "href": "blog/2024/12/25/pca/index.html",
    "title": "GWAS 前 PCA 步骤详解",
    "section": "",
    "text": "基因组广泛关联研究（GWAS）旨在探索遗传变异与表型特征之间的关系，但由于群体结构（即不同人群间的遗传差异）和样本亲缘关系的影响，可能会导致假阳性或假阴性结果。为了控制这些偏差，主成分分析（PCA）成为了 GWAS 前的重要步骤。\n这里，我们将详细介绍 GWAS 前 PCA 的原因以及如何通过一系列步骤有效进行 PCA 分析。\n\n为什么要进行 PCA？\n\n1. 去除群体结构的影响\n在多种族或多地区样本的 GWAS 中，样本的群体结构可能会影响分析结果。例如，不同的群体可能拥有不同的基因频率，这种结构性差异如果不加以控制，可能会误导结果，导致某些表型与基因变异之间的假相关。\n\n\n2. 去除亲缘关系的干扰\n如果样本中存在亲缘关系（如父母-子女、兄弟姐妹等），这些亲缘关系会增加样本间的相关性，从而影响 GWAS 的准确性。PCA 能够帮助识别并去除这些干扰，确保样本的独立性。\n\n\n3. 降低计算复杂度\nPCA 能够通过减少数据的维度来降低后续分析的计算复杂度，并帮助更清晰地理解数据的结构。\n\n\n\nPCA 的步骤\n进行 PCA 时，主要分为以下几个关键步骤：\n\n1. LD剪枝（LD-Pruning）\n在进行 PCA 前，我们通常会进行 LD 剪枝，即去除那些高度相关（连锁不平衡，LD）变异位点。这样做的目的是减少冗余信息，使得 PCA 能够更准确地反映独立的遗传变异。\n在 PLINK 中，可以使用以下命令进行 LD 剪枝：\nplink2 --bfile ${genotypeFile} \\\n       --indep-pairwise 50 5 0.2 \\\n       --out ${outPrefix}.prune\n--indep-pairwise 50 5 0.2 进行 LD 剪枝，窗口大小为 50 个 SNP，步长为 5，LD 阈值为 0.2，表示去除那些与其他 SNP 高度相关的 SNP。--out 指定输出文件的前缀。这将生成一个包含独立 SNP 的文件 ${outPrefix}.prune.in，后续将用于 PCA 计算。\n\n\n2. 去除亲缘关系样本\nPCA 计算时需要去除亲缘关系较近的样本，通常是 2 度以内的亲属。PLINK 的 --king-cutoff 命令可以用来筛选样本，并去除与其他样本亲缘关系过近的样本。\nplink2 --bfile ${genotypeFile} \\\n       --king-cutoff 0.0884 \\\n       --out ${outPrefix}.king.cutoff\n--king-cutoff 0.0884 此命令通过阈值 0.0884（约对应亲缘关系为 2 度的样本）去除亲缘关系过近的样本。这会生成两个文件：plink_results_king.king.cutoff.in.id（保留的样本 ID）和 plink_results_king.king.cutoff.out.id（被排除的样本 ID）。\n\n\n3. 使用无亲缘关系样本和独立 SNP 进行 PCA\n接下来，使用去除亲缘关系的样本和独立 SNP 来运行 PCA。PCA 计算的目的是识别样本间最显著的遗传变异。我们可以使用 PLINK 中的 --pca 命令来进行 PCA 计算。\nplink2 --bfile ${genotypeFile} \\\n       --keep ${outPrefix}.king.cutoff.in.id \\\n       --extract ${outPrefix}.prune.in \\\n       --freq counts \\\n       --threads ${threads} \\\n       --pca approx allele-wts 10 \\\n       --out ${outPrefix}.pca\n--keep ${outPrefix}.king.cutoff.in.id 指定只使用无亲缘关系的样本，--extract ${outPrefix}.prune.in 指定仅使用经过 LD 剪枝的独立 SNP。--freq counts 计算等位基因频率，--pca approx allele-wts 10 请求进行 PCA 计算，并输出前 10 个主成分的等位基因权重。此命令会生成多个输出文件，包括主成分得分文件（.eigenvec）、主成分方差解释比例文件（.eigenval）等。\n\n\n4. 将 PCA 结果投影到所有样本\n完成 PCA 后，我们可以将主成分的得分投影到所有样本中。这样可以确保即使是在 PCA 分析后没有被直接计算的样本，也能够获得与前几个主成分的关联。\nplink2 --bfile ${genotypeFile} \\\n       --threads ${threads} \\\n       --read-freq ${outPrefix}.acount \\\n       --score ${outPrefix}.eigenvec.allele 2 6 header-read no-mean-imputation variance-standardize \\\n       --score-col-nums 7-16 \\\n       --out ${outPrefix}_projected\n--read-freq ${outPrefix}.acount 读取计算过的等位基因频率，--score ${outPrefix}.eigenvec.allele 2 6 header-read no-mean-imputation variance-standardize 使用主成分分析的结果对所有样本进行投影，--score-col-nums 7-16 指定投影的主成分列。投影后的结果可以用于进一步的 GWAS 分析，确保将群体结构和亲缘关系的影响考虑在内。\n\n\n5. 分析和解释 PCA 结果\n完成 PCA 分析后，我们可以查看每个主成分的解释比例，了解各个主成分对于遗传变异的贡献。通常，前几个主成分会解释大部分的方差，因此我们关注的是这些主成分的贡献。\n在 PLINK 的输出文件中，.eigenval 文件包含每个主成分的特征值，这些特征值表示该主成分对于数据方差的贡献比例。通过查看这些值，我们可以判断哪些主成分最能解释数据中的变异。\n\n\n\n总结\nPCA 是 GWAS 分析中的一个关键步骤，能够有效去除群体结构和亲缘关系的影响，从而提高 GWAS 结果的可靠性和准确性。通过 PCA，我们不仅能去除数据中的噪声，还能更好地理解样本之间的遗传结构，为 GWAS 的成功开展奠定坚实基础。"
  },
  {
    "objectID": "blog/2024/12/23/plink_command/index.html",
    "href": "blog/2024/12/23/plink_command/index.html",
    "title": "PLINK 常用命令介绍",
    "section": "",
    "text": "PLINK 是一个广泛使用的基因组学分析工具，尤其适用于大规模遗传数据分析。无论是进行全基因组关联研究（GWAS），还是进行简单的基因型数据质量控制，PLINK 都是研究人员的重要工具之一。这里，我们将介绍 PLINK 中一些常用的命令，帮助大家更高效地处理和分析基因型数据。\n\n1. 基本文件操作命令\n\n–make-bed\n将现有的文本格式数据（如 .ped 和 .map 文件）转换为二进制格式（.bed, .bim, .fam 文件），以便提高数据处理速度。\nplink --ped input.ped --map input.map --make-bed --out output\n这条命令将 input.ped 和 input.map 文件转换为二进制格式，并保存为 output.bed、output.bim 和 output.fam 文件。\n\n\n–bfile\n在进行 PLINK 命令时，可以通过指定 .bed、.bim 和 .fam 文件前缀来加载二进制格式的数据文件。\nplink --bfile mydata --freq\n该命令加载 mydata.bed、mydata.bim 和 mydata.fam 文件，并计算 SNP 频率。\n\n\n\n2. 数据过滤命令\n\n–keep 和 –remove\n--keep 用于保留指定样本，--remove 用于排除指定样本。这两个命令接受一个包含样本 ID 的文件。\nplink --bfile mydata --keep keep_list.txt --make-bed --out filtered_data\n这个命令将 keep_list.txt 文件中的样本保留在数据集中，并生成新的二进制文件 filtered_data.bed。\n\n\n–extract 和 –exclude\n--extract 用于选择指定的 SNPs（通过 .txt 文件列出），而 --exclude 用于排除某些 SNPs。\nplink --bfile mydata --extract snp_list.txt --make-bed --out selected_snps\n此命令将 snp_list.txt 文件中的 SNPs 提取出来，并保存为新的二进制文件 selected_snps.bed。\n\n\n–maf 和 –geno\n--maf 用于设置最小等位基因频率（Minor Allele Frequency），--geno 用于排除缺失率较高的 SNP。\nplink --bfile mydata --maf 0.01 --geno 0.05 --make-bed --out filtered_data\n这个命令将过滤掉 MAF 小于 1% 或缺失率大于 5%的 SNP。\n\n\n\n3. 关联分析命令\n\n–assoc\n进行简单的关联分析，计算每个 SNP 和表型之间的关联性。\nplink --bfile mydata --assoc --out association_results\n该命令将进行每个 SNP 和表型的关联分析，并将结果保存为 association_results.assoc 文件。\n\n\n–linear\n进行线性回归分析，适用于连续表型。\nplink --bfile mydata --linear --out linear_results\n此命令进行线性回归分析，输出与表型相关的 SNP 及其统计信息。\n\n\n–logistic\n进行 logistic 回归分析，适用于二分类表型（如病例对照研究）。\nplink --bfile mydata --logistic --out logistic_results\n此命令进行 logistic 回归分析，输出 SNP 与表型的关系。\n\n\n\n4. 质量控制命令\n\n–check-sex\n用于检查样本的性别是否与遗传数据一致。\nplink --bfile mydata --check-sex --out sex_check\n该命令将检查数据集中所有样本的性别，并生成 sex_check.sexcheck 文件。\n\n\n–missing\n用于计算样本和 SNP 的缺失情况，帮助识别潜在的质量问题。\nplink --bfile mydata --missing --out missing_data\n这个命令将生成一个包含缺失数据统计的文件 missing_data.lmiss 和 missing_data.imiss。\n\n\n\n5. 文件合并命令\n\n–merge 和 –bmerge\n--merge 合并多个 PLINK 数据集，--bmerge 与 --merge 类似，但适用于二进制文件。\nplink --bfile data1 --bmerge data2.bed data2.bim data2.fam --make-bed --out merged_data\n这个命令将二进制文件 data1 和 data2 合并，并保存为新的 merged_data.bed 文件。\n\n\n\n6. 数据导出命令\n\n–recode\n将数据从二进制格式转换为其他格式，例如 .ped 格式。\nplink --bfile mydata --recode --out ped_format\n该命令将 mydata 数据集转换为 .ped 和 .map 文件格式，并保存为 ped_format.ped 和 ped_format.map。\n\n\n–recode vcf\n将 PLINK 二进制数据转换为 VCF 格式。\nplink --bfile mydata --recode vcf --out vcf_output\n此命令将 mydata 数据集转换为 .vcf 格式，并输出为 vcf_output.vcf 文件。\n\n\n\n7. 并行化命令\n\n–threads\n指定 PLINK 使用的线程数，从而加速计算。\nplink --bfile mydata --assoc --threads 4 --out assoc_results\n该命令将在进行关联分析时使用 4 个线程，以加速处理。"
  },
  {
    "objectID": "blog/2024/12/20/maf/index.html",
    "href": "blog/2024/12/20/maf/index.html",
    "title": "从基因到疾病：等位基因频率与效应大小的关系",
    "section": "",
    "text": "基因变异如何影响疾病风险？在遗传学研究中，等位基因频率（Minor Allele Frequency, MAF） 和 效应大小（Effect Size, OR, Odds Ratio） 是两个关键参数，用于衡量基因变异的稀有程度及其对疾病风险的影响。这里，我们试着用简单直观的方式，带大家了解等位基因频率和效应大小的关系，以及它们在不同疾病中的作用。\n\n等位基因频率与效应大小是什么？\n等位基因频率（MAF）是指某个变异（如 SNP，单核苷酸多态性）在群体中较少出现的等位基因的比例。根据频率，变异可以分为以下几类：\n\n非常罕见变异（Very Rare）：MAF &lt; 0.001\n罕见变异（Rare）：0.001 ≤ MAF &lt; 0.01\n低频变异（Low-frequency）：0.01 ≤ MAF &lt; 0.05\n常见变异（Common）：MAF ≥ 0.05\n\n而效应大小通过优势比（OR, Odds Ratio） 衡量某个变异对疾病风险的影响程度：\n\nOR &gt; 3：高效应大小，意味着这个变异对疾病的影响很强。\nOR接近1：低效应大小，表示影响很小。\n\n\n\n等位基因频率与效应大小的关系\n\n高效应大小且非常罕见变异：孟德尔遗传病\n\n特点：极少数人携带的变异直接导致疾病发生。\n例子：囊性纤维化（CFTR 基因突变）、镰状细胞贫血（HBB 基因突变）。\n应用：这种高效应变异易于通过家系研究或全外显子组测序发现。\n\n\n\n低频变异，效应大小中等：复杂疾病中的关键位点\n\n特点：变异频率较低，但对特定人群的疾病风险有显著影响。\n例子：一些罕见心血管疾病或代谢性疾病的相关位点。\n挑战：需要更大样本量和更高精度的分析工具来发现。\n\n\n\n常见变异，低效应大小：复杂疾病中常见变异\n\n特点：频率较高，单个变异对疾病的影响微弱，但多个变异可能协同作用。\n例子：2 型糖尿病、肥胖、抑郁症等多基因疾病。\n应用：通过全基因组关联研究（GWAS）和多基因风险评分（PRS）进行疾病预测。\n\n\n\n罕见变异但效应较低：难以检测的遗传因素\n\n特点：这种变异影响微弱且在人群中极其罕见，目前的技术难以发现其对疾病的潜在作用。\n挑战：需要结合更大规模数据和功能实验加以探索。\n\n\n\n\n为什么这对研究很重要？\n\n揭示疾病的遗传基础\n理解等位基因频率和效应大小的关系，有助于解析不同变异在疾病发生中的作用。对于孟德尔遗传病，关注高效应大小的罕见变异；而对于复杂疾病，更多地关注常见变异和低频变异的累积效应。\n\n\n优化疾病预测与治疗\n高效应变异可以作为明确的诊断标志物，用于早期筛查和治疗设计。低效应变异为复杂疾病的风险预测提供信息，通过多基因风险预测模型提高预测准确性，能够为个性化医疗和疾病干预策略提供新思路，从而推动精准医学发展。"
  },
  {
    "objectID": "blog/2024/12/16/perceptron/index.html",
    "href": "blog/2024/12/16/perceptron/index.html",
    "title": "感知机：人工神经网络的起点",
    "section": "",
    "text": "感知机的诞生：人工神经网络的起点\n1958年，弗兰克·罗森布拉特（Frank Rosenblatt）在康奈尔大学航空实验室提出了感知机，这是第一个模拟人脑神经元功能的算法模型。感知机的设计灵感来自人脑的神经元连接方式，目标是通过数学模型模仿人类学习的过程。\n\n核心思想\n感知机的基本功能是进行线性分类。它通过将输入值加权求和后与阈值比较，来决定输出属于哪个类别。简单来说，感知机的输出是一个二元值（如 0 或 1 ），用来表示输入数据所属的类别。\n\n\n数学模型\n感知机基于以下公式：\n\\[\ny = f\\left(\\sum_{i=1}^{n} w_i x_i + b\\right)\n\\]\n\n\n\n感知机的局限性：XOR 问题的挑战\n尽管感知机在某些任务中表现良好，但其能力受到线性可分性限制。1969年，著名的《Perceptrons》一书由马文·明斯基（Marvin Minsky）和西摩·帕珀特（Seymour Papert）撰写，他们指出感知机无法解决非线性可分的问题，例如XOR问题。感知机只能通过一条直线分割两个类别的数据点。当数据分布无法用一条直线区分（如 XOR 问题），感知机便无能为力。这一挑战让研究者认识到，单层感知机无法胜任复杂任务，需要更复杂的多层结构来解决。\n\nclass Perceptron:\n    def __init__(self, input_size, learning_rate=0.01, epochs=1000):\n        # Initialize weights to zeros, with one extra weight for the bias\n        self.weights = np.zeros(input_size + 1)  # +1 for the bias\n        self.learning_rate = learning_rate  # Learning rate controls weight updates\n        self.epochs = epochs  # Number of iterations over the training data\n\n    def activation_function(self, x):\n        # Apply step function: return 1 if x &gt;= 0, else return 0\n        return 1 if x &gt;= 0 else 0\n\n    def predict(self, x):\n        # Compute the weighted sum (dot product), including bias\n        z = self.weights.T.dot(np.insert(x, 0, 1))  # Insert bias term into input\n        return self.activation_function(z)\n\n    def train(self, X, y):\n        for _ in range(self.epochs):\n            for xi, target in zip(X, y):\n                prediction = self.predict(xi)\n                error = target - prediction\n                update = self.learning_rate * error\n                \n                self.weights[1:] += update * xi \n                self.weights[0] += update\n\n# Training data (XOR logic)\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny = np.array([0, 1, 1, 0])\n\n\n\n多层感知机（MLP）的出现：突破局限\n为了克服单层感知机的局限性，研究者提出了多层感知机（Multi-Layer Perceptron, MLP）。它通过引入隐藏层和非线性激活函数（如 sigmoid 函数），实现了对复杂数据的学习能力。1986 年，David Rumelhart等人提出了反向传播（Backpropagation）算法，这使得训练多层感知机变得可行。该算法通过梯度下降更新权重，优化了模型性能。此外，隐藏层中的非线性激活函数打破了单层感知机的线性限制，使模型能够处理复杂的非线性关系。\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nclass MLPerceptron:\n    def __init__(self, input_size, learning_rate=0.05, epochs=2000):\n        self.input_size = input_size\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.model = Sequential()\n        self.model.add(Input((input_size,)))  \n        self.model.add(Dense(2, activation=\"tanh\", ))  \n        self.model.add(Dense(1, activation=\"sigmoid\")) \n        self.model.compile(\n            loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n        )\n\n    def train(self, X, y):\n        self.model.fit(X, y, epochs=self.epochs, verbose=0)\n\n    def predict(self, x):\n        prediction = self.model.predict(np.array([x]))\n        return 1 if prediction &gt;= 0.5 else 0  \n\n\n\n感知机的现代继承者\n虽然感知机本身不再是当前人工智能的核心技术，但它为后续技术的发展奠定了重要基础。现代深度学习模型（如卷积神经网络、循环神经网络）可以被视为感知机的扩展和进化。多层感知机推动了深度神经网络的研究热潮，尤其是在 2010 年代 GPU 计算力提升后，深度学习应用于图像识别、自然语言处理等领域。感知机的提出使得研究者开始探索机器如何学习这一基本问题，也激发了人工智能领域的快速发展。\n\n\n感知机的意义与启示\n感知机的历史体现了技术发展的非线性特性：早期的技术突破可能因为局限性而受到批评，但它们却是后续更复杂技术的基石。感知机虽然简单，但它的概念推动了我们对人类学习过程和计算机模拟的理解。虽然感知机的局限曾导致人工智能研究进入低谷期（AI冬天），但最终多层感知机证明了它的价值。"
  },
  {
    "objectID": "blog/2024/12/04/install_r_on_ubuntu/index.html",
    "href": "blog/2024/12/04/install_r_on_ubuntu/index.html",
    "title": "Ubuntu 20.04 上安装 R",
    "section": "",
    "text": "如果你是 Ubuntu 用户，并且打算安装 R 语言环境来进行数据分析、统计计算或者数据科学工作，本文将为你提供一个详细的安装步骤。\n从下载源代码到配置必要的依赖项，我们将一步步带你完成整个过程。\n\n1. 更新系统包管理器\n首先，我们需要确保系统包管理器是最新的。运行以下命令来更新 APT 源，并执行系统的全面升级。\nsudo apt update\nsudo apt full-upgrade\nsudo apt autoremove\n这三条命令将会更新你的软件包列表，升级现有软件包，并移除不再需要的包，保持系统整洁。\n\n\n2. 下载 R 源代码\n由于 R 的最新版本可以通过源代码安装，这里我们选择手动下载 R 4.4.2 版本的源代码包，并解压。\nwget https://cran.r-project.org/src/base/R-4/R-4.4.2.tar.gz\ntar -xvzf R-4.4.2.tar.gz\n解压完成后，你会看到一个名为 R-4.4.2 的文件夹。\n\n\n3. 安装必要的依赖项\nR 的编译需要一些库和工具，所以需要先安装依赖项。运行以下命令来安装缺失的库：\nsudo apt install -y libx11-dev libxext-dev libxmu-dev libxt-dev\nsudo apt install -y libdeflate-dev texinfo texlive-base texlive-fonts-recommended texlive-fonts-extra texlive-latex-base texlive-latex-extra latexmk libcurl4-openssl-dev libxml2-dev libxt-dev pandoc ghostscript\n\n\n4. 编译 R\n在安装完所有依赖项之后，我们可以开始编译 R。首先创建一个目录来存放 R 的安装文件：\nmkdir -p ~/software/r/r-4.4.2\n然后进入到 R 的源代码目录，使用 ./configure 命令来配置编译选项，然后使用 make 命令开始编译：\ncd R-4.4.2/\n./configure --prefix=$HOME/software/r/r-4.4.2\nmake\n这一步可能需要一些时间，具体时间取决于你的机器配置。\n\n\n5. 安装 Java 环境（可选）\n如果你需要在 R 中运行与 Java 相关的功能，比如使用 rJava 包，可能需要安装 Java。可以运行以下命令来检查 Java 版本并安装 OpenJDK 11：\njava -version\nsudo apt install openjdk-11-jre-headless\n然后设置 Java 的环境变量：\necho $JAVA_HOME\nsudo update-alternatives --config java\n编辑 ~/.bashrc 文件，添加 JAVA_HOME 变量的配置：\nvi ~/.bashrc\n# 在文件末尾添加如下内容：\nexport JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64 #具体路径根据你的安装路径而定\nexport PATH=$JAVA_HOME/bin:$PATH\n保存并退出后，使用以下命令使更改生效：\nsource ~/.bashrc\n\n\n6. 完成 R 安装\n在完成所有必要的配置后，可以将 R 安装到指定目录。然后，进入到 R-4.4.2 的文件夹中，运行以下命令来安装 R：\nsudo make install\n\n\n7. 验证安装\n安装完成后，可以通过运行以下命令来验证 R 是否安装成功：\nR --version\n如果返回的是 R 的版本信息，那么说明安装已经成功。\n\n\n\nR 安装成功"
  },
  {
    "objectID": "blog/2024/12/02/SAIGE_GWAS/index.html",
    "href": "blog/2024/12/02/SAIGE_GWAS/index.html",
    "title": "使用 SAIGE 进行 GWAS 分析的详细流程",
    "section": "",
    "text": "本文介绍如何基于 UK Biobank 数据和 DNAnexus 平台，使用 SAIGE 工具在大规模基因组关联研究（GWAS）中分析遗传变异与性状之间的关联。SAIGE 的简要介绍参考这里。\n\n流程概览\nSAIGE GWAS 分析分为以下步骤：\n\n准备数据：合并自测基因型数据\n生成 GRM 模型和方差比文件\n执行单变异关联测试\n（可选）合并结果文件\n\n\n\nStep 1: 合并自测基因型数据\n在第一步中，我们将 22 条常染色体的基因型数据文件合并为 PLINK 格式的文件集（.bim, .bed, .fam），为后续分析生成输入文件。\n\n使用 Swiss Army Knife (SAK) 应用\n在 DNAnexus 平台上，使用 Swiss Army Knife (SAK) 应用完成文件合并：\n\n输入文件：上传或选择包含染色体 1 至 22 的 PLINK 文件。\n命令行代码：\n\nls *.bed | sed -e 's/.bed//g'&gt; files_to_merge.txt;\nplink --merge-list files_to_merge.txt --make-bed --out ukb_cal_chr1_22_v2_merged;\nrm files_to_merge.txt;\n这段代码会创建一个包含所有待合并文件的列表，并使用 PLINK 将其合并为单一文件集。\n\n输出：ukb_cal_chr1_22_v2_merged.bed、.bim 和 .fam 文件，作为下一步的输入。\n\n\n\n\nStep 2: 生成 GRM 模型和方差比文件\n使用 saige_gwas_grm 应用生成遗传相关矩阵（GRM）模型文件和方差比文件。这些文件用于混合模型的拟合和关联测试。\n\n输入文件\n\nPLINK 文件集：ukb_cal_chr1_22_v2_merged 文件。\n表型文件：包含样本 ID、表型和协变量（如性别、年龄）的文件，要求与基因型数据匹配。\n\n\n\n命令实例\ndx run saige_gwas_grm \\\n  -igenotype_file=ukb_cal_chr1_22_v2_merged.bed \\\n  -iphenotype_file=phenotype.txt \\\n  -icovariate_columns=age,sex \\\n  -imem_instance=mem3_ssd1_v2_x32\n\n\n\nStep 3: 执行单变异关联测试\n单变异关联测试使用 saige_gwas_svat 应用逐染色体进行分析，计算每个变异与表型之间的关联。\n\n批量运行\n对于 UK Biobank 数据，每条染色体的数据存储在单独的 BGEN 文件中，可以使用批量模式运行：\n\n选择 BGEN 文件和索引文件：每条染色体的 .bgen 和 .bgen.bgi 文件。\n运行命令：\ndx run saige_gwas_svat \\\n  -igenotypes_bgen=chr*.bgen \\\n  -igenotypes_bgen_index=chr*.bgen.bgi \\\n  -imodel_rda=model.rda \\\n  -ivariance_ratio_txt=variance_ratio.txt \\\n  -iphenotype_file=phenotype.txt \\\n  -imem_instance=mem3_ssd3_v2_x24\n\n\n\n输出文件\n\n每条染色体的关联结果文件，例如：saige_step2_ukb_imp_chr1_v3.txt。\n\n\n\n\nStep 4: 合并结果文件\n可选地将所有染色体的结果文件合并为单一文件，以便进一步分析（如显著性筛选和孟德尔随机化分析）。\n\n使用 SAK 应用\n在 SAK 的命令行中执行以下代码：\nhead -1 saige_step2_ukb_imp_chr1_v3.txt &gt; saige_step2_ukb_imp_all_chr.txt;\ntail -n +2 -q saige_step2_ukb_imp_chr*_v3.txt &gt;&gt; saige_step2_ukb_imp_all_chr.txt;\ngzip saige_step2_ukb_imp_all_chr.txt;\n\n\n\n后续分析\n\n显著性筛选：根据 GWAS 的统计学标准筛选显著变异（如 p 值 &lt;5×10−8）。\n下游分析：\n\n\n使用显著变异进行孟德尔随机化分析。\n进行生物学功能注释，识别潜在的致病基因或路径。"
  },
  {
    "objectID": "blog/2024/11/28/SAIGE/index.html",
    "href": "blog/2024/11/28/SAIGE/index.html",
    "title": "SAIGE",
    "section": "",
    "text": "SAIGE\nSAIGE（Scalable and Accurate Implementation of GEneralized mixed models）是一个用于大规模基因组关联研究（GWAS）的 R 包，旨在通过广义混合模型（Generalized Mixed Models, GMM）处理样本之间的相关性，并对大规模遗传数据进行高效分析。SAIGE 具有处理大量样本、考虑遗传关系、避免样本相关性干扰等优点，是解析全基因组数据的理想工具。\n本文将介绍 SAIGE 的主要功能、使用场景及其工作原理。\n\n1. SAIGE 的主要特点\n\n1.1 高效的计算方法\nSAIGE 的核心优势之一是其高效的计算方法。它能够处理大规模数据集，尤其适用于大约几十万到上百万的样本。SAIGE 通过采用广义混合模型（GMM）来估计样本之间的遗传相关性，从而准确估计每个基因位点的效应大小，并进行全基因组关联测试。\n\n\n1.2 处理样本相关性\n在 GWAS 中，样本之间的相关性（例如家族结构或人口结构）可能会导致结果的偏差。SAIGE 通过基因型关系矩阵（GRM）来处理样本的相关性，使得 GWAS 结果更加可靠。无论是亲缘关系还是其他类型的样本相关性，SAIGE 都能够有效地调整，避免不必要的假阳性结果。\n\n\n1.3 支持多种表型类型\nSAIGE 不仅支持定量性状（例如身高、体重等），还支持二元性状（例如病例对照数据）。它能够处理不同类型的表型，并针对不同表型类型应用相应的统计方法，例如，Firth 的偏差减少逻辑回归（Firth’s Bias-Reduced Logistic Regression）来分析稀有变异。\n\n\n1.4 稀有变异分析的扩展功能（SAIGE-GENE+）\nSAIGE-GENE+ 是 SAIGE 的扩展，用于稀有变异的集成分析。它提供了多种常用的稀有变异测试方法，包括 BURDEN 测试、SKAT 测试和 SKAT-O 测试，帮助识别变异集与表型的关系。\n\n\n1.5 高度可定制化\nSAIGE 允许用户根据实际需求调整参数，例如显著性阈值、遗传关系矩阵类型（完整或稀疏）等。此外，它还支持条件分析，可以在现有 GWAS 信号的基础上进行进一步的变异筛选。\n\n\n\n2. SAIGE 的工作原理\n\n\n\nSAIGE 工作流程\n\n\nSAIGE 使用广义混合模型（GMM）来估计样本之间的相关性，并对每个基因位点进行全基因组关联分析。其核心思想是通过计算每对样本之间的遗传关系矩阵（GRM），从而调整样本间的相关性，避免这些关系对关联分析结果的干扰。\n在分析过程中，SAIGE 会先使用 GRM 计算出样本间的相关性，并根据遗传相关性调整表型和基因型数据。接下来，使用 REML（限制最大似然法）或其他拟合方法进行模型估计，最后通过统计检验得到每个 SNP 与表型之间的关联。\n\n2.1 计算遗传关系矩阵（GRM）\n遗传关系矩阵（GRM）用于量化样本之间的基因型相似性。SAIGE 支持两种类型的 GRM：完整 GRM 和 稀疏 GRM。对于大规模数据集，稀疏 GRM 可以显著减少计算成本。我们可以使用 PLINK 等工具计算 GRM 文件，并将其作为输入提供给 SAIGE。\n\n\n2.2 模型拟合与关联分析\nSAIGE 使用 限制最大似然法（REML） 或其他拟合方法来估计基因型和表型之间的关系。对于稀有变异，SAIGE 还提供了 Firth 偏差减少逻辑回归，提高了小样本数据和稀有变异分析的准确性。"
  },
  {
    "objectID": "blog/2024/11/22/cumulative_hazard/index.html",
    "href": "blog/2024/11/22/cumulative_hazard/index.html",
    "title": "累积风险图（Cumulative Hazard Plot）中的 Y 轴含义",
    "section": "",
    "text": "问题\n星球里有同学提问：\n\n\n\ncumulative hazard plot\n\n\n\n累积风险函数\n在时间事件数据中，累积风险图（cumulative hazard plot）的 Y 轴表示的是累积风险函数（cumulative hazard function, \\(H(t)\\)）。\\(H(t)\\) 描述的是从起始时间到 \\(t\\) 时刻，某个事件发生的累积风险总和。它是通过对瞬时风险函数（即危险率，hazard rate, \\(h(t)\\)）在时间上的积分得到的，公式为 (Allison 2010)：\n\\[\nH(t) = \\int_{0}^{t} h(u) \\, du\n\\]\n其中，\\(h(t)\\) 是已生存到 \\(t\\) 时刻的对象在 \\(t\\) 时刻发生事件的速率。\\(H(t)\\) 没有上限值，它随着时间 \\(t\\) 的增加而单调递增。\n\n\n累积风险与 HR（风险比）的关系\nHR 是比较两组（如治疗组与对照组）瞬时风险率 \\(h(t)\\) 的比值：\n\\[\nHR = \\frac{h_1(t)}{h_2(t)}\n\\]\n它是一个相对指标，表示某组的风险率是另一组的多少倍。对 HR 的理解，可以类比 OR 的概念，只是 HR 是在时间上的比值，而 OR 无时间信息。如果时间对事件的影响较小（比如事件发生概率很低或者随访时间很短），HR 和 OR 的数值可能接近。但在事件发生率较高时，OR 通常会高于 HR，因为 OR 夸大了实际的效应。\n前面的累积风险函数 \\(H(t)\\) 与瞬时风险率 \\(h(t)\\) 紧密相关，若 HR 在所有时间上恒定（即比例风险假设成立），则累积风险函数之间的关系为：\n\\[\nH_1(t) = HR \\cdot H_2(t)\n\\]\n这意味着在比例风险假设成立的情况下，累积风险图中两组曲线的斜率比值大致等于 HR，即如果两组的 HR 恒定不变，则累积风险曲线在对数尺度下呈现平行关系。而如果 HR 随时间变化，累积风险曲线的差异会随着时间的推移而增大或缩小。从这个角度看，累积风险图可以通过曲线的趋势和两组曲线间的相对关系直观地展示 HR 的变化。\n\n\n\n\n\n\n\n\nReferences\n\nAllison, Paul D. 2010. Survival Analysis Using SAS: A Practical Guide. Book. Sas Institute."
  },
  {
    "objectID": "blog/2024/09/25/jc/index.html",
    "href": "blog/2024/09/25/jc/index.html",
    "title": "星球JC | 高尿酸血症及相关代谢疾病风险模型",
    "section": "",
    "text": "大家好，这一期预测模型星球Journal Club的分享来自青岛大学的 hedwig 同学。\n\n这篇文章是 2024年 发表在 science 子刊 advanced scicence，题为 Multimodal Machine Learning-Based Marker Enables Early Detection and Prognosis Prediction for Hyperuricemia (Zeng et al. 2024)。\n\n\n\ntitle\n\n\n\n研究背景\n高尿酸血症（HUA）是指由于体内尿酸生成增加或排泄减少而导致的血清尿酸（SUA）水平升高。SUA 升高不仅会导致痛风，同时也会增加其他代谢异常风险，如慢性肾病、高血压、心血管疾病以及糖尿病。因此，早期识别HUA和预测痛风风险，能够为提早干预和预后管理提供宝贵见解。\n目前，HUA 或痛风的风险评估主要依赖于临床指标或多基因风险评分（PRSs），缺乏将遗传和临床特征相结合的预测模型。另外，现有模型只是单纯预测 HUA 是否发生，但没有更加具体地量化 HUA 风险。\n本研究旨在结合遗传和临床数据，开发并验证一个叠加的多模态机器学习模型，以便及时识别 HUA，及早预测痛风以及代谢相关疾病。\n\n\n研究方法\n\n研究类型\n多中心研究。\n\n\n研究人群\n英国和中国两个队列。\nUKBB 是一项正在进行的前瞻性研究，研究对象为 2006 年至 2010 年间招募的 50 万名年龄在 40-69 岁之间的个体，收集数据包括临床、基因型以及多次随访数据；南方医院数据集包括接受健康体检的参与者的信息。参与者为从 2015 年至 2020 年期间到医院体检的年龄 ≥18 岁的人群。\n\n\n模型构建\n\n\n\nstudy design\n\n\nUKBB 数据随机按 8:2 分为训练集和内部测试集；南方医院数据作为外部测试集。\n筛选相关临床特征：根据以往文献选择了 10 个重要变量，此外，对于基因数据，采用 LASSO 筛选对 HUA 有预测价值的 SNP。考虑到基因数据和临床特征变量的尺度不同，研究对所有变量进行标准化处理。\n构建预测模型：研究采用集成学习的方式，将多个机器学习模型集成起来，将基分类器输出的预测概率作为 meta-classifier 的输入特征。其中，基分类器包含 7 个模型：Light Gradient-Boosting Machine、classical extreme Gradient Boosting、Categorical Boosting、Random Forest、Adaptive Boosting、Logistic Regression 以及 K-Nearest Neighbor，meta-classifier 为 classical extreme Gradient Boosting。\n\n\n\nensemble learning\n\n\n预后价值判断：对于 meta-classifier 输出的概率，研究即视为 ISHUA 分值，使用最大约登指数进行 cutoff 的确定（全部使用终点为痛风的 cutoff），依据 ISHUA 的 cutoff 值，研究将人群分为低风险与高风险亚组，进而利用随访数据，使用 KM 生存曲线及 Cox 回归，评价其对痛风及代谢相关疾病预后的预测价值。\n生活方式评价：利用模型划分的风险亚组，估计高风险组中生活方式与痛风及代谢相关疾病发生的相关关系。\n\n\n\n研究结果\n\n\n\nTable 1\n\n\n对于临床特征：利用单因素 Logistic 回归分析 10 个临床特征与 HUA 之间的相关关系，筛选的临床特征均与高尿酸血症具有显著相关性。此外使用 Cox 回归分析这 10 个特征与与痛风之间的关系。\n对于基因特征：从 GWAS 分析结果中选择与 SUA 相关（p 值小于 \\(5\\times{10}^5\\) 或以往研究报道在跨种族人群中相关）的 SNP，对选中的 SNP 进行注释，并基于确定的 SNP 进行富集分析（GO 和 KEGG 分析）。\n\n\n\nenrichment analysis\n\n\n\n模型性能评价\n在训练集中，使用遗传特征的模型预测 HUA 的 AUC 为 0.703（95%CI：0.700-0.705），使用临床特征的模型预测 HUA 的 AUC 为 0.822（95%CI：0.820-0.824），而结合了遗传和临床特征的堆叠多模态模型 AUC 为 0.859（95%CI：0.857-0.861）。\n在内部测试集和外部测试集中，堆叠多模态模型的预测性能也明显优于单独的遗传或临床模型。此外，基于两个年龄亚组（以 40 岁为界）评价模型表现，发现模型在不同年龄组中均具有较好的性能。\n\n\nISHUA 的预后评估\nISHUA 与已知的 HUA 风险因素（人口统计、临床和遗传）相关性。\n\n\n\nassociation\n\n\nISHUA 预测痛风及其他代谢相关性结局（Gout、AF、CAD、DmT2、HF、Hypertension、ESRD、All-cause death），Kaplan-Meier 生存曲线显示，ISHUA 分组能够较好地区分痛风及其他代谢相关性疾病。\n\n\n\nprognosis\n\n\n此外，还探讨了改变生活方式对不良结局的潜在获益。\n\n\n\nlifestyle\n\n\n\n\n\nTake home message\n\n文章同时纳入基因和临床特征数据，提升对 HUA 的预测能力；\n结合长期随访数据，观察模型的风险分组对相关疾病发生发展的预测价值。\n\n\n\n\n\n\n\n\nReferences\n\nZeng, Lin, Pengcheng Ma, Zeyang Li, Shengxing Liang, Chengkai Wu, Chang Hong, Yan Li, et al. 2024. “Multimodal Machine Learning-Based Marker Enables Early Detection and Prognosis Prediction for Hyperuricemia.” Journal Article. Advanced Science 11 (34): 2404047. https://doi.org/10.1002/advs.202404047."
  },
  {
    "objectID": "blog/2024/09/11/software_2.0/index.html",
    "href": "blog/2024/09/11/software_2.0/index.html",
    "title": "Software 2.0",
    "section": "",
    "text": "在过去几十年里，软件开发的核心一直是由人类编写规则和逻辑的传统编程，即Software 1.0。程序员用代码构建起一个个复杂的系统，通过显式的规则解决问题。但随着深度学习和神经网络技术的崛起，这种模式正在逐渐被颠覆。Andrej Karpathy 提出的Software 2.0概念，深入探讨了其对未来的影响。\n\n什么是 Software 2.0 ？\nSoftware 2.0 是指通过训练神经网络来生成程序，替代手动编写代码。简单来说，机器不再依赖程序员的硬编码，而是通过大量的数据和学习算法来获得解决问题的能力。例如，过去我们可能需要为图像识别编写复杂的规则，而在 Software 2.0 中，算法通过提供大量标记图像进行训练，自动学会识别不同类别的物体。\n在这个新的编程范式中，代码不再由人类编写，而是通过机器学习生成。这个过程的核心不再是编写清晰的规则，而是设计良好的神经网络架构，并用足够多的高质量数据进行训练。最终，机器通过训练模型来预测和处理任务。\n\n\n\nsoftware 2.0\n\n\n\n\nSoftware 1.0 与 2.0 的对比\n传统的 Software 1.0 主要依赖人类编写的逻辑和规则。在这种模式下，开发者根据明确的需求编写代码，并对程序行为进行详细的控制。而在 Software 2.0 中，开发人员的角色更多地转向了管理数据、设计模型架构以及优化算法。\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n特性\nSoftware 1.0\nSoftware 2.0\n\n\n\n\n开发方式\n人类编写显式规则\n通过神经网络训练自动生成\n\n\n代码控制\n开发人员详细控制代码逻辑\n依赖数据和训练模型生成解决方案\n\n\n依赖因素\n编程语言、开发工具\n数据质量、模型架构、训练算法\n\n\n开发者角色\n编写程序和调试代码\n管理数据、设计架构、优化模型\n\n\n\n从表中可以看出，Software 2.0 是一种更为抽象的开发模式，开发者的工作重心从直接编写代码转移到了更高层次的架构设计和数据处理。\n\n\n数据的重要性\n在 Software 2.0 中，数据是最关键的资源。模型的性能和准确性与数据量和质量息息相关。因此，数据的标注、收集和处理成为了决定模型成败的关键。与传统编程不同，开发人员不再关心每一个细节的实现，而是通过大量高质量的数据来训练神经网络，使其能够自主推导出解决问题的最佳方案。\n\n\n开发者角色的转变\n在这种新的编程范式下，开发者的角色发生了根本性变化。程序员不再需要直接编写实现任务的规则，而是专注于以下几方面：\n\n数据管理：负责收集、清理和标注数据。\n模型设计：选择和设计合适的神经网络架构，如卷积神经网络（CNN）或循环神经网络（RNN）。\n超参数调整：在训练过程中调整参数优化模型。\n模型评估与调优：通过测试集评估模型效果，分析模型的表现并不断改进。\n\n这种转变带来了开发流程的简化，也使开发人员的工作重点转移到了模型架构和数据质量的把控上。\n\n\n神经网络的能力与挑战\n通过神经网络，机器可以在各种复杂任务上表现出超越传统规则编程的能力。这些任务包括图像分类、自然语言处理、自动驾驶等领域。然而，神经网络本身也带来了新的挑战，例如：\n\n黑箱问题：难以解释其决策过程。\n数据依赖性：模型的性能高度依赖数据质量。\n\n\n\n未来展望\nSoftware 2.0 的出现标志着软件开发进入了一个全新的时代。随着计算能力的增强、数据资源的增长以及深度学习技术的突破，越来越多的领域将从传统编程模式转向数据驱动的神经网络模型。\n未来，开发者将逐渐从具体的代码编写中解放出来，转而专注于更高层次的架构设计和数据优化。Software 2.0 不仅提高了开发效率，还能在许多复杂任务中表现出前所未有的能力，彻底改变我们构建软件的方式。\n\n结语\nSoftware 2.0 的核心思想是用数据和神经网络取代传统代码编写。随着这项技术的进步，编程范式将发生深刻变革。开发者需要适应这一趋势，掌握更多的数据处理和模型设计技能，以应对这个数据驱动的新时代。"
  },
  {
    "objectID": "blog/2024/09/08/journal_category/index.html",
    "href": "blog/2024/09/08/journal_category/index.html",
    "title": "2023年中科院期刊分区表升级版",
    "section": "",
    "text": "前面我们爬了2023年最新JCR影响因子，一直没有提供给大家2023年中科院期刊分区表。\n今天我们爬了下中国科学院文献情报中心期刊分区表中的2023年升级版，大家公众号后台回复2023年中科院分区，即可拿到分区excel表格。\n\n\n\n2023年中科院期刊分区表\n\n\nps：直接复制这几个字后台回复即可。"
  },
  {
    "objectID": "blog/2024/09/04/casual_inference/index.html",
    "href": "blog/2024/09/04/casual_inference/index.html",
    "title": "从随意推断（casual inference）到因果推断（causal inference）",
    "section": "",
    "text": "因果分析的核心首先是因果问题，它决定了我们分析什么数据、如何分析这些数据以及我们的推断结论适用于什么人群。提出一个好的因果问题是比较难的，相对来讲，因果分析则要简单得多。本书属于方法应用性质，主要聚焦于因果推断的分析阶段。在本书前六章中，我们将讨论什么是因果问题，如何改进我们的问题，以及思考一些例子。\n\n因果问题是我们可以通过统计方法提出的一组更广泛问题中的一部分，这些问题依据数据科学的主要目的可以大致分为：描述、预测和因果推断 (Hernán, Hsu, and Healy 2019)。然而，在实际中，受到所使用统计方法的影响（如，这三类问题都可使用回归来处理）以及我们讨论它们的方式影响，我们常常会将这三种问题混淆在一起。当我们实际想要进行的是基于非随机数据的因果推断时，我们经常使用“关联”（association）这样的委婉说法，而不是直接声明我们想要估计因果效应 (Hernán 2018)。\n例如，最近一项关于流行病学研究中所使用语言的研究显示，估计效应的描述中最常见的词根是 “associate”，而许多研究人员也默认 “associate” 至少暗示了某种因果效应 (Figure 1) (Haber et al. 2022)。在分析的全部研究中，只有大约 1% 使用了 “cause” 这个词根。此外，三分之一的研究基于结论提出了相关的行动建议，而其中 80% 的建议都暗含了某种因果效应。而提出了行动建议的这些研究通常比那些只是描述效应的研究（如，使用 “associate” 和 “compare” 这样的词根）要有更强的因果效应暗示。另一方面，尽管一些研究暗示其目标是因果推断，但只有大约 4% 使用了本书将讨论的那些正式的因果推断模型，其它研究更多的做法是通过对先前相关研究或理论的引用讨论来证明他们建立的因果关系的合理性。\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\nFigure 1: 研究人员使用的词根的因果强度排名。具有更多 “Strong” 排名的词根比那些具有更多 “None” 或 “Weak” 评分的词根具有更强的因果暗示。数据来自 Haber 等人。\n\n\n\n\n因为没有明确地提出带有具体因果假设和目标的因果问题，我们最终都得到了“薛定谔的因果推断”：\n\n我们的结果表明，“薛定谔的因果推断”是很常见的，即研究一方面避免声明（甚至明确否认）对估计因果效应的兴趣，但另一方面又充满了因果意图、推断、因果暗示和行动建议。\n— Haber et al. (2022)\n\n这种方法是随意推断（casual inference）的一个例子：在没有做必要的工作来理解因果问题并处理因果假设的情况下进行推断。\n\n\n\n\n\n\nReferences\n\nHaber, N. A., S. E. Wieten, J. M. Rohrer, O. A. Arah, P. W. G. Tennant, E. A. Stuart, E. J. Murray, et al. 2022. “Causal and Associational Language in Observational Health Research: A Systematic Evaluation.” Am J Epidemiol 191 (12): 2084–97.\n\n\nHernán, Miguel A. 2018. “The C-Word: Scientific Euphemisms Do Not Improve Causal Inference From Observational Data.” American Journal of Public Health 108 (5): 616–19. https://doi.org/10.2105/ajph.2018.304337.\n\n\nHernán, Miguel A., John Hsu, and Brian Healy. 2019. “A Second Chance to Get Causal Inference Right: A Classification of Data Science Tasks.” CHANCE 32 (1): 42–49. https://doi.org/10.1080/09332480.2019.1579578.\n\nCitationBibTeX citation:@online{lu2024,\n  author = {Lu, Zhen},\n  title = {从随意推断（casual {inference）到因果推断（causal}\n    Inference）},\n  date = {2024-09-04},\n  url = {https://leslie-lu.github.io/blog/2024/09/04/casual_inference/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLu, Zhen. 2024. “从随意推断（casual Inference）到因果推断（causal\nInference）.” September 4, 2024. https://leslie-lu.github.io/blog/2024/09/04/casual_inference/."
  },
  {
    "objectID": "blog/2024/08/29/docker/index.html",
    "href": "blog/2024/08/29/docker/index.html",
    "title": "github actions使用docker渲染quarto文档",
    "section": "",
    "text": "在用github pages更新静态网站内容时，发现github actions突然报之前并未出现过的错误：\n\n\n\nerror\n\n\n错误表明系统在尝试调用UPower、Vulkan服务时遇到了问题，但是我也没找到到底是在哪里调用。而且UPower是一个用于管理电池电量和电源管理的服务，通常在桌面环境中使用。我尝试更新unbuntu上upower这些包的时候，发现github pages这种方式并没有给使用这类包的权限，看google上似乎也没有针对这个错误的比较好的解决方式。\n没办法，只能选择换一种方法来render网站的内容，我又不想在本地利用quarto每次手动来render网页，最后只能尝试选择利用docker中配置的ubuntu环境来render github repository中更新的内容，然后再像以前一样，把render出来的html，重新利用actions publish到新的repository中，这样来更新网页内容，避开直接使用actions出现的这个错误。\n我给docker的配置Dockfiles如下，给有可能遇见同样错误的同学参考：\n# install R and dependencies\nENV DEBIAN_FRONTEND=noninteractive\n# Update package indices\nRUN apt-get update -qq\n# Install helper packages\nRUN apt-get install --no-install-recommends -y software-properties-common dirmngr\n# Add the signing key for the R repository\nRUN wget -qO- https://cloud.r-project.org/bin/linux/ubuntu/marutter_pubkey.asc | tee -a /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc\n# Add the R 4.0 repository from CRAN\nRUN add-apt-repository \"deb https://cloud.r-project.org/bin/linux/ubuntu $(lsb_release -cs)-cran40/\"\nRUN apt-get install --no-install-recommends -y r-base \nRUN R -e \"if (!requireNamespace('renv', quietly=TRUE)) install.packages('renv', repos = c(CRAN = 'https://cloud.r-project.org'))\"\nWORKDIR /project\nCOPY renv.lock .\nENV RENV_PATHS_LIBRARY /renv\nRUN mkdir -p renv\nCOPY .Rprofile .Rprofile\nCOPY renv/activate.R renv/activate.R\nCOPY renv/settings.json renv/settings.json\nRUN R -e \"renv::activate(); renv::restore(repos = 'https://cloud.r-project.org')\"\n\n# install python and dependencies\nRUN apt-get update && apt-get install -y python3.10 python3-pip \\\n    && rm -rf /var/lib/apt/lists/*\nCOPY requirements.txt .\nRUN pip3 install --no-cache-dir --break-system-packages -r requirements.txt\n这样使用docker的方式就不会报错了：\n\n\n\nsuccess\n\n\n完整代码已经放在了星球里，感兴趣的同学可以自行查看。"
  },
  {
    "objectID": "blog/2024/08/14/clinical_prediction_model/index.html",
    "href": "blog/2024/08/14/clinical_prediction_model/index.html",
    "title": "欢迎加入预测模型星球",
    "section": "",
    "text": "由来\n前面我们给大家推荐了这本几位非常厉害的教授老师主编的《临床预测模型方法与应用》，陆陆续续地，在各个平台上，大家都反馈已经收到了这本书，并且这本书还很大很厚，涵盖了方法学、操作、专题以及案例。\n\n\n\nfeedback\n\n\n那这时候，就有同学和我说，按照以往看书的习惯，收到书的前一俩周，还是可以翻翻，但是后面就会慢慢地放在那里，然后就不了了之了。\n所以，我们就想，能不能有一个平台，让大家一起学习这本书，一起讨论、交流，一起进步呢？\n\n\n预测模型星球\n\n\n\n知识星球\n\n\n向大家完整介绍下这个星球，也鼓励更多想要讨论交流的小伙伴进入到我的星球里面，大家一起愉快地学习。\n\n首先，你要有实体书，不然我们每周讨论学习某一个章节的时候，你可能会有点懵。\n\n购买的二维码链接在这里。\n\n知识星球的目的是和大家一起营造一个国内高质量的预测模型类研究的知识圈，让星球里的人能够受益。\n\n来到这里你会遇到一群志同道合的人，一起学习、相互交流、共同促进。这个交流圈高度专注于预测类研究领域。\n\n\n\ncontent\n\n\n\n我们会每周一起学习讨论这本书的一个章节，每周一次，每次一个小时或一个章节，讨论的内容会包括这一章的重点内容、难点、案例分析等。\n除了这本书，如果未来人数足够，我们还会不定期地邀请一些同样从事预测类研究的博士生、学者，来和大家分享他们的研究成果、经验、心得等。\n\n后续我们会根据星球中同学们的需求，不定时地开展更多主题的workshop，具体时间请关注我们的公众号和星球，我们会在这两个平台上发布最新的信息。"
  },
  {
    "objectID": "blog/2024/08/04/workshop_001/index.html",
    "href": "blog/2024/08/04/workshop_001/index.html",
    "title": "星球第一期workshop上线",
    "section": "",
    "text": "我们星球正式上线第一期workshop啦！\n\n\n\nworkshop\n\n\n本期workshop主题是“Statistical Methods for Analysis with Missing Data”。本期workshop将从缺失数据的概念、缺失数据的类型、缺失数据的机制、缺失数据的影响、缺失数据的处理方法等方面展开讲解，帮助大家更好地理解缺失数据的问题，掌握缺失数据的处理方法。\n\n\n\nscreenshot\n\n\n目前暂定的安排是每周一节一小时，直到本期workshop的全部内容结束。\n\n\n\nused files\n\n\n后续我们会根据星球中同学们的需求，不定时地开展更多主题的workshop，具体时间请关注我们的公众号和星球，我们会在这两个平台上发布最新的信息。"
  },
  {
    "objectID": "blog/2024/07/19/new_book/index.html",
    "href": "blog/2024/07/19/new_book/index.html",
    "title": "预测模型领域新书推荐",
    "section": "",
    "text": "首先，今天这篇不是软文哦。\n\n\n\n\n临床预测模型方法与应用\n\n\n非常高兴向大家推荐和我们课题组一直保持良好合作的荷兰 Utrecht University 王俊峰教授参与主编的新书《临床预测模型方法与应用》。\n王老师是临床预测模型领域内的专家，大家感兴趣的可以去看王老师的google scolar。在和王老师合作做项目的过程中，我也是收获很多，扫除了一些知识上的盲点和疑区。因而，对于关注我的同学们而言，如果有对临床预测模型感兴趣的，我也是非常推荐这本书。\n这本书由南京医科大学公共卫生学院的陈峰教授作序，主编人员都是在预测模型、生物统计领域内有着丰富经验和深刻见解的科研人员，王老师在我们合作的项目文章里也给与了我悉心的指导，北京天坛医院谷鸿秋教授也是刚刚作为一作发表了NEJM，这本书可以说是大咖云集了。\n\n\n\n序言\n\n\n\n\n\n序言\n\n\n相信很多做科研的同学，一直想找一本这个方向领域的权威且全面的中文书，这本书应该是一个不错的选择。如果是对预测模型感兴趣的小伙伴可以直接下单预定啦，也可以关注下8月份王老师在北大、复旦的讲座。这本书8月份会正式上市，目前可以扫码下图进行预定。\n\n\n\n预定"
  },
  {
    "objectID": "blog/2024/06/07/loss_function/index.html",
    "href": "blog/2024/06/07/loss_function/index.html",
    "title": "常用损失函数",
    "section": "",
    "text": "loss function\n在机器学习/深度学习任务中，衡量模型预测值与真实值之间的差异的指标称为损失函数。损失函数是模型训练的关键组成部分，它可以帮助我们优化模型参数，使得模型的预测值更加接近真实值。预测任务的目标也是最小化损失函数，如，我们利用反向传播算法等方法，通过更新损失函数相对于模型参数的梯度来最小化损失函数，提高模型的预测能力。此外，有效的损失函数还可以帮助我们平衡模型的偏差和方差，提高模型的泛化能力。\n依据预测任务的不同，损失函数可以分为回归任务和分类任务两大类。回归任务的损失函数通常是均方误差（MSE）或平均绝对误差（MAE），而分类任务的损失函数则有交叉熵损失函数、Hinge损失函数等。本文将介绍常用的损失函数及其应用场景。\n\n均方误差（MSE）\n均方误差（Mean Squared Error，MSE）是回归任务中最常用的损失函数之一，它衡量模型预测值与真实值之间的差异。MSE的计算公式如下：\n\\[\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n\\]\n可以看到，MSE是预测值与真实值之间差值的平方和的均值，它对较大差异分配更高的惩罚。MSE非负，越小，说明模型的预测值与真实值之间的差异越小，模型的预测能力越好。MSE对异常值敏感，因为它是差值的平方和，异常值的平方会放大差异，导致模型的预测能力下降。\n其在pytorch中的实现：\n\ntorch.nn.MSELoss(reduction='mean')\n\n\n\n平均绝对误差（MAE）\n平均绝对误差（Mean Absolute Error，MAE）是回归任务中另一种常用的损失函数。MAE的计算公式如下：\n\\[\nMAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n\\]\n相比于MSE，MAE是预测值与真实值之间差值的绝对值的均值，它对异常值不敏感，因为它是差值的绝对值的和，不会对某一异常值的差异分配过高的权重。MAE的值越小，说明模型的预测值与真实值之间的差异越小，模型的预测能力越好。\n针对MAE和MSE的优缺点，我们可以根据具体的任务需求选择合适的损失函数。如果任务需要重点关注异常值，可以选择MSE，否则选择MAE。\n\ntorch.nn.L1Loss(reduction='mean')\n\n\n\nHuber loss\nHuber loss是一种结合了MSE和MAE的损失函数，它在差值较小的情况下使用MSE，差值较大的情况下使用MAE。Huber loss的计算公式如下：\n\\[\nL_{\\delta}(y, \\hat{y}) = \\left\\{\n\\begin{array}{ll}\n\\frac{1}{2}(y - \\hat{y})^2, & \\text{if } |y - \\hat{y}| \\leq \\delta \\\\\n\\delta |y - \\hat{y}| - \\frac{1}{2} \\delta^2, & \\text{otherwise}\n\\end{array}\n\\right.\n\\]\n其中，\\(\\delta\\)是一个超参数，用于控制MSE和MAE之间的平衡。Huber loss对异常值不敏感，同时保留了MSE的平滑性，是一种较为稳健的损失函数。\n\ntorch.nn.SmoothL1Loss(reduction='mean')\n\n\n\n二元交叉熵损失函数（Binary Cross Entropy Loss）\n交叉熵损失函数（Cross Entropy Loss）是二分类任务中最常用的损失函数之一，我们前面也以及介绍过。交叉熵损失函数的计算公式如下：\n\\[\nL(y, \\hat{y}) = -\\frac{1}{n} \\sum_{i=1}^{n} y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)\n\\]\n其中，\\(y_i\\)是真实标签，\\(\\hat{y}_i\\)是模型预测的概率值。交叉熵损失函数对于模型预测的概率值和真实标签之间的差异进行了惩罚，使得模型更加关注预测正确的类别。交叉熵损失函数是一种凸函数，可以通过梯度下降等方法进行优化。\n\ntorch.nn.BCELoss(weight=None, reduction='mean')\n\n\n\n多类交叉熵损失函数（Categorical Cross Entropy Loss）\n多类交叉熵损失函数是多分类任务中常用的损失函数之一，它是交叉熵损失函数的扩展。多类交叉熵损失函数的计算公式如下：\n\\[\nL(y, \\hat{y}) = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{m} y_{ij} \\log(\\hat{y}_{ij})\n\\]\n其中，\\(y_{ij}\\)是真实标签，\\(\\hat{y}_{ij}\\)是模型预测的概率值。\n\ntorch.nn.CrossEntropyLoss(weight=None,ignore_index=-100, reduction='mean')\n\n\n\nHinge损失函数\nHinge损失函数是支持向量机（SVM）中常用的损失函数之一，它适用于二分类任务。Hinge损失函数的计算公式如下：\n\\[\nL(y, \\hat{y}) = \\max(0, 1 - y \\cdot \\hat{y})\n\\]\nHinge损失函数旨在最大化决策边界的间隔，即使得正确分类的样本距离决策边界的距离尽可能大。Hinge损失函数对于误分类的样本进行了惩罚，使得模型更加关注分类边界附近的样本，从而尽可能把数据点推向远离决策边界的方向。\n代码已经放进了星球里。"
  },
  {
    "objectID": "blog/2024/05/13/hierarchical_composite_endpoints/index.html",
    "href": "blog/2024/05/13/hierarchical_composite_endpoints/index.html",
    "title": "Hierarchical composite endpoints治疗效应的可视化",
    "section": "",
    "text": "复合终点\n有时，根据主要研究目的，我们很难从多个终点指标中选出其中某一个作为主要终点，此时，我们可以利用复合终点来作为主要终点。\nHierarchical composite endpoints (HCE)可以整合不同类型、不同重要性终点成一个有序终点指标，以表示患者经历的不同严重程度的终点。如，在固定随访的RCT中，outcomes of interest可以是death、hospitalization，而这两个终点存在严重程度的差异。很明显，死亡是最严重的。同样最终死亡的两个患者，生存时间更长，意味治疗效应更好；同样最终住院的两个患者，入院前时间更长，治疗效应更好；同样未住院的两个患者，某一实验室指标的change from baseline更大，效应更好。\n对于这种HCE，我们可以计算win odds(Gasparyan et al. 2021)来比较组间差异，然而，治疗效应的可视化受到复合终点的影响，不容易像单纯的生存曲线那样用合适的工具可视化出来。\n针对这一问题，AstraZeneca的Martin Karpefors等人提出了一种新的方法，即maraca plot(Karpefors, Lindholm, and Gasparyan 2023)。这种方法可以将复合终点中time to event(TTE)以及连续性终点的治疗效应可视化出来，同时也可以用来比较不同治疗组之间的差异。对应的R包可以方便地实现这一点。\nmaraca plot\nmaraca基于ggplot2，其中，对于TTE采用Kaplan-Meier曲线展示cumulative proportions，对于连续性终点可选用箱线图、violin plot以及scatter plot展示连续性分布。这种方法可以同时展示HCE的不同组成成分。\n来看一个例子。\n\nlibrary(maraca)\ndata(hce_scenario_a, package = \"maraca\")\ndata &lt;- hce_scenario_a\ndata |&gt; head()\n\n  SUBJID              GROUP GROUPN      AVAL0       AVAL    TRTP\n1      1          Outcome I      0 120.440921   120.4409  Active\n2      2 Continuous outcome  40000   3.345229 40003.3452 Control\n3      3 Continuous outcome  40000  22.802615 40022.8026  Active\n4      4          Outcome I      0 577.311386   577.3114 Control\n5      5         Outcome II  10000 781.758081 10781.7581  Active\n6      6        Outcome III  20000 985.097981 20985.0980 Control\n\n\n具体变量意义，大家可以查看?hce_scenario_a。\n可视化如下：\n\ncolumn_names &lt;- c(outcome = \"GROUP\", arm = \"TRTP\", value = \"AVAL0\")\ntte_outcomes &lt;- c(\"Outcome I\", \"Outcome II\", \"Outcome III\", \"Outcome IV\")\ncontinuous_outcome &lt;- \"Continuous outcome\"\narm_levels &lt;- c(active = \"Active\", control = \"Control\")\nmaraca_object &lt;- maraca(\n  data, tte_outcomes, continuous_outcome, arm_levels, column_names,\n  fixed_followup = 3*365, compute_win_odds = TRUE\n)\nAZ_colors &lt;- c(\"#830051\", \"#F0AB00\")\nplot(maraca_object, density_plot_type = \"default\") + theme_bw() +\n  scale_color_manual(values = AZ_colors) +\n  scale_fill_manual(values = AZ_colors)\n\n\n\n\n\n\n\n结果解释\n怎么看这张图？\n首先是x轴上HCE的5个组成成分，x轴上每个成分的长度大小，代表了患者达到不同成分终点的比例，可以看到，continuous outcome的比例最大，说明这个终点的患者所占比例最大。其次，cumulative percentage显示active组在四个TTE终点上是存在差异的。再然后是continuous outcome的分布，偏向x轴右侧代表change from baseline更大。而这些结合起来，就是win odds的结果，可以看到，和我们从可视化的角度看到的结果是一致的。\n代码已经放进了星球里。\n\n\n\n\n\nReferences\n\nGasparyan, S. B., E. K. Kowalewski, F. Folkvaljon, O. Bengtsson, J. Buenconsejo, J. Adler, and G. G. Koch. 2021. “Power and Sample Size Calculation for the Win Odds Test: Application to an Ordinal Endpoint in COVID-19 Trials.” Journal Article. Journal of Biopharmaceutical Statistics 31 (6): 765–87.\n\n\nKarpefors, M., D. Lindholm, and S. B. Gasparyan. 2023. “The Maraca Plot: A Novel Visualization of Hierarchical Composite Endpoints.” Journal Article. Clinical Trials (London, England) 20 (1): 84–88. https://doi.org/10.1177/17407745221134949.\n\nCitationBibTeX citation:@online{lu2024,\n  author = {Lu, Zhen},\n  title = {Hierarchical Composite {endpoints治疗效应的可视化}},\n  date = {2024-05-13},\n  url = {https://leslie-lu.github.io/blog/2024/05/13/hierarchical_composite_endpoints/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLu, Zhen. 2024. “Hierarchical Composite\nEndpoints治疗效应的可视化.” May 13, 2024. https://leslie-lu.github.io/blog/2024/05/13/hierarchical_composite_endpoints/."
  },
  {
    "objectID": "blog/2020/04/30/index.html",
    "href": "blog/2020/04/30/index.html",
    "title": "一文搞懂如何选择卡方检验",
    "section": "",
    "text": "在实际进行数据处理过程中，我发现有的时候，会有小伙伴在选择何种检验方法时，存在一定的疑问。今天咱们梳理一下究竟如何选择卡方检验及其相关检验方法。\n\n四格表资料卡方检验\n\n非配对设计时\n\n当总样本量n&gt;=40且所有理论数T&gt;=5时，用普通的Pearson卡方检验，若所得的P约等于检验水准，改用确切概率法；\n当总样本量n&gt;=40但有1&lt;=T&lt;5时，用连续性校正的卡方检验；\n当总样本量n&lt;40或有理论数T&lt;1时，不能用卡方检验，改用确切概率法。\n\n配对设计时\n\n四格表配对卡方检验对称性检验：McNemar检验；\n配对卡方检验的一致性检验：Kappa一致性检验。\n\n\n\n\nR*C 表资料卡方检验\nR*C 表资料可以分为双向无序、单向有序、双向有序属性相同和双向有序属性不同4类：\n\n双向无序R*C表：表中两个分类变量皆为无序分类变量，对于该类资料：\n\n若研究目的为多个样本率或构成比的比较，可用行*列表资料的卡方检验；\n若研究目的为分析两个分类变量之间有无关联性以及关系的密切程度，可用行*列表资料的卡方检验，以及Pearson列联系数进行分析\n\n单向有序R*C表有两种形式：一种是R*C表中的分组变量有序、指标变量无序，此种单向有序R*C表可用行*列表资料的卡方检验；另一种是R*C表中的分组变量无序、指标变量有序，此种单向有序R*C表宜用秩和检验或行均分检验进行分析。\n具体采用哪种检验方法，需要重点关注谁为指标变量、谁为分组变量以及指标变量是否有序。\n双向有序属性相同的R*C表，表中的两个分类变量皆为有序且属性相同，研究目的通常是分析两种方法的一致性，实际上是2*2配对设计的扩展，此时宜用Kappa一致性检验，也可以用特殊模型分析方法。一致性检验与处理配对设计2*2表的思路一样。\n除了对该方表资料作一致性检验外，还可以对两种方法的结果不一致的部分作比较，此时称为对称性检验。\n双向有序属性不同的R*C表，表中的两个分类变量皆为有序且属性不同。对于此类资料，有3种分析目的，所以也就有3种相应的统计分析方法：\n\n只关心试验分组变量取不同水平时，有序的指标结果变量之间的差别是否有显著性差异，可用单向有序列联表的第二种行均分检验；\n希望研究两个有序变量之间是否有相关关系，需要运用定性资料的相关分析方法，包括Spearman秩相关分析和典型相关分析；\n若两个有序变量之间有相关关系，但不能推断两个变量之间是否是线性相关，是否呈直线变化关系，还需要进一步研究两个变量之间是否呈直线变化关系，即两个有序分类变量间是否存在线性变化趋势，还需要进一步进行线性趋势检验即趋势卡方检验，以判断两变量是否线性相关。"
  },
  {
    "objectID": "blog/2020/04/28/index.html",
    "href": "blog/2020/04/28/index.html",
    "title": "如何理解残差与误差",
    "section": "",
    "text": "线性回归因变量的观测值是样本真实数据，估计值是回归模型估计出来的预测值，二者的差别称为残差。误差项表示对因变量而言，模型自变量无法解释的部分，通常假定误差服从均值为0的分布。\n注意：残差与误差项不同，残差是针对样本真实数据而言，可以视为对总体模型中误差项的估计。\n残差越小，说明拟合的回归模型越好，模型的预测值更加贴近实际值。\n虽然我们可能会更关心回归系数的大小，但实际上残差能告诉我们这些系数的估计值是否可靠，而这一点更为重要，回归系数无论表现多好，如果其不能稳定可靠，我们的计算和预测是没有意义的。\n因此，在数据分析中一定要学会观察残差图，任何模型拟合的问题基本上都能在残差图中体现出来。\n一条对数据点拟合较好的回归线必然穿过所有数据点的中央，所有的点围绕这条线随机波动，反映在残差中就应该是围绕0随机波动，不应该有任何趋势。如果残差能看出趋势，则说明模型拟合肯定有问题。"
  },
  {
    "objectID": "blog/2020/04/26/index.html",
    "href": "blog/2020/04/26/index.html",
    "title": "Python与矩阵",
    "section": "",
    "text": "矩阵可以被看作是排列的向量或堆放在一起的数字。矩阵的意义非常重要，它可以作用在一个具体的向量上，使向量空间位置发生变换。\n\n矩阵的Python表示\n对于矩阵而言，最直观的描述就是一个m*n的数字方阵，它可以看作是n个m维列向量从左到右并排摆放，也可以看作是m个n维行向量从上到下进行叠放。\n\n# 矩阵的表示：\nimport numpy as np\nA = np.array([[1, 2],\n              [3, 4],\n              [5, 6],\n              [7, 8]])\nprint(A)\n# 在形容矩阵的形状和规模时，一般采用其行数和列数来进行描述\nprint(A.shape)\n# 通过矩阵A的shape属性，可以获取一个表示矩阵规模的元组对象\n# 这个元组对象包含两个元素：第一个元素表示行数，第二个表示列数\n\n[[1 2]\n [3 4]\n [5 6]\n [7 8]]\n(4, 2)\n\n\n前面提到，n维的行向量可以看作是一个1*n的特殊矩阵；同理，n维的列向量也同样可以看作是一个n*1的特殊矩阵。这样，一方面，可以将矩阵和向量的Python表示方法统一起来；另一方面，在接下来要介绍的矩阵与向量的乘法运算中，可以将其看作是矩阵与矩阵之间乘法的一种特殊形式，从而统一运算方式。\n\nB=np.array([[1,2,3,4]]) #行向量；用生成矩阵的方法生成了一个1*4的矩阵，用来表示一个四维的行向量\nprint(B.shape)\nprint(B.T) #转置成列向量；因为是矩阵形式，所以可以用转置方法，得到对应的四维列向量\nC=np.array([[1],\n            [2],\n            [3],\n            [4]]) #列向量\nprint(C)\nprint(C.shape)\n\n(1, 4)\n[[1]\n [2]\n [3]\n [4]]\n[[1]\n [2]\n [3]\n [4]]\n(4, 1)\n\n\n\n\n特殊形态的矩阵\n\n方阵：行数和列数相等的一类矩阵，称为方阵，其行数或列数称为它的阶数。\n\n\nA = np.array([[1, 1, 1, 1],\n              [2, 2, 2, 2],\n              [3, 3, 3, 3],\n              [4, 4, 4, 4]]) #4阶方阵\nprint(A)\nprint(A.shape)\n\n[[1 1 1 1]\n [2 2 2 2]\n [3 3 3 3]\n [4 4 4 4]]\n(4, 4)\n\n\n\n对称矩阵：开始介绍对称矩阵之前，先说明一下矩阵转置的概念。\n对于指定的矩阵，若将其行和列上的元素进行位置互换，即原行元素作新列，原列元素作新行，即可得到一个全新的矩阵，这个新矩阵称为原矩阵的转置矩阵，行和列互换的矩阵操作就称为矩阵的转置。\n若原矩阵和转置后新得到的矩阵相等，那么将原矩阵称为对称矩阵。由此可见，矩阵对称的前提条件是该矩阵首先必须是一个方阵；此外，对称矩阵的一个典型特征为：沿着从左上到右下的对角线，关于这条对角线相互对称的元素都是彼此相等的。\n可以说，对称矩阵是最重要的矩阵，在矩阵的相关分析中扮演极其重要的角色。\n\n\n# 矩阵的转置：\nimport numpy as np\nA = np.array([[1, 2, 3, 4],\n              [5, 6, 7, 8]])\nprint(A)\nprint(A.T)\n# 对称矩阵：\nimport numpy as np\nS = np.array([[1, 2, 3, 4],\n              [2, 5, 6, 7],\n              [3, 6, 8, 9],\n              [4, 7, 9, 0]])\nprint(S)\nprint(S.T)\n\n[[1 2 3 4]\n [5 6 7 8]]\n[[1 5]\n [2 6]\n [3 7]\n [4 8]]\n[[1 2 3 4]\n [2 5 6 7]\n [3 6 8 9]\n [4 7 9 0]]\n[[1 2 3 4]\n [2 5 6 7]\n [3 6 8 9]\n [4 7 9 0]]\n\n\n\n零矩阵：所有元素均为0的矩阵，称为零矩阵。\n\n\nA = np.zeros([5, 3])\nprint(A)\n\n[[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]]\n\n\n\n对角矩阵：若方阵在非对角线位置上元素全部为0，则称为对角矩阵，0元素的位置可以省去不写。\n\n\nA = np.diag([1, 2, 3, 4, 5])\nprint(A)\nprint(A.T)\n\n[[1 0 0 0 0]\n [0 2 0 0 0]\n [0 0 3 0 0]\n [0 0 0 4 0]\n [0 0 0 0 5]]\n[[1 0 0 0 0]\n [0 2 0 0 0]\n [0 0 3 0 0]\n [0 0 0 4 0]\n [0 0 0 0 5]]\n\n\n\n单位矩阵/单位阵：对角位置上元素全部为1，其余位置元素均为0的特殊对角矩阵，称为单位矩阵。\n\n\nI = np.eye(5)\nprint(I)\n\n[[1. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0.]\n [0. 0. 1. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 1.]]"
  },
  {
    "objectID": "blog/2020/03/22/index.html",
    "href": "blog/2020/03/22/index.html",
    "title": "概率密度与累积分布",
    "section": "",
    "text": "概率分布是统计学的基础，统计学中不少概念和思想都与概率分布有关系。理解了概率分布，很多原来不明白的问题很可能就迎刃而解了。\n\n理解累积分布和概率密度的概念。\n累积分布通俗地说，就是从0一直累积到100%，累积的速度可以相等，也可以不相等，累积分布函数一般用F(x)来表示；概率密度也就是概率的密度，通俗而言，密度就是在某个点上数据比较集中，反映了数据的不同变化特征，概率密度函数一般用f(x)表示。\n理解累积分布的斜率与概率密度的关系。\n累积分布的斜率越大，概率密度也越大。事实上，密度值等于累积分布中对应点的斜率。\n理解累积分布与概率密度的关系。\n密度与累积分布的关系是：密度=累积分布的增加量/长度，或者，累积分布的增加量=密度*长度，即概率密度曲线下面积等于相应累积分布函数增加量。对于累积分布而言，如果看整条曲线，即从0增加到100%，因此对应的概率密度曲线下，其面积一定也是100%。\n\n我们常说的正态分布、二项分布、t分布等概率分布其实都是概率密度函数，所以它们的曲线下方的面积都是100%，只是不同点的密度不同而已。如有的分布可能在0的时候密度较高（数据主要集中在0附近）。"
  },
  {
    "objectID": "blog/2020/03/11/index.html",
    "href": "blog/2020/03/11/index.html",
    "title": "Python与向量",
    "section": "",
    "text": "Python中一般使用numpy库生成一个向量，但其默认生成的是行向量。\n\nimport numpy as np\n# 行向量\na= np.array([1,2,3,4])\nprint(a)\n\n[1 2 3 4]\n\n\n但我们一般使用列向量的形式，因此需要对其做一些处理。有人想，转置处理就可以了，也就是把向量的行索引和列索隐交换位置。但是numpy重的转置方法对于一维数组是无效的：\n\na= np.array([1,2,3,4])\nprint(a.transpose()) #从程序的运行结果来看，确实是无效的\n\n[1 2 3 4]\n\n\n应该如何表示一个列向量呢？\n\nA= np.array([1,2,3,4])\nA_t= A[:, np.newaxis] #增加一个维度\nprint(A_t)\nprint(A_t.shape) #列向量本身就是二维表示的\n\n[[1]\n [2]\n [3]\n [4]]\n(4, 1)\n\n\n这种做法比较复杂，更直观更简单的实现方法是：显然，我们一直把向量看作是一个维数为1的数组，其实也可以看作是行数为1或列数为1的一个二维数组。而二维数组对应的就是矩阵，因此向量还可以看作是一个特殊的矩阵，即可以把行向量看作是一个1m的特殊矩阵，可以把列向量看作是一个n1的特殊矩阵。\n\n# 在对行向量进行初始化时，使用了numpy中的二维数组的初始化方法，因此在语句中多嵌套了一层中括号\nA= np.array([[1,2,3,4]])\nprint(A)\nprint(A.T) #此时可以直接通过行向量转置的方法生成对应的列向量\n\n[[1 2 3 4]]\n[[1]\n [2]\n [3]\n [4]]\n\n\n\n向量的加法\n两个维数相同的向量才能进行加法运算，只要将相同位置上的元素对应相加即可，结果向量的维数保持不变。\n\nu= np.array([[1,2,3]]).T\nu\nv= np.array([[5,6,7]]).T\nprint(u+v)\n\n[[ 6]\n [ 8]\n [10]]\n\n\n\n\n向量的数乘\n向量的数乘就是将参与乘法运算的标量同向量的每个元素分别相乘，以此得到最终的结果向量，结果向量的维数依然保持不变。从几何意义上来看，向量的数乘就是将向量沿着所在直线的方向拉伸相应的倍数，拉伸方向和参与运算的标量符号一致。\n\nu= np.array([[1,2,3]]).T\nprint(3*u)\n\n[[3]\n [6]\n [9]]\n\n\n\n\n向量间的乘法：内积和外积\n向量间的乘法分为内积和外积两种形式。\n向量的内积运算：参与内积运算的两个向量必须维数相等，运算规则是先将对应位置上的元素相乘，然后合并相加，最终运算结果是一个标量。可能这样说不太好理解，要是从几何表示上看，内积的意义就非常清晰了。\n内积的几何表示u*v= |u||v|cosθ，它表示向量u在向量v方向上的投影长度乘以向量v的模长。需要注意的是，在实际运算向量内积时，无论是行向量间的内积还是列向量间的内积，最终的运算结果都是一样的。\n\nu= np.array([3,5,2])\nv= np.array([1,4,7])\n# 若使用numpy库中的内积运算函数dot进行运算，传入的参数必须是用一维数组表示的行向量\nprint(np.dot(u,v)) #一维乘一维，结果还是一维\n\nuC= u[:,np.newaxis]\nvC= v[:,np.newaxis]\nprint(uC)\nprint(vC)\nprint(np.dot(uC,vC)) #行数为1的二维数组，即二维数组形式的行向量，报错\n\nu= np.array([[3,5,2]])\nv= np.array([[1,4,7]])\nprint(np.dot(u,v)) #列数为1的二维数组，即二维数组形式的行向量，报错\n\nu= np.array([[3,5,2]]).T\nv= np.array([[1,4,7]]).T\nprint(np.dot(u,v)) #列数为1的二维数组，即二维数组形式的列向量，报错\n\n二维数组形式的向量怎么进行内积运算呢？我们知道二维数组表示下得向量的本质上是矩阵，只不过是行数或列数为1的特殊矩阵。若将这种表示方法下的向量作为传入内积运算函数dot的参数，就需要依据矩阵的乘法法则来计算。\n\nu= np.array([[3,5,2]])\nv= np.array([[1,4,7]]).T\nprint(np.dot(u,v)) #二维乘二维，结果还是二维\n\n[[37]]\n\n\n向量的外积运算：这里只讨论在二维平面和三维空间中的运算情况。\n在二维平面中，与内积类似，外积也有一种表达式：u*v= |u||v|sinθ。在二维平面中，向量的外积表示两个向量张成的平行四边形的“面积”。当然，这个面积要打上引号，因为若两个向量的夹角大于180度，那么向量外积运算所得到的结果为负。\n\nu= np.array([3,5])\nv= np.array([1,4])\nprint(np.cross(u,v)) #一维乘一维，结果还是一维\n\n7\n\n\n/tmp/ipykernel_7445/2827165229.py:3: DeprecationWarning: Arrays of 2-dimensional vectors are deprecated. Use arrays of 3-dimensional vectors instead. (deprecated in NumPy 2.0)\n  print(np.cross(u,v)) #一维乘一维，结果还是一维\n\n\n而在三维空间中，外积要复杂一点，其计算所得到的结果是一个向量而不是一个数值。其最终得到的结果向量也是有明确的物理含义的，即表示u和v两个向量张成平面的法向量。\n\nx= np.array([3,3,9])\ny= np.array([1,4,12])\nprint(np.cross(x,y))\n\n[  0 -27   9]\n\n\n\n\n向量先数乘后叠加：向量的线性组合\n基于向量加法和数量乘法这两类基本运算，将其进行组合应用。针对向量u和v，先求出标量c和向量u的数量积，再求出标量d和向量v的数量积，最后再将二者进行叠加，就得到向量u和v的线性组合cu+dv，这里的标量c和d可以取任意值，包括0。\n\nu= np.array([[1,2,3]]).T\nv= np.array([[4,5,6]]).T\nw= np.array([[7,8,9]]).T\nprint(3*u + 4*v + 5*w)\nprint(u)\nu.shape\nnp.array([[1,2,3]]).shape\nprint(np.dot(u.T,v.T)) #报错，带有第二个维度无法进行向量内积\n\n进一步思考：我们知道，两个向量相加，在几何上就是将两个向量首尾依次连接，所得到的结果向量就是连接最初的起点和最终的终点的有向连线。我们假定有3个非零的三维列向量u、v和w，讨论以下几种不同的线性组合情况：\n\n第一种情况：cu的所有线性组合构成的图像\n由于标量c可以取0，因而cu的所有线性组合构成的图像可以表示为三维空间中一条穿过原点(0,0,0)的直线，包括原点本身。\n第二种情况：cu+dv的所有线性组合构成的图像\n\n当向量u和向量v不在一条直线上时，即u和v不共线\ncu+dv的所有线性组合构成的图像可以表示为三维空间中的一个通过原点(0,0,0)的二维平面。\n当向量u和向量v处在一条直线上时\n则cu+dv的所有线性组合构成的图像可以表示为三维空间中一条穿过原点(0,0,0)的直线，包括原点本身。\n\n第三种情况：cu+dv+ew的所有线性组合构成的图像\n\n当向量u、v、w不在一个平面上时，cu+dv+ew的所有线性组合构成的图像是整个三维空间。\n当向量u、v、w处在一个平面上时，cu+dv+ew的所有线性组合构成的图像是三维空间中的一个通过原点(0,0,0)的二维平面。\n当向量u、v、w处在一条直线上时，cu+dv+ew的所有线性组合构成的图像是三维空间中一条穿过原点(0,0,0)的直线，包括原点本身。\n\n\n我们发现，在讨论上述线性组合的多种不同情况时，均反复提到了共线、共面的概念。这些特殊的性质会对一组向量线性组合所得到的结果向量在空间中的位置产生重要影响，它们构成了线性代数中非常重要的概念。后面我们也将使用更加专业的词汇对其进行描述和介绍，即线性相关和线性无关。"
  },
  {
    "objectID": "blog/2020/02/23/index.html",
    "href": "blog/2020/02/23/index.html",
    "title": "协方差分析",
    "section": "",
    "text": "医学试验设计一个很重要的目的就是为了排除非处理因素的干扰影响，使试验误差的估计降到最低限度，从而可以准确地获得处理因素的试验效应。但在某些实际问题中，有些因素在目前还不能控制或难以控制。如在动物饲养试验中，各组动物所增加的平均体重不仅仅与各种饲料营养价值高低有关，还与各动物的进食量有关，甚至与各动物的初始体重等因素及其交互作用都有关系。如果直接进行方差分析，会因为混杂因素的影响而无法得出正确的结论。\n协方差分析是将回归分析与方差分析结合起来使用的一种分析方法。在协方差分析中，先将定量的影响因素（即难以控制的混杂因素）看作自变量/协变量，建立因变量随自变量变化的回归方程，利用回归方程把因变量的变化中受不易控制的定量因素的影响扣除掉，从而能够较合理地比较定性的影响因素处在不同水平下，经回归分析手段修正以后的因变量的总体均数之间是否有显著性的差别，这就是协方差分析的基本思想。\n协方差分析用于比较一个变量Y在一个或几个因素不同水平上的差异，但Y在受这些因素影响的同时，还受到另一个变量X的影响，且X变量的取值难以人为控制，不能作为方差分析中的一个因素处理。此时如果X与Y之间可以建立回归关系，则可以用协方差分析的方法排除X对Y的影响，然后再用方差分析的方法对各因素水平的差异进行统计推断。在协方差分析中，我们称Y为因变量，X为协变量，即在方差分析中用来校正因变量的数值型变量。\n也许有人会问随机因素的影响也是不能人为控制的，为什么不能把X作为一种随机因素处理呢？\n这里的差异主要在于作为随机因素处理时，虽然每一水平的影响是不能人为控制的，但我们至少可以得到几个属于同一水平的重复，因此可以把它们分别用另一因素的不同水平处理，最后在方差分析时，我们才能排除这一随机因素的影响，对另一因素的各水平进行比较。\n例如，当我们考虑动物窝别对增重的影响时，一般可把它当作随机因素处理。一方面是由于它不易数量化，另一方面是同一窝一般有几只动物，可分别接受另一因素不同水平的处理。如果我们考虑试验开始时动物初始体重的影响，这时一般的方法是选初始体重相同的动物作为一组，分别接受另一因素的不同水平处理，此时用方差分析没有问题。但显然，这种方法往往是很困难的，一般需要很大的样本。若可供试验的动物样本很少，初始体重又有明显差异，无法选出体重相当的动物，那就只好认为初始体重X与最终体重Y有回归关系，采用协方差分析的方法排除初始体重的影响，再来比较其他因素如饲料种类、数量对增重的影响。协方差分析既利用了回归分析的基本方法，又用到了方差分析的基本方法，这就是协方差分析的基本思想。\n消除初始体重影响的另一种方法是对最终体重与初始体重的差值进行统计分析，这种方法与协方差分析的生物学意义是不同的。对差值进行分析的生物学假设是初始体重对以后的体重增量没有任何影响，而协方差分析则是假设体重增量中包含初始体重的影响（不仅仅是初始体重对最终体重的影响），这种影响的大小与初始体重成正比，如果这一比值为1，协方差分析与对差值进行方差分析是相同的。但如果比值不为1，它们的结果将是不同的。也就是说，协方差分析假设使初始体重不同的因素在以后的生长过程中也会发挥作用，而对差值进行方差分析则是假设这些因素以后不再发挥作用，这两种生物学假设是有很大区别的。\n在学习中需要注意统计学知识背后的研究假设。由于协方差分析包含了对协变量影响是否存在及其大小等一系列统计检验与估计，它显然比对差值进行分析等方法有更广泛的适用范围，因此除非有明显的证据说明对差值进行分析的生物学假设是正确的，一般情况下还是应采用协方差分析方法。\n在医学研究中，很多情况下都需要借助协方差分析来排除非处理因素的干扰，从而准确地估计处理因素的试验效应。如，评价3种药物治疗高脂血症的效果，寻求各方面自然条件基本相同的受试者是很困难的，但是把患者的年龄、体重指数、用药前的血脂水平等作为协变量进行协方差分析，就简单很多。同样，比较几种不同营养奶粉对婴幼儿体重增长的作用差异，把研究对象的性别、年龄、基线体重等混杂因子作为协变量进行协方差分析，则是非常有效的统计分析方法。\n和方差分析一样，协方差分析也属于参数分析，因变量Y应当满足以下假设条件：\n\n在效应因子的每一个水平上，因变量Y服从正态分布，且方差相等；\n在效应因子的每一个水平上，因变量Y和协变量X呈线性关系，且斜率相同。"
  },
  {
    "objectID": "blog/2020/02/20/index.html",
    "href": "blog/2020/02/20/index.html",
    "title": "试验设计与方差分析（3）",
    "section": "",
    "text": "正交设计\n析因设计的缺点是当因素个数较多时（3个因素以上），所需试验单位数、处理组数、试验次数和方差分析的计算量会剧增。减少多因素试验次数的有效方法是采用正交试验设计。\n当析因设计要求的试验次数太多时，一个非常自然的想法就是从析因设计的全部水平组合中选择一部分有代表性的水平组合进行试验，因此就出现了分式析因设计。但是对于试验设计知识较少的实际工作者而言，选择适当的分式析因设计还是比较困难的，而正交试验设计是研究多因素多水平的又一种试验设计方法。\n它是根据正交性从全部的试验组合中挑选出部分有代表性的水平组合进行试验，这些有代表性的水平组合具备均匀分散、齐整可比的特点。正交试验设计是分式析因设计的主要方法，高效、快速、经济。日本著名统计学家田口玄一将正交试验选择的水平组合列成表格，称为正交表。\n例如做一个3因素3水平的试验，按全面试验要求，需进行333=27种水平组合的试验，且尚未考虑每一组合的重复数，而正交试验设计可以大大减少工作量。\n正交设计在医学研究中的用途相当广泛，在具体的操作上，也比析因设计简单，可寻找疗效好的药物配方、医疗仪器多个参数的优化组合、医疗产品的生成工艺、生物体的培养条件等。\n假定在一个农业试验中要考察3个小麦品种、3种不同的肥料和3种播种方式对小麦产量的影响，并假定有9个地力基本相同的试验小区。在这个问题中，有3个可能影响小麦产量的因子：品种、肥料和播种方式，每个因子有3个水平，如果要做完全试验，就需要333=27个小区，而实际上总共只有9个小区，显然，完全试验在当前的情况下行不通。\n因此我们可退一步考虑，按照上述正交设计的想法，设计要求品种、肥料和播种方式中的任意两个的不同水平的搭配都出现一次，满足这种性质的试验设计就是正交试验。\n下面给出正交设计的一般性陈述：考虑设计一个试验，安排m个因子，做n次试验，若它满足以下两个条件，则其为正交试验：\n\n每一因子的不同水平在试验中出现相同的次数（均衡性）；\n任意两因子的不同水平组合在试验中出现相同的次数（正交性）。\n\n就定义来说，等重复的完全试验显然满足上述两个条件，因此当然也是正交试验设计。但由于其要求的试验次数太多，所以实际上很难实施。我们通常所说的正交试验设计，是指既满足上述两条件，同时试验次数n又远远小于N的设计。\n正交试验设计的方案可以用一张表来表示，这张表就被称为正交设计表。一般来说，正交设计表的第1行为表头，标明每列所代表的因子，最左一列标明试验的序号，由1到n。注意：试验的序号并不表示试验的时间先后顺序，先后顺序要按照随机化原则来安排。表中每列的数字代表相应因子的水平序号，每行的数字代表在相应试验中各因子的水平序号，有：\n\n每列中不同数字出现的次数相同（试验的均衡性）；\n每两列中不同的数字组合出现的次数相同（试验的正交性）。\n\n假定因子对响应变量的影响无交互效应（许多实际情况正是这样），正交试验的优点是在很少的试验次数（与全面试验相比）中，所得数据可以简便而有效地对因子效应进行参数估计和方差分析。\n其方法可一般地归纳如下：\n\n总均值的估计=试验数据的总平均值\n某因子的某个主效应的估计=该因子的该主效应所出现的试验数据的平均值-总平均值\n总平方和=（试验数据-总平均值）的平方和，自由度=n-1\n某因子的主效应平方和=重复数*参数估计的平方和，自由度=水平数-1\n残差平方和=总平方和-因子效应平方和的和，自由度=总平方和-因子效应自由度的和"
  },
  {
    "objectID": "blog/2020/02/17/index.html",
    "href": "blog/2020/02/17/index.html",
    "title": "试验设计与方差分析（1）",
    "section": "",
    "text": "完全随机设计\n完全随机设计采用完全随机化的分组方法，将全部试验对象分配到k个处理组，各组分别接受不同的处理，试验结束后比较各组均数之间的差别有无统计学意义，推断处理因素的效应。\n在方差分析中，常称上述的处理因素为因子，用A、B、C等表示因子在试验中所处的不同情况或状态称为水平。\n方差分析解决问题的思路是：从所有观测值的总变异（总方差）中分析出系统误差和随机误差，并用数量表示，在一定意义下比较系统误差和随机误差，若两者的差别不大，说明试验条件的变化（因素水平的不同）对试验结果影响不大；若两者相差较大，且系统误差大得多，说明系统条件变化引出的误差不可忽视。\n\n\n随机区组设计\n随机区组设计是根据局部控制和随机排列的原理进行的：将研究的对象按照不同的性质划分为等于重复次数的区组，例如将试验土地按土壤肥力程度等不同的性质划分为等于重复次数的区组，使区组内环境差异最小，而区组间环境允许存在差异，每个区组即为一次完整的重复，区组内的各处理都独立地随机排列，这是随机排列设计中最常用最基本的设计。\n随机区组设计的优点：富于伸缩性，单因素、复因素以及综合试验等都可应用；能提供无偏的误差估计，在大区域试验中能有效地降低非处理因素等试验条件的单向差异，降低误差；对试验地的地形要求不严，只对每个区组内的非处理因素等试验条件要求尽量一致，因此不同区组可分散设置在不同地段上。\n随机区组设计的缺点：这种设计方法不允许处理数太多，处理数过多，区组必然增大，局部控制的效率会降低，所以处理数一般不超过20个，最好在10个左右。\n随机区组设计考虑了个体差异的影响，可分析处理因素和个体差异对试验效应的影响，所以又称为两因素实验设计，比完全随机设计的检验效率高。\n该设计是将受试对象先按配比条件配成配伍组（如动物试验时，可按同窝别、同性别、体重相近等进行配伍），每个配伍组有3个或以上的受试对象，再按随机化原则分别将各配伍组中的受试对象分配到各个处理组。\n在进行单因素随机区组试验结果的统计分析时，可将处理看作A因素，区组看作B因素，剩余部分则为试验误差。总平方和=区组间平方和+处理间平方和+试验误差平方和，总自由度=区组自由度+处理自由度+误差自由度。"
  },
  {
    "objectID": "blog/2020/02/06/index.html",
    "href": "blog/2020/02/06/index.html",
    "title": "R语言数据处理分析实例",
    "section": "",
    "text": "前言\nR是一种块状结构程序语言，“块”由大括号划分，当“块”只包含一条语句时大括号可以省略。程序语句由换行符或分号分隔。和许多脚本语言一样，R语言不需要“声明”变量。\n接下来我们一起来处理分析一个简单的数据任务。\n实例\n一组学生参加了数学、科学和英语三科考试。为了给所有学生确定一个综合的成绩衡量指标，需要将这些科目的成绩组合起来，将排名在前20%的学生评定为A，接下来20%的学生评定为B，依此类推，最后，按字母顺序对学生进行排序。请试处理分析。\n如下，我们首先创建原始数据框：\n\nstudentname= c(\"John Davis\", \"Angela Williams\", \"Bullwinkle Moose\", \n               \"David Jones\", \"Janice Markhammer\", \"Cheryl Cushing\", \n               \"Reuven Ytzrhak\", \"Greg Knox\", \"Joel England\", \"Mary Rayburn\")\nmath= c(502, 600, 412, 358, 495, 512, 410, 625, 573, 522)\nscience= c(95, 99, 80, 82, 75, 85, 80, 95, 89, 86)\nEnglish= c(25, 22, 18, 15, 20, 28, 15, 30, 27,18)\nscoredata= data.frame(studentname, math, science, English, stringsAsFactors=FALSE)\nscoredata\n\n         studentname math science English\n1         John Davis  502      95      25\n2    Angela Williams  600      99      22\n3   Bullwinkle Moose  412      80      18\n4        David Jones  358      82      15\n5  Janice Markhammer  495      75      20\n6     Cheryl Cushing  512      85      28\n7     Reuven Ytzrhak  410      80      15\n8          Greg Knox  625      95      30\n9       Joel England  573      89      27\n10      Mary Rayburn  522      86      18\n\n\n观察此数据集，马上可以发现一些明显的挑战：首先，三科考试成绩是无法比较的。由于它们的均值和标准差相差很大，对它们求三科的平均成绩是没有意义的。因此，在组合这些考试成绩之前，必须将其转换为可比较的成绩。\n因而，我们做如下处理，将三科成绩进行标准化，这样每科考试成绩都是用单位标准差来表示，而非原始的分值尺度表示：\n\nz= scale(scoredata[2:4])\nz\n\n             math     science     English\n [1,]  0.01269128  1.07806562  0.58685145\n [2,]  1.14336936  1.59143020  0.03667822\n [3,] -1.02568654 -0.84705156 -0.69688609\n [4,] -1.64871324 -0.59036927 -1.24705932\n [5,] -0.06807144 -1.48875728 -0.33010394\n [6,]  0.12806660 -0.20534583  1.13702468\n [7,] -1.04876160 -0.84705156 -1.24705932\n [8,]  1.43180765  1.07806562  1.50380683\n [9,]  0.83185601  0.30801875  0.95363360\n[10,]  0.24344191 -0.07700469 -0.69688609\nattr(,\"scaled:center\")\n   math science English \n  500.9    86.6    21.8 \nattr(,\"scaled:scale\")\n     math   science   English \n86.673654  7.791734  5.452828 \n\n\nscale函数为数据对象按列进行中心化（center=TRUE）或标准化（center=TRUE, scale=TRUE），默认情况下，函数scale对矩阵或数据框的指定列进行均值为0、标准差为1的标准化，给出各科成绩标准化后的结果以及各科成绩的均值和标准差。\n然后，我们通过mean函数来计算各行的均值以获得三科成绩的综合评价得分，并使用cbind函数将其添加到学生成绩的原始数据框中：\n\nScore= apply(z, 1, mean)\nScore\n\n [1]  0.5592028  0.9238259 -0.8565414 -1.1620473 -0.6289776  0.3532485\n [7] -1.0476242  1.3378934  0.6978361 -0.1768163\n\nscoredata2= cbind(scoredata, Score)\nscoredata2\n\n         studentname math science English      Score\n1         John Davis  502      95      25  0.5592028\n2    Angela Williams  600      99      22  0.9238259\n3   Bullwinkle Moose  412      80      18 -0.8565414\n4        David Jones  358      82      15 -1.1620473\n5  Janice Markhammer  495      75      20 -0.6289776\n6     Cheryl Cushing  512      85      28  0.3532485\n7     Reuven Ytzrhak  410      80      15 -1.0476242\n8          Greg Knox  625      95      30  1.3378934\n9       Joel England  573      89      27  0.6978361\n10      Mary Rayburn  522      86      18 -0.1768163\n\n\napply函数可将一个任意函数应用到矩阵、数组、数据框的任何维度上，在矩阵或数据框中，1表示行，2表示列。cbind函数进行列合并，增加列。\n接下来，我们通过函数quantile给出三科成绩综合评价得分的百分位数，将学生的百分位数排名重编码为一个新的类别型成绩变量grade，如下：\n\ny= quantile(Score, c(0.8,0.6,0.4,0.2))\ny\n\n       80%        60%        40%        20% \n 0.7430341  0.4356302 -0.3576808 -0.8947579 \n\nscoredata2$grade[Score &gt;=y[1]]= \"A\"\nscoredata2$grade[Score &lt; y[1] & Score &gt;=y[2]]= \"B\"\nscoredata2$grade[Score &lt; y[2] & Score &gt;=y[3]]= \"C\"\nscoredata2$grade[Score &lt; y[3] & Score &gt;=y[4]]= \"D\"\nscoredata2$grade[Score &lt; y[4]]= \"E\"\nscoredata2\n\n         studentname math science English      Score grade\n1         John Davis  502      95      25  0.5592028     B\n2    Angela Williams  600      99      22  0.9238259     A\n3   Bullwinkle Moose  412      80      18 -0.8565414     D\n4        David Jones  358      82      15 -1.1620473     E\n5  Janice Markhammer  495      75      20 -0.6289776     D\n6     Cheryl Cushing  512      85      28  0.3532485     C\n7     Reuven Ytzrhak  410      80      15 -1.0476242     E\n8          Greg Knox  625      95      30  1.3378934     A\n9       Joel England  573      89      27  0.6978361     B\n10      Mary Rayburn  522      86      18 -0.1768163     C\n\n\nquantile函数求分位数，这里求Score数值型向量的20%、40%、60%和80%分位点。\n接下来，我们使用strsplit函数以空格把学生姓名拆分为姓氏和名字：\n\nname= strsplit(scoredata2$studentname, \" \")\nname\n\n[[1]]\n[1] \"John\"  \"Davis\"\n\n[[2]]\n[1] \"Angela\"   \"Williams\"\n\n[[3]]\n[1] \"Bullwinkle\" \"Moose\"     \n\n[[4]]\n[1] \"David\" \"Jones\"\n\n[[5]]\n[1] \"Janice\"     \"Markhammer\"\n\n[[6]]\n[1] \"Cheryl\"  \"Cushing\"\n\n[[7]]\n[1] \"Reuven\"  \"Ytzrhak\"\n\n[[8]]\n[1] \"Greg\" \"Knox\"\n\n[[9]]\n[1] \"Joel\"    \"England\"\n\n[[10]]\n[1] \"Mary\"    \"Rayburn\"\n\nlastname= sapply(name, \"[\", 2)\nfirstname= sapply(name, \"[\", 1)\nscoredata3= cbind(firstname, lastname, scoredata2[-1])\nscoredata3\n\n    firstname   lastname math science English      Score grade\n1        John      Davis  502      95      25  0.5592028     B\n2      Angela   Williams  600      99      22  0.9238259     A\n3  Bullwinkle      Moose  412      80      18 -0.8565414     D\n4       David      Jones  358      82      15 -1.1620473     E\n5      Janice Markhammer  495      75      20 -0.6289776     D\n6      Cheryl    Cushing  512      85      28  0.3532485     C\n7      Reuven    Ytzrhak  410      80      15 -1.0476242     E\n8        Greg       Knox  625      95      30  1.3378934     A\n9        Joel    England  573      89      27  0.6978361     B\n10       Mary    Rayburn  522      86      18 -0.1768163     C\n\n\nstrsplit函数应用到一个字符串向量上会返回一个列表，使用sapply函数分别提取列表中每个成分的第一个元素和第二个元素，[是一个可以提取某个对象的一部分的函数，再使用cbind函数将它们添加到学生成绩的原始数据中。\n最后，我们使用order函数依姓氏和名字对数据集进行排序，完成这个数据任务。\n\nscoredata4= scoredata3[order(lastname, firstname),]\nscoredata4\n\n    firstname   lastname math science English      Score grade\n6      Cheryl    Cushing  512      85      28  0.3532485     C\n1        John      Davis  502      95      25  0.5592028     B\n9        Joel    England  573      89      27  0.6978361     B\n4       David      Jones  358      82      15 -1.1620473     E\n8        Greg       Knox  625      95      30  1.3378934     A\n5      Janice Markhammer  495      75      20 -0.6289776     D\n3  Bullwinkle      Moose  412      80      18 -0.8565414     D\n10       Mary    Rayburn  522      86      18 -0.1768163     C\n2      Angela   Williams  600      99      22  0.9238259     A\n7      Reuven    Ytzrhak  410      80      15 -1.0476242     E"
  },
  {
    "objectID": "blog/2020/02/04/index.html",
    "href": "blog/2020/02/04/index.html",
    "title": "样本量和检验效能的估计问题",
    "section": "",
    "text": "统计分析人员经常会被问到这样一个问题：我这个研究到底需要多少个研究对象呢？\n这个问题可以通过检验效能分析或样本量估算来解决，它在实验设计中占有重要地位。检验效能分析可以帮助在给定置信度的情况下，判断检测到给定效应值时所需的样本量；反过来，它也能够在给定置信度水平情况下，计算在某样本量内能检测到给定效应值的概率，如果该概率过低，可以考虑修改或放弃该实验。\n由于检验效能分析针对的是假设检验，我们回顾下假设检验的过程。\n在统计假设检验中，首先要对总体分布参数作出一个假设（无效假设），然后从总体分布中抽样，通过样本计算所得的统计量来对总体参数进行推断。假定无效假设为真，若计算获得观测样本的统计量或更大统计量的概率（p值）非常小，小于预先设定的阈值（检验的显著性水平），便可以拒绝无效假设，接受备择假设。\n科学研究中，越来越强调样本量的估算。确定适当的样本含量可以节约资源，并可防止因为样本含量过少引起的检验效能偏低，出现假阴性错误，这是当前医学研究中值得注意的问题。\n样本量的估算方法很多，不同的统计检验方法使用的计算公式也不一样。一般影响样本量的因素有以下7种：\n\n研究事件的发生率：研究事件预期的发生率越高，所需的样本量越小，反之则越大；\n研究因素的有效率：有效率越高，即实验组和对照组比较数值差异越大，样本量就可以越小，使用小样本就能够达到统计学上的显著性，反之则越大；\n设定假设检验的I类错误概率α，即检验水准或显著性，为假阳性错误出现的概率。α越小，所需的样本量越大，反之则越小。α水平由研究者根据具体情况决定，通常α取0.05或0.01；\n设定假设检验的II类错误概率β，或检验效能1-β。II类错误为假阴性错误，即在特定的α水准下，若总体参数之间确实存在着差别，此时该次实验能发现此差别的概率。检验效能又称把握度，即避免假阴性错误的能力，β越小，检验效能越高，所需的样本量越大，反之就越小。β水平由研究者根据情况决定，通常取β为0.2、0.1或0.05，即1-β=0.8、0.9或0.95，也就是说把握度为80%、90%或95%；\n了解由样本推断总体的一些信息。总体标准差一般未知，可用样本标准差代替；\n处理组间差别的估计，即确定容许误差。容许误差越小，需要的样本量越大；\n采用统计学检验时，当研究结果高于和低于效应指标的界限均有意义时，应该选择双侧检验，所需的样本量就大。当研究结果仅高于或低于效应指标的界限有意义时，则应该选择单侧检验，所需的样本量就小。\n\n在这些影响因素中，确定样本含量最重要的4个因素为I类错误概率、II类错误概率、推断总体的一些信息和容许误差。\n研究者放宽显著性水平时（换句话说，使得拒绝无效假设更容易时），检验功效增加。类似地，样本量增加，检验功效增加。\n通常来说，研究目标是维持一个可接受的显著性水平，尽量使用较少的样本，然后最大化统计检验的功效，也就是说，最大化发现真实效应的概率，并最小化发现错误效应的概率，同时把研究成本控制在合理的范围内。"
  },
  {
    "objectID": "blog/2020/02/01/index.html",
    "href": "blog/2020/02/01/index.html",
    "title": "R语言基础–数据的输入",
    "section": "",
    "text": "前言\nR可从键盘、文本文件、Excel、流行的统计软件、特殊格式的文件，以及多种关系型数据库中导入数据。\n\n\n键盘输入数据\nR中的函数edit()会自动调用一个允许手动输入数据的文本编辑器，步骤如下：\n\n创建一个空数据框(或矩阵)，其中变量名和变量的模式需与预期的最终数据集一致；\n针对这个数据对象调用文本编辑器，输入数据，并将结果保存回此数据对象中。\n\n\n\n\nexample\n\n\n类似于age=numeric(0)的赋值语句创建一个指定模式但不含实际数据的变量。编辑的结果需要赋值回对象本身，函数edit()事实上是在对象的一个副本上进行操作的，如果不将其赋值到一个目标，所有修改会全部丢失。单击列的标题，可以用编辑器修改变量名和变量类型（数值型、字符型），可通过单击未使用列的标题来添加新的变量。mydata= edit(mydata)的更简洁的等价写法是fix(mydata)。\n\n\n从带分隔符的文本文件导入数据\n可以使用read.table()函数，此函数可读入一个表格格式的文件并将其保存为一个数据框，语法如下：\ndata= read.table(file,header = TRUE,sep= \"delimiter\",row.names='name')\n其中，file是一个带分隔符的ASCII文本文件，header表示文件是否在首行包含变量名，sep指定分隔行内数据的分隔符，row.names指定一个或多个行标记符（指定某变量为行名，该列即不再有标签，导致数据会少一列）。\n默认情况下，字符型变量将转换为因子。设置选项stringsAsFactors=FALSE，将停止对所有字符型变量的转换，或者使用选项colClasses指定每一列的类，如logical（逻辑型）、numeric（数值型）、characer（字符型）、factor（因子）。\n\n\n导入Excel数据\n读取一个Excel文件可以在Excel中将其导出为一个逗号分隔文件csv，再使用前文描述的方式将其导入R中。或者可以使用xlsx包，函数read.xlsx()导入一个工作表到一个数据框中：\nread.xlsx(file, sheetIndex, sheetName, rowIndex)\nread.xlsx()允许指定工作表中特定的行（rowIndex）和列（colIndex），配合上对应每一列的类（colClasses）。对于大型的工作簿，可以使用函数read.xlsx2()，该函数用Java来运行更加多的处理过程，可获得可观的质量提升。"
  },
  {
    "objectID": "blog/2020/01/30/index.html",
    "href": "blog/2020/01/30/index.html",
    "title": "R语言基础–函数",
    "section": "",
    "text": "前言\nR中作为数据处理基石的函数，可分为数值（数学、统计、概率）函数和字符处理函数。\n\n\n数值函数\n对数据做变换是数学函数的一个主要用途。数学函数也被用作公式中的一部分，用于绘图函数和在输出结果之前对数值做格式化。\n统计函数我们在进行统计学方法分析时一定会用到。\n概率函数通常用来生成特征已知的模拟数据，以及在用户编写的统计函数中计算概率值。\n\n\n字符处理函数\n数学和统计函数是用来处理数值型数据的，而字符处理函数可以从文本型数据中抽取信息，或者为打印输出和生成报告重设文本的格式。\n这里我们不举例子，大家可以下载下图所示的一份文档，可以打印出来，随时查阅。\n\n\n\ncheat sheet"
  },
  {
    "objectID": "blog/2020/01/28/index.html",
    "href": "blog/2020/01/28/index.html",
    "title": "R语言基础–因子",
    "section": "",
    "text": "因子\n因子在R中非常重要，因为它决定了数据的分析方式以及如何进行结果展示。因子也在R中具有许多强大运算的基础，包括许多针对表格数据的运算。因子的设计思想来源于统计学中的名义变量或分类变量，这些变量本质上不是数字，而是对应分类。例如血型，尽管可以用数字对它们进行编码。\n变量可分为名义型(无序分类变量)、有序型(表示顺序而非数量关系)和连续型变量。连续型变量可以呈现某个范围内的任意值，同时表示顺序和数量，如年龄是一个连续型变量。R中名义型变量和有序型变量称为因子。\n函数factor()以一个整数向量的形式存储类别值，同时一个由字符串（原始值）组成的内部向量将映射到这些整数上，如：\n\n\n\nfactor\n\n\n将此向量存储为(1, 2, 1, 1)，并在内部将其关联为1=Type1和2=Type2（具体赋值根据字母顺序决定）。针对向量diabetes进行的任何分析都会将其视为名义型变量并自动选择合适的统计方法。\n在R中，因子可以简单地看作一个附加更多信息的向量（尽管它们内部机理是不同的）。这额外的信息包括向量中不同值的记录，我们称之为“水平”。\n要表示有序型变量，需要为函数factor()指定参数ordered=TRUE，如：\n\n\n\nordered-factor\n\n\n此时顺序为’Excellent’‘Improved’‘Poor’（对于字符型向量，因子的水平默认依字母顺序创建），这里恰好与逻辑顺序一致。若不一致，可以通过指定levels选项覆盖默认排序：\n\n\n\nordered-factor-levels"
  },
  {
    "objectID": "blog/2020/01/22/index.html",
    "href": "blog/2020/01/22/index.html",
    "title": "R语言编程入门",
    "section": "",
    "text": "前言\n类似其他计算机高级语言，R用户只需要熟悉其命令、语句及简单的语法规则，就可以做数据管理和分析处理工作。R把大部分常用的复杂数据计算的算法作为标准函数调用，用户仅需要指出函数名及其必要的参数即可，这一特点使得R编程十分简单。\nR是面向对象的、区分大小写的解释型数组编程语言，输入后可直接给出结果。R中功能靠函数实现。R的函数分为“高级”和“低级”函数，高级函数可调用低级函数，这里的高级函数习惯上称为泛型函数。plot()就是泛型函数，可以根据数据的类型调用底层的函数，应用相应的方法绘制相应的图形。这就是面向对象编程的思想。\n\n\n数据集\n创建含有研究信息的数据集，这是任何数据分析的第一步。数据集通常是由数据构成的一个矩形数组，行表示观测，列表是变量。\nR可以处理的数据类型包括数值型（如100）、字符型（如“流光相约”）、逻辑型（TRUE/FALSE）、复数型（如2+3i）和因子型（表示不同类别）。R中有许多用于存储数据的结构，包括标量、向量、矩阵、数组、数据框和列表。多样化数据结构赋予了R极其灵活的数据处理能力。\n\n\n标量与向量\n标量可以看成是只含一个元素的向量，用于保存常量，如f=3、g=’US’和h=TRUE。\n向量是一系列元素的组合，用于存储数值型、字符型或逻辑型数据的一维数组。执行组合功能的函数c()可用来创建向量：\n\n\n\nvector\n\n\n提示：单个向量中的数据必须是相同的类型或模式（数值型、字符型或逻辑型），同一向量中不可混杂不同类型数据。\n通过在方括号中给定元素所处位置的数值访问向量中的元素，如a[c(2, 4)]用于访问向量a中第2个和第4个元素。\n\n\n矩阵\n矩阵是一个二维数组，和向量类似，其中元素必须类型相同，即一个矩阵中只能包含一种数据类型（数值型、字符型或逻辑型），可通过函数matrix()创建矩阵：\nmymatrix= matrix(vector,nrow = ,ncol = ,byrow = ,dimnames = list())\n其中，vector包含了矩阵的元素，nrow和ncol用于制定行和列的维数，dimnames包含了可选的、以字符型向量表示的行名和列名，byrow表明矩阵应当按行填充（byrow=T）还是按列填充（byrow=F），默认按列填充。\n\n\n\nmatrix\n\n\n我们可以使用下标和方括号来选择矩阵中的行、列或元素。X[i,]表示矩阵X中的第i行，X[,j]表示第j列，X[i, j]表示第i行第j个元素。\n\n\n数组\n矩阵都是二维的，仅能包含一种数据类型，当维度超过2时，需要使用数组，可通过函数array()创建：\nmyarray= array(vector,dimensions,dimnames)\n其中，vector包含了数组中的数据，dimensions是一个数值型向量，给出了各个维度下标的最大值，dimnames是可选的、各维度名称标签的列表。\n\n\n\narray\n\n\n数组是矩阵的一个自然推广，从数组中选取元素的方式与矩阵相同。\n\n\n数据框\n与通常在SAS、SPSS和STATA中看到的数据集类似，不同的列可以包含不同类型的数据。数据框是R中最常处理的数据结构，可使用函数data.frame()创建：\nmydata=data.frame(col1,col2,col3,...)\n其中，列向量col1，col2，col3等可谓任何类型（如字符型、数值型或逻辑型）。\n选取数据框中元素的方式比较多，既可以使用下标记号，也可以直接指定列名，如patientdata\\(age，其中\\)被用来选取一个给定数据框中的某个特定变量。\n在每个变量名前都输入一次数据框名可能十分麻烦，我们可以联合使用函数attach()和detach()，或单独使用函数with()来简化代码。函数attach()将数据框添加到R的搜索路径中，R在遇到一个变量名后，将检查搜索路径中的数据框，以定位到这个变量，如\n\n\n\ndataframe\n\n\n函数detach()将数据框从搜索路径中移除，不会影响数据框本身。注意：函数attach()和detach()最好是分析一个单独的数据框，并且不太可能有多个同名对象。当同名时，原始对象将进行优先运算。另一种方式是使用函数with()，如\n\n\n\nwith function\n\n\n花括号{}之间的语句都针对该数据框执行，无需担心名称冲突；若仅有一条语句，花括号可以省略。函数with()的局限性在于赋值仅在此函数的括号内有效。若需要创建在with()结构外依然存在的对象，使用特殊赋值符号 -&gt;&gt; ，即可保存对象到全局环境中。\n\n\n列表\n列表是一些对象（成分）的有序集合，允许整合若干对象到单个对象名下，其中的对象可以是任何数据结构，如某个列表可以是若干向量、矩阵、数据框甚至其他列表的组合，使用函数list()创建列表：\nmylist=list(object1,object2,...)\n可以通过在双重方括号中指明代表某个成分的数字或名称来访问列表中的元素，如mylist[[4]]。由于列表允许以一种简单的方式组织和重新调用可能不相干的信息，且许多R函数的运行结果都是以列表的形式返回的，由分析人员决定需要取出其中哪些成分，列表是R中的重要数据结构。"
  },
  {
    "objectID": "blog/2020/01/20/index.html",
    "href": "blog/2020/01/20/index.html",
    "title": "计算机概论3",
    "section": "",
    "text": "内存\n前面提到CPU所使用的数据都是来自内存（Main Memory），不论是软件程序还是文件数据，都必须要读入内存后CPU才能利用。个人电脑的内存主要组件为动态随机存取内存（Dynamic Random Access Memory，DRAM），随机存取内存只有在通电时才能记录与使用，断电之后数据就消失，因此我们也称这种RAM为挥发性内存。\nDRAM根据技术的更新又分好几代，而使用上较广泛的有所谓的SDRAM与DDR SDRAM两种。新一代的PC大多使用DDR内存。\n在某种意义上，内存的容量有时比CPU的速度还要重要。如果内存容量不够大的话将会导致某些大容量数据无法被完整地加载，此时已存在内存当中但暂时没有被使用到的数据就必须要先被释放，使得可用内存容量大于该数据，那份新数据才能够被加载。所以。通常越大的内存代表越快速的系统，这是因为系统不用常常释放一些内存中的数据。\n\n\nCPU的二级高速缓存\n除了内存外，个人电脑中还有许多类似内存的存储结构存在，最为我们所熟知的还有CPU内的二级高速缓存。我们现在知道CPU的数据都由内存提供，但CPU到内存之间还是得要通过内存控制器。如果某些很常用的程序或数据可以放置到CPU内存的话，那么CPU数据的读取就不需要跑到内存重新读取，这对于性能来说是一个很大的提升，这就是二级缓存的设计理念。新一代的CPU都有内置容量不等的L2缓存在CPU内部，以加快CPU的运行性能。\n\n\n只读存储器\n还记得你的电脑在开机的时候可以按下[Del]按键来进入一个名为BIOS（Basic Input Output System）的界面吧？BIOS是一个程序，这个程序是写死到主板上面的一个存储芯片中的，这个存储芯片在没有通电时也能够记录数据，这就是只读存储器（Read Only Memory，ROM）。ROM是一种非易失性的存储。\nBIOS对于个人电脑来说是非常重要的，它是系统在启动的时候首先会去读取的一个小程序。另外，固件（firmware）很多也是使用ROM来进行软件的写入（固件：固定在硬件上面的控制软件）。BIOS就是个固件，控制着启动时各项硬件参数的获取与启动设备的选择。"
  },
  {
    "objectID": "blog/2020/01/18/index.html",
    "href": "blog/2020/01/18/index.html",
    "title": "Python正则表达式与模式匹配",
    "section": "",
    "text": "很多商业分析都依赖模式匹配，也称为正则表达式（regular expression）。举例来说，我们可能需要分析一下来自深圳的所有订单。此时，你需要识别的模式就是“深圳”这个词。同样，你可能需要分析来自某个供应商的商品质量，此时你要识别的模式就是供应商的名字。\nPython包含了re模块，它提供了在文本中搜索特定模式/正则表达式的强大功能。要在脚本中使用re模块提供的功能，我们需要在脚本上方加入代码import re。\n导入re模块后，可以使用一大波函数和元字符来创建和搜索任意复杂的模式。元字符（metacharacter）是正则表达式中具有特殊意义的字符，使正则表达式能够匹配特定的字符串。\n常用的元字符包括 |、()、[]、.、*、+、?、^、$和(?P&lt;name&gt;)。如果你在正则表达式中见到这些字符，要知道程序不是要搜索这些字符本身，而是要搜索它们要描述的东西。\nre模块包含了很多有用的函数，用于创建和搜索特定的模式。一起来看一个示例代码：\n\n\n\nsample code\n\n\n第一行赋值字符串变量string，下一行将字符串拆分为列表，列表中的每个元素都是一个单词。使用re.compile和re.I函数以及用r表示的原始字符串，创建一个名为pattern的正则表达式。re.compile函数将文本形式的模式编译成为正则表达式（正则表达式不是必须编译的，但编译是个好习惯，因为这样可以显著地提高程序运行速度），re.I函数确保模式是不区分大小写的，即能同时在字符串中匹配“The”和“the”，原始字符串标志r可确保Python不处理字符串中的转义字符（如、），这样在进行模式匹配是，字符串中的转义字符和正则表达式中的元字符就不会有意外的冲突。利用for循环在列表变量sring_list的各个元素之间进行迭代，取出列表中所有的单词，使用re.search函数将每个单词与正则表达式进行比较，如果相匹配，那么count的值就加1。print语句打印出正则表达式在字符串汇总找到模式“The”（不区分大小写）的次数。\n再看另一个示例：\n\n\n\nsample code 2\n\n\n这个示例想要打印出相匹配的字符串，而不是相匹配的次数，这里使用到了(?P&lt;name&gt;)元字符和group函数。(?P&lt;name&gt;)元字符使匹配的字符串可以在后面的程序中通过组名符号name来引用，这里称为match_word。后面if语句中使用了group函数获取分段截获的字符串，如果相匹配，那么就在search函数返回的数据结构中找出match_word组合中的值，并打印出来。\n最后一个示例：\n\n\n\nsample code 3\n\n\n我们演示了使用re.sub函数在文本中用一种模式替换另一种模式。将正则表达式赋给变量string_to_find不是必需的，但若正则表达式特别长或复杂的话，将它赋给一个变量，然后传入re.compile函数有助于理解。最后使用re.sub函数以不区分大小写的方式在变量string中寻找模式，将发现的每个模式替换成a。"
  },
  {
    "objectID": "blog/2020/01/16/index.html",
    "href": "blog/2020/01/16/index.html",
    "title": "R语言的初体验",
    "section": "",
    "text": "R语言是从起源于贝尔实验室的S统计绘图语言演变而来的。与S语言类似，R也是一种为统计计算和绘图而生的语言和环境，它是一套开源的数据分析解决方案，由一个庞大且活跃的全球性研究型社区维护。\n\nR的特点总结\n\n软件本身及程序包的源代码公开；\n涵盖了多种行业数据分析中几乎所有的方法；\n任意一个分析步骤的结果均可被轻松保存、操作，并作为进一步分析的输入；\nR拥有顶尖水准的制图功能；\nR可运行于多种平台上，包括Windows、UNIX和Mac OS X；\n可轻松地从各种类型的数据源读写数据，包括文本文件、数据库管理系统、统计软件，乃至专门的数据仓库；\n每个函数都有统一格式的帮助和运行实例。\n\n\n\nR的帮助系统\nR提供了大量的帮助功能，学会如何使用这些帮助文档有助于编程。R的内置帮助系统提供了当前已安装包中所有函数的细节、参考文献以及使用示例。帮助文档可以通过以下函数进行查看。\n\nhelp.start()：打开帮助文档首页\nhelp(foo)或?foo：查看函数foo的描述说明等帮助信息(如返回值)\nhelp.search(‘foo’)或??foo：以foo为关键词搜索本地帮助文档\nRSiteSearch(‘foo’)：以foo为关键词搜索在线文档和邮件列表存档\napropos(‘foo’, mode=‘function’)：列出名称中含有foo的所有可用函数，在只知道函数的部分名称时搜索可用\nexample(foo)：查看函数foo的使用范例\ndata() 列出当前已加载包中所含的所有可用示例数据集\nvignette() 列出当前已安装包中所有可用的vignette文档\nvignette(‘foo’) 为主题foo显示指定的vignette文档\n\n\n\n工作空间和目录\n工作空间（workspace）是当前R的工作环境，存储着所有你定义的对象（向量、矩阵、函数、数据框和列表）。在一个R会话结束时，你可以将当前工作空间保存到一个镜像中，以便在下次启动R时自动载入它。当前的工作目录（working directory）是R用来读取文件和保存结果的默认目录。\n用于管理工作空间和目录的部分标准命令如下：\n\ngetwd()：查看当前工作目录\nsetwd()：重新设定当前工作目录。如果需要读入一个不在当前工作目录下的文件，需要在调用语句中写明文件的完整路径。setwd()命令的路径中使用正斜杠/。R将反斜杠。即使在Windows平台上运行R，在路径中也要使用正斜杠。\nls()：列出当前工作空间中的对象\nrm(objectlist)：删除一个或多个对象\noptions()：显示或设置当前选项\nhistory(#)：显示最近使用的#个命令(默认值为25)\nsavehistory(‘myfile’) 保存命令历史到文件myfile.Rhistory中\nloadhistory(‘myfile’) 载入命令历史文件myfile.Rhistory\nsave.image(‘myfile’) 保存工作空间到文件myfile.RData中\nload(‘myfile’) 读取工作空间myfile.RData到当前会话中\nsave(objectlist, file=‘myfile’) 保存指定对象到一个文件中\nq()：结束对话退出R，并询问是否保存工作空间\n\n\n\nR包\nR提供了大量备用功能，通过可选模块的下载和安装来实现。目前有15364个称为包的用户贡献模块可从https://cran.r-project.org/web/packages下载。这些包提供了横跨各种领域、数量庞大的功能，包括分析地理数据、处理蛋白质质谱，甚至是心理测验分析的功能。\nR包是R函数、数据、预编译代码以一种定义完善的格式组成的集合，具有详细的说明和示例。计算机上存储包的目录称为库（library）。.libPaths()显示库所在位置，library()则可以显示库中包。\n第一次安装一个包，使用命令install.packages()即可，在括号中输入要安装的包名称，一个包仅需安装一次。update.packages()更新已安装的包。installed.packages()列出已安装的包的相关信息(如版本号、依赖关系等)。Windows下的R包是经过编译的zip文件，安装时不要解压缩。安装路径为“Pacakges&gt;install packages from local files”，选择本地磁盘上存储zip包的文件夹。\n包的安装是指从某个CRAN镜像站点下载它并将其放入库中的过程。安装好以后，必须被载入到会话中才能使用包，需要使用library()函数载入该包。在一次应用中，包只需载入一次，如果需要，我们可以自定义启动环境以自动载入会频繁使用的包。search()显示已加载并可使用的包。help(‘package_name’)输出某个包的简短描述以及包中可用的函数名称和数据集名称的列表，help()查看包中任意函数或数据集的描述，R的帮助系统包含了每个函数的一个描述（同时带有示例），每个数据集的信息也被包括其中。\n\n\nR的使用\nR是面向对象的，区分大小写的解释型数组编程语言。R中多数功能是由程序内置函数、用户自编函数和对对象的创建和操作所实现的。一次交互式会话期间的所有数据对象都被保存在内存中。R语句由函数和赋值构成，R使用 -&gt; 而非 = 作为赋值符号。R也允许使用 = 为对象赋值，但是它不是标准语法，某些情况下会出现问题。R具有完备的数据存取、管理、分析和显示等功能，将数据处理和统计分析融为一体。以后我们继续学习R语言。"
  },
  {
    "objectID": "blog/2020/01/10/index.html",
    "href": "blog/2020/01/10/index.html",
    "title": "Python基础要素之字符串",
    "section": "",
    "text": "字符串是Python中的另一种基本数据类型。它通常是指人类可以阅读的文本，但更广泛地说，它是一个字符序列，并且字符只有在组成这个序列时才有意义。一些对象看上去是数值，但实际上是字符串，比如邮政编码，你对邮政编码做加减乘除是没有意义的，所以最好在代码中将其作为字符串来处理。接下来介绍用于字符串管理的一些模块、函数和操作。\n\n字符串\n字符串可以包含在单引号、双引号、3个单引号或3个双引号之间：\n\n\n\n字符串\n\n\nOutput #14展示了一个包含在单引号之间的简单字符串。\n\n\n+、*、len()\n\n\n\noperator\n\n\nOutput #18展示了使用+操作符将两个字符串相加。+操作符按照原样相加，如果你想在结果字符串中留出空格的话，就必须在原字符串中加上空格（Output #18在字母a后加了空格），Output #19中的*操作符也是这样，其将字符串重复一定的次数。Output #20展示了使用内置函数len来确定字符串中字符的数量。len函数将空格与标点符号都计入字符串长度，所以结果是23个字符。\n\n\nsplit函数\n使用split函数将一个字符串拆分成一个子字符串列表（列表中的子字符串正好可以构成原字符串）。split函数可以在括号中使用两个附加参数，第一个附加参数表示使用哪个字符进行拆分，第二个参数表示进行拆分的次数。\n\n\n\nsplit\n\n\n在Output #21中，括号中没有附加参数，所以split函数使用空格字符（默认值）对字符串进行拆分。因为这个字符串有5个空格，所以被拆分成具有6个子字符串的列表。Output #22中，第一个附加参数是用空格来拆分字符串，第二个附加参数是2，说明只想用前两个空格进行拆分，生成一个带有3个元素的列表。第二个参数会在我们解析数据的时候派上用场。举例来说，你可能会解析一个日志文件，文件中包含时间戳、错误代码和由空格分隔的错误信息。在这种情况下，可以使用前两个空格进行拆分，解析出时间戳和错误代码，但是不使用剩下的空格进行拆分，以便完整地保留错误信息。Output #23和Output #24中，括号中的附加参数是都好，split函数在出现逗号的位置拆分字符串。\n\n\njoin函数\n使用join函数将列表中的子字符串组合成一个字符串。join函数将一个参数放在join前面，表示使用这个字符（或字符串）在子字符串之间进行组合。print(\"Output #25: {0}\".format(','.join(string2_list)))这里join函数将子字符串组合成一个字符串，子字符串之间为逗号。因为列表中有6个子字符串，所以组合后有5个逗号。\n\n\nstrip函数\n使用strip、lstrip和rstrip函数从字符串两端删除不想要的字符，这三个函数都可以在括号中使用一个附加参数来设定要从字符串两端删除的字符（或字符串）。\n\n\n\nstrip\n\n\n可以看到，string3的左侧有几个空格，右侧包含制表符（、几个空格和换行符（）。在Output #26中，你会看到句子前面有空白，句子下面有一个空行，句子后面有你看不到的制表符和空格。{0:s}中的s表示传入的值应该格式化为一个字符串。下面展示了从字符串两端删除其他字符的方法，将要删除的字符作为strip函数的附加参数即可：通过将美元符号、下划线、短划线和加号作为附加参数，通知程序从字符串两端删除它们。\n\n\nreplace函数\n使用replace函数将字符串中的一个或一组字符替换为另一个或另一组字符。replace函数在括号中使用两个附加参数，第一个参数作用是在字符串中查找要替换的字符或一组字符，第二个参数是要用来替换的一个或一组字符。\n\n\n\nreplace\n\n\nOutput #32展示了使用replace函数将字符串中的空格替换为!@!。Output #33展示了使用逗号替换字符串中的空格。\n\n\nlower、upper、capitalize函数\nlower和upper函数分别用来将字符串中的字母转换为小写和大写，capitalize函数对字符串中的第一个字母应用upper函数，对其余字母应用lower函数：\n\n\n\nlower-upper-capitalize\n\n\nOutput #36对句子的首字母大写。Output #37将capitalize函数放在一个for循环中，对string8_list这个列表中的每个元素首字母大写，其余字母小写。"
  },
  {
    "objectID": "blog/2020/01/08/index.html",
    "href": "blog/2020/01/08/index.html",
    "title": "如何用Python自编k-近邻算法？",
    "section": "",
    "text": "k-近邻算法概述\n简单地说，kNN依据不同特征值之间的距离进行分类。它不具有显式的学习过程，实际上是利用训练数据集对特征向量空间进行划分，并作为其分类的模型，即我们知道训练集中每一数据与所属分类的对应关系，输入没有标签的新数据后，将新数据的每个特征与训练集中数据对应的特征进行比较，然后算法提取训练集中特征最为相似数据的分类标签，选择k个最相似数据中出现次数最多的类别作为新数据的分类。\n\n\n自编kNN函数\n\n\n\nkNN\n\n\nclassify() 函数有 4 个输入参数：待分类的输入向量 inX ，训练集 dataSet，训练集标签向量 labels，参数 k 为选择最近数据点个数，其中，inX 维度为 1*N，dataSet 维度为 M*N，labels 维度为 1*M，k 为奇数。\ndataSet.shape 以元组形式返回训练集维度 (M, N)，dataSetsize 为训练集的样本个数 M。这里距离度量采用欧式距离，因而 tile 函数将输入数据重复 M 行 1 列（从而与训练集维度相同），分别和训练集中的每个数据点对应特征相减再平方，再按行相加，不保持其二维特性，即得输入数据与训练集中每个数据点之间的欧式距离。\n计算完距离之后，argsort 函数对距离按照从小到大的次序排列，并返回排序后对应的原始索引值。使用 for 循环确定前 k 个距离最小元素所属的类别 voteIlabel，使用 get 函数按照字典 classCount 键值取得相应的字典值，如果字典中存在这个键，get 函数就返回对应的字典值，如果不存在，则返回 0，用这种方式计数 k 个数据中每个标签出现的次数。因而字典 classCount 中键值为标签，字典值为 k 个标签对应的个数。\n使用 sorted 函数对字典 classCount 进行排序：items 函数同时引用字典的键和值，结果是一个列表，其中包含的是键-值对形式的元组。由于字典没有隐含排序，我们可以按照字典的键或字典值来排序，这里的 key 就是排序的规则，关键字函数设置用于排序的关键字。使用 operator 模块中的 itemgetter 函数对列表按照每个元组第二个索引位置（即字典值，标签个数）进行排序，reverse=True 对应降序。所以最后返回 sortedClassCount 列表中第一个元组的第一个值，即在 k 个标签中出现次数最多的标签，这样即完成了一个简单的 kNN 算法。\n\n\n创建训练集\n\n\n\n训练集\n\n\n我们创建了一个简单的训练集，有 4 组数据，每组数据有两个我们已知的属性/特征值。向量 labels 包含了每个数据的标签信息，labels 包含的元素个数等于 group 矩阵行数。\n\n\n运行kNN\n我们需要在脚本中导入两个模块 NumPy 和运算符 operator（kNN 执行排序操作时使用到）：\n\n\n\nimport modules\n\n\n保存脚本文件，改变当前路径到存储脚本文件位置，进入 Python：\n\n\n\nrun kNN\n\n\n输出结果应该是 B，也可以改变输入数据运行。这样，我们已经构造了一个简单的 kNN 分类器。"
  },
  {
    "objectID": "blog/2020/01/06/index.html",
    "href": "blog/2020/01/06/index.html",
    "title": "Python基础要素之数值",
    "section": "",
    "text": "前面我们已经了解了如何创建、运行脚本，接下来我们了解下Python中最常用的数据类型。\n\n数值\nPython中最主要的4种数值类型分别是整数、浮点数、长整数和复数，这里只介绍整数和浮点数（即带小数点的数）。\n整数：\n\nx = 9\nprint(\"Output #4: {0}\".format(x))\nprint(\"Output #5: {0}\".format(3**4))\nprint(\"Output #6: {0}\".format(int(8.3)/int(2.7)))\n\nOutput #4: 9\nOutput #5: 81\nOutput #6: 4.0\n\n\nOutput #6演示了将数值转换成整数并进行除法运算。\n浮点数：\n\nprint(\"Output #7: {0:.3f}\".format(8.3/2.7))\ny = 2.5*4.8\nprint(\"Output #8: {0:.1f}\".format(y))\nr = 8/float(3)\nprint(\"Output #9: {0:.2f}\".format(r))\nprint(\"Output #10: {0:.4f}\".format(8.0/3))\n\nOutput #7: 3.074\nOutput #8: 12.0\nOutput #9: 2.67\nOutput #10: 2.6667\n\n\n\nfrom math import exp, log, sqrt\nprint(\"Output #11: {0:.4f}\".format(exp(3)))\nprint(\"Output #12: {0:.2f}\".format(log(4)))\nprint(\"Output #13: {0:.1f}\".format(sqrt(81)))\n\nOutput #11: 20.0855\nOutput #12: 1.39\nOutput #13: 9.0\n\n\n\n\n.format 格式化\n\n# Add two numbers together\nx = 4\ny = 5\nz = x + y\nprint(\"Output #2: Four plus five equals {0:d}.\".format(z))\n# Add two lists together\na = [1, 2, 3, 4]\nb = [\"first\", \"second\", \"third\", \"fourth\"]\nc = a + b\nprint(\"Output #3: {0}, {1}, {2}\".format(a, b, c))\n\nOutput #2: Four plus five equals 9.\nOutput #3: [1, 2, 3, 4], ['first', 'second', 'third', 'fourth'], [1, 2, 3, 4, 'first', 'second', 'third', 'fourth']\n\n\n\n\ntype函数\nPython提供一个名为 type 的函数，可以对所有对象调用这个函数，来获得关于Python如何处理这个对象的更多信息。\n函数的语法非常简单：type(variable) 会返回Python中的数据类型。如果你对一个数值变量调用这个函数，它会告诉你这个数值是整数还是浮点数，还会告诉你这个数值是否能当作字符串进行处理。\n此外，由于Python同样是面向对象的语言，所以你可以对Python中所有命名对象调用 type 函数，不仅是变量，还有函数、语句等。"
  },
  {
    "objectID": "blog/2020/01/04/index.html",
    "href": "blog/2020/01/04/index.html",
    "title": "Python初体验",
    "section": "",
    "text": "目前Python这门语言有多火也不用多说，各种公众号推送制造的焦虑让你感觉实在不学不行，接下来我们就来体验一下别人口中的这门似乎很神奇的编程语言。\n\n创建Python脚本\nPython的安装这里就不说了，当然如果你是mac用户，恭喜你的笔记本自带了Python2（前几天官方已停止对2的更新了）。如何在Python shell中简单地运行代码呢？Windows用户打开命令行窗口，mac用户打开终端，输入 python3，按下回车键，就能看见Python提示符 &gt;&gt;&gt;：\n\n\n\nPython提示符\n\n\n面对复杂多代码的任务，我们需要把代码都写在Python脚本上，然后运行脚本 ，提高工作效率。我们可以选择一个自己喜欢的文本编辑器，可供选择的有很多：Spyder、Pycharm、Jupyter notebook、Visual Studio code等。打开编辑器，一般我们将 #!/usr/bin/env python3 作为第一行。以井号开头的代码行为注释行，Windows系统不读取也不执行该行代码，但是像macOS这样的基于Unix的系统会根据这一行来找到执行该脚本的Python版本，加入这一行可以使你的脚本在不同操作系统之间具有可移植性。我们将上面这俩行代码放到Pycharm中，保存为 first-script.py 文件，这就是一个简单的Python脚本了。\n\n\n运行Python脚本\n对于在编辑器内运行，编辑器会有一个绿色三角运行按钮，点击一下即可运行输出：\n\n\n\nPycharm运行按钮\n\n\n当然，我们也可以选择在命令行或者终端中运行脚本：打开命令行或者终端，提示符会是一个具体的文件夹，即目录，如mac：/Users/luzhen。我们将脚本保存在桌面上，同时在终端中切换到桌面目录：\n\n\n\n终端切换目录\n\n\nmac上下一步就是为脚本添加执行权限，输入命令：chmod +x first-script.py。chmod是一个Unix命令，表示改变访问权限（change access mode）。+x 表示在访问设置中添加执行权限，而非读、写权限。这样Python就可以执行脚本了。mac上只要你在一个脚本上运行了chmod命令，以后就可以随意运行该脚本，无需第二次执行chmod命令。\n接下来就可以运行脚本了：\n\n\n\n终端运行脚本\n\n\n可以看到终端窗口已经完成了脚本输出的打印，脚本运行成功！当然，Windows上还有一种运行方法，直接输入 python3 first-script.py，也可成功执行脚本，mac上同样适用。\n\n\n与命令行交互的几个小技巧\n\n使用向上箭头键得到以前的命令\n在命令行和终端窗口中，你可以通过按向上箭头键找到以前输入的命令，可以减少每次运行Python脚本时必需的输入量，特别是当Python脚本的文件名特别长或需要在命令行上提供额外的参数（比如输入文件名或输出文件名）的时候。\n用Ctrl+c停止脚本\n我们已经学会了运行脚本，那么如何提前中断和停止Python脚本呢？Windows是 Ctrl+C，mac是 Control+c，就可以停止通过命令开始的进程。（进程：计算机对一系列命令的处理过程。对于一个脚本或程序，计算机将它解释成一个进程，如果这个程序非常复杂，就解释成一系列进程，这些进程可以顺序执行，也可以并发执行。）\n读懂出错信息并找到解决方案\n当窗口显示了错误信息时，我们先读懂出错信息。某些情况下，出错信息中明确指出了代码中出现错误的行，我们可以集中精力解决这一行的错误（你的文本编辑器应该设置成显示行号，可以在网上搜索一下）。出错信息也是编程的一部分，学会编程也包括学会如何有效地调试程序错误。最好的做法是将整个错误信息（至少是信息的主要部分）复制到搜索引擎上，看看别人是如何调试这种错误的。\n\n这样以后，接下来我们就可以来了解认识Python的语言基础要素了。\n人生苦短，一起学习Python。"
  },
  {
    "objectID": "blog/2020/01/01/index.html",
    "href": "blog/2020/01/01/index.html",
    "title": "鼠年加油",
    "section": "",
    "text": "回望一幕幕送别 何尝不轻描淡写 独自出姑苏城外 流光未曾相约 他日他乡重逢 风轻柔河流缓缓\n\n\n新年快乐，鼠年加油 没有珍惜的时间，2020开始去珍惜 没有完成的事情，2020开始去完成 从来不相信鸡汤，也不制造鸡汤 等2021年再回顾这一年 希望感受到的不再是很多尴尬的空白 每到新的一年，你的朋友圈是不是也有刷屏的感慨和祝福 一年初始，心愿是美好的 如果能坚持做下去，该是多么圆满 新年少偷点懒，多学习新东西 多看看书，多写点公众号😂 多做一些分享，和小伙伴们共同进步 希望每个你新年新气象，活成你想要的样子 感谢你的关注❤️"
  },
  {
    "objectID": "blog/2019/11/28/index.html",
    "href": "blog/2019/11/28/index.html",
    "title": "统计学是干嘛的？",
    "section": "",
    "text": "统计学之所以存在，关键的原因只有一个，那就是变异及由此产生的抽样误差。没有变异，没有抽样误差，就没有统计学存在的理由。当我们把多个随机结果放在一起的时候，却能发现一定的规律性。正是因为这种规律的存在，所以我们仍然可以在变异中寻找规律，这也正是统计学的主要目的：从各种看似杂乱的现象中找出潜在的规律。\n\n抽样调查\n既然是规律，那就一定要在大多数人中存在，只在一小部分人中存在的现象不是规律，而是偶然，因为更多的是大多数人没有存在该现象，这才是规律。要证明一种现象是不是真正的规律，需要在大量人群中进行验证。由于我们无法接触到理论意义上的总体，因而我们换一种思路，调查部分具有代表性的样本，然后用统计学方法将样本的结果推广到总体，这就是我们所说的抽样调查。\n\n\n统计推断与参数估计\n统计学通常利用样本数据来推断总体结果，就是我们所说的用样本统计量推断总体参数。总体参数是客观存在的，经典的频率主义学派认为，总体参数是一个客观存在且固定的数值，而贝叶斯学派认为连总体参数自身也是个随机变量，所以也需要我们去估计。样本随机，样本统计量也是随机的，用它来估计总体参数，估计结果会存在一定的误差。但科学合理的抽样调查，其推断的结果是可靠的。偏差的样本会导致偏差的结论。样本必须足够代表总体。当然还需要考虑其他因素，比如调查员的水平、总体人群的变化等影响因素。\n\n\n抽样误差\n然而，即使代表性非常好的样本，也是无法真正等同于总体的，总会存在一定的抽样误差。样本统计量之间的差异就反映了抽样误差。由于抽样误差的存在，如果用样本统计量直接估计总体参数，那么肯定会有一定的偏差。所以在估计总体参数时需要考虑到抽样误差带来的偏差，因而我们在点估计之外，用置信区间来估计总体参数。抽样误差带来的偏差是多大呢？在实际中，我们不可能通过多次抽样，计算每个样本间统计量的差异大小从而去估计偏差大小，我们只能通过一次样本计算。这种根据一次样本计算抽样误差的大小就是标准误（standard error）。标准误几乎在所有统计方法中都会出现，因为它可以提示结果的可靠性：如果标准误较小，则说明抽样误差小，这意味着样本很稳定，对总体的代表性很好，由此推论结果较为可靠；如果标准误较大，则说明抽样误差大，提示样本代表性不强，这种情况下一般需要加大样本量，否则结果不可靠。"
  },
  {
    "objectID": "blog/2019/12/11/index.html",
    "href": "blog/2019/12/11/index.html",
    "title": "因果推断",
    "section": "",
    "text": "因果推断\n禁止言论就意味着禁止了思想，同时也扼杀了与此相关的原则、方法和工具。\n\n\n干预\ndo算子表明正在进行主动干预而非被动观察，这一概念在经典统计学中没有涉及。临床试验中使用do算子来确保观察到的病人存活期的变化能完全归因于药物本身，而没有混杂其他影响寿命长短的因素。如果不进行干预而让病人自行决定是否服用该药物，那么其他因素就可能会影响病人的决定，而服药和未服药的两组病人的存活期的差异也就无法再被仅仅归因于药物。例如，假设只有重症期的病人服用了这种药，那么两组之间的比较结果实际上反映的是其病情的严重程度，而非药物的影响。在数学上，我们把自行服药病人的生存期的观测概率称为条件概率，这里的概率是以观察到病人服用药物为条件的。【观察到】和【进行干预】是有本质的区别的。我们不认为气压计读数下降是风暴来临的原因，因为观察到气压计读数下降意味着风暴来临的概率增加，但人为使气压计读数下降并不能影响风暴来临的概率。因果推断要做的就是如何在不实际实施干预的情况下预测干预的效果。\n\n\n反事实\n假如某人在服用某种药物一个月后死亡，我们现在要关注的问题就是这种药物是否导致了他的死亡。为了回答这个问题，我们需要假设：如果他没服用这种药物，是否会避免死亡？反事实推理输出有关反事实世界的答案。\n语言会塑造思想。你无法回答一个你提不出来的问题；你也无法提出一个你的语言所不能描述的问题。"
  },
  {
    "objectID": "blog/2020/01/02/index.html",
    "href": "blog/2020/01/02/index.html",
    "title": "因果推断-2",
    "section": "",
    "text": "统计值的不确定性意味着什么？\n统计推断利用统计方法生成一个问题答案的实际估计值，并给出对该估计值的不确定性大小的统计估计。这种不确定性反映了样本数据集的代表性以及可能存在的测量误差或数据缺失。数据永远是从理论上无限的总体中抽取的有限样本。我们无法避免根据样本测量的概率无法代表整个总体的相应概率的可能性。统计学提供了很多方法来应对这种不确定性，包括极大似然估计、倾向评分、置信区间、显著性检验等。\n\n\n相关与独立\n以因果模型的路径图(因果图)来表示的变量之间的听从模式通常会导向数据中某种显而易见的模式或者相关关系。“A和B之间没有连接路径”翻译成统计语言，就是“A和B相互独立”，即发现A的存在不会改变B发生的可能性。\n\n\n想象力与规划\n历史学家尤瓦尔·赫拉利在他的《人类简史》一书中指出，人类祖先想象不存在之物的能力是一切的关键，正是这种能力让他们得以交流得更加顺畅。在获得这种能力之前，他们只相信自己的直系亲属或者本部落的人。而此后，信任就因共同的幻想(例如信仰无形但可想象的神，信仰来世，或者信仰领袖的神性)和期许而延伸到了更大的群体。我们的智人祖先新掌握的因果想象力使他们能够通过一种被我们称为“规划”的复杂过程更有效地完成许多事情，例如他们可以通过想象和比较几个狩猎策略的结果来完成一次狩猎活动。而要做到这一点，思维主体必须具备一个可供参考并且可以自主调整的关于狩猎现实的心理模型。心理模型是施展想象力的舞台，它使我们可以通过对模型局部的自主调整修改来试验重估不同情景的概率，人类的心理模型因而具有一种模块性，其涉及预测对环境进行刻意改变后的结果，并根据预测结果选择行为方案以催生出自己所期待的结果。"
  },
  {
    "objectID": "blog/2020/01/05/index.html",
    "href": "blog/2020/01/05/index.html",
    "title": "可惜没如果——因果关系的三个层级",
    "section": "",
    "text": "前言\n我们平时在统计中最常接触到的就是相关关系，而在因果推断中，实际上相关关系是处在因果关系三个层级的最低层级。正如我们所熟知的，相关不能说明因果，我们从观察到的数据中得到的相关对于因果的解释并不能起到直接的作用。\n\n\n因果关系的三个层级\n\n第一层级：关联\n我们一般通过观察寻找规律。如果观察到某一事件改变了观察到另一事件的可能性，我们便说这一事件与另一事件相关联。更进一步地，我们基于被动观察做出预测。\n典型的数据预测问题就是：如果我观察到···会怎样？例如，一家商店可能会问你：购买牙膏的顾客同时购买牙线的可能性有多大？这类问题是统计学的安身立命之本，统计学家主要通过收集和分析数据给出答案。\n我们可以这样解答：首先采集所有顾客购物行为的数据，筛选出购买牙膏的顾客，计算出当中购买牙线的人数比例。这个比例也就是我们所说的条件概率，用来反映(针对大数据)买牙膏和买牙线两种行为之间的关联程度，即 P(牙线|牙膏)。典型常用的关联度量方法即相关分析或回归分析，具体操作就是将一条直线拟合到数据点集中，再去确定这条直线的斜率。\n我们在学习统计的时候，几乎所有老师都会和你说：相关不代表因果。统计学本身不能告诉我们，牙膏或牙线哪个是因，哪个是果。但是从商店的角度看，因果这件事并不重要——好的预测无需好的解释。\n当前的机器学习程序（包括应用深度神经网络的程序）几乎仍然完全是在关联模式下运行的。它们由一系列观察结果驱动，致力于拟合出一个函数，就像我们试图用点集拟合出一条直线一样。深度神经网络为拟合函数的复杂性增加了更多的层次，但其拟合过程仍然由原始数据驱动。如果无人驾驶汽车的程序设计者想让汽车在新情况下做出不同的反应，他就必须明确地在程序中添加这些新反应的描述代码，否则机器没有应对新情况的灵活性和适应性。\n\n\n第二层级：干预\n在第一层级中，我们基于被动观察发现规律，做出预测。而当我们开始寻求主动对环境做出改变时，我们就迈上了因果关系的第二层级。这一层级的一个典型问题是：如果我们把牙膏的价格翻倍，牙线的销售额将会怎样？问出这个问题的时候，我们实际上已经脱离了收集到的数据本身，而要对数据的环境做出干预。我们把这样的问题记作 P(牙线|do(牙膏))，即如果我们实施...行动，将会怎样。\n毫无疑问，干预比关联更高级，因为其不仅涉及被动观察，还涉及主动改变现状。无论你的数据集有多大、神经网络有多深，只要你使用的是被动收集的数据，就无法回答有关干预的问题。从统计学中学到的任何方法都不足以让我们明确表述类似如果价格翻倍将会发生什么这样的问题，更谈不上回答了。\n为什么我们无法仅仅通过观察来回答牙线的问题呢？为什么不直接进入存有历史购买信息的数据库中，看看在牙膏价格翻倍的情况下对应的牙线的销售情况呢？原因在于，在历史销售信息中，牙膏涨价可能是出于完全不同的原因，如产品供不应求，其他商店也不得不涨价等。而我们只想刻意干预牙膏价格，这一结果就可能与历史上顾客在别处买不到便宜牙膏时的购买行为大相径庭。简单地说，我们只想知道单纯的牙膏涨价这个因所对应的牙线的果，而历史数据中各种影响因素完全超出了我们所提出问题的范畴，因而无法仅仅利用观察历史数据来回答干预的问题。\n因果推断则可以帮助我们解决这一问题。\n我们知道预测干预结果的一种非常直接的方法是：在严格控制的条件下进行实验。更加有趣的是，一个足够强大准确的因果模型可以在不进行实验的前提下，利用第一层级（关联）的数据来回答第二层级（干预）的问题。\n日常生活中，我们一直都在实施干预，尽管我们不会这么一本正经地称呼它。当我们服用阿司匹林试图治疗头痛时，就是在干预一个变量（人体内阿司匹林的量），以影响另一个变量（头痛的状态）。如果我们关于阿司匹林治愈头痛的因果知识是正确的，那么我们的结果变量的值将会从头痛变为不头痛。\n\n\n第三层级：反事实\n但是到此仍然不能回答所有我们感兴趣的因果问题：现在已经不头痛了，是因为我吃的阿司匹林么？是因为我吃的食物么？是因为我心情变好了么？\n正是这些问题将我们带入到因果关系的第三层级：反事实。要回答以上问题，我们就必须回到过去改变历史，假如我们没有服用过阿司匹林，会怎样？世界上没有哪个实验可以撤销对一个已接受过治疗的人所进行的治疗，进而比较治疗与未治疗两种条件下的结果。\n数据就是事实，而在反事实世界里，观察到的事实被完全否定。回到牙膏的例子，第三层级的问题是：假如我们把牙膏价格翻倍，之前买了牙膏的顾客仍然选择购买的概率是多少？在这个问题中，我们所做的就是将真实的世界（我们知道顾客以当前的价格购买了牙膏）和虚构的世界（牙膏价格翻倍）进行对比。\n倘若那天，把该说的话好好说，该体谅的不执着，你会怎么做？对这类问题的回答让我们得以从历史和他人的经验中获取经验教训。从想象的反事实中，我们获得了灵活性、反省能力以及改善行为的能力。\n因果关系第三层级的典型问题就是：假如我当时做了…会怎样？和为什么？两者都涉及观察到的世界与反事实世界的比较，仅靠干预实验无法回答这样的问题。"
  },
  {
    "objectID": "blog/2020/01/07/index.html",
    "href": "blog/2020/01/07/index.html",
    "title": "NumPy函数库基础",
    "section": "",
    "text": "机器学习算法涉及很多线性代数知识，我们会经常使用 NumPy 函数库，用线性代数简化不同的数据点上执行的相同数学运算。将数据表示为矩阵形式，只需要执行简单的矩阵运算而不需要复杂的循环操作。\n在Python shell开发环境中输入下面命令：from numpy import *，将 NumPy 函数库中的所有模块引入到当前的命名空间，输入以下命令：\n\n\n\nNumPy\n\n\n上述命令构造了一个 4*4 的随机数组（随机数组在不同计算机上输出可能不同）。\n在 NumPy 函数库中存在两种不同的数据类型（矩阵 matrix 和数组 array），二者都可以用于处理行列表示的数字元素。虽然看起来很相似，但在这两个数据类型上执行相同的数学运算可能得到不同的结果，其中的矩阵 matrix 与 MATLAB 中 matrices 等价。\n调用 mat() 函数可将数组转换为矩阵：\n\n\n\nmat\n\n\n如何进行矩阵求逆呢？不记得或者没学过矩阵求逆也没关系，NumPy 库 .I 操作符可以很方便地算出矩阵的逆运算：\n\n\n\n逆矩阵\n\n\n接着执行矩阵乘法，得到矩阵与其逆矩阵相乘的结果：\n\n\n\n矩阵乘法\n\n\n结果应该是单位矩阵，除了对角线元素是 1，4*4 矩阵的其他元素应该全是 0。上面实际输出略有不同，矩阵里还留下了很多数值非常小的元素，这是计算机处理误差产生的结果，我们来看一下误差值：\n\n\n\n误差\n\n\n函数 eye(4) 创建 4*4 的单位矩阵。\n只要能够顺利完成上面的例子，你就已经正确地安装并初步使用了 NumPy 函数库。后面我们会对它有更深的了解。"
  },
  {
    "objectID": "blog/2020/01/09/index.html",
    "href": "blog/2020/01/09/index.html",
    "title": "笔记",
    "section": "",
    "text": "表示的重要性\n表示问题必须优先于获取问题。如果缺少问题的表示方法，我们也就不知道如何存储信息以供使用。人工智能对认知研究的一个主要贡献就是确立表示第一，获取第二的范式。\n通常，在寻求一个好的表示方法的过程中，关于如何获取知识的洞见就会自然产生，无论这种洞见是来自数据，还是来自程序员。\n人类的大脑肯定拥有某种简洁的信息表示方式，同时还拥有某种十分有效的程序用以正确解释每个问题，并从存储的信息表示中提取正确答案。我们需要给机器装备同样高效的表示信息和提取答案的算法，因果图和因果推断就派上了用场。\n\n\n打破规则\n计算机不能理解因果关系，我们必须教会它如何打破规则，让它理解观察到某事件和使某事件发生之间的区别。\n我们需要告诉计算机：无论何时，如果你想使某事发生，那就删除因果图中指向该事的所有箭头，之后继续根据逻辑规则进行分析。这样做的原因很简单：使某事发生就意味着将它从所有其他影响因子中解放出来，并使它受限于唯一的影响因子——能强制其发生的那个因子。\n计算机能够进行因果推理的前提是，计算机懂得有选择地打破逻辑规则。\n\n\n概率的重要性\n构建因果模型不仅仅是画箭头，箭头背后还隐藏着概率。当我们绘制一个从 X 指向 Y 的箭头时，我们是在暗指，某些概率规则或函数具体说明了如果 X 发生改变，Y 将如何变化。在某些情况下我们可能知道这个规则具体是什么，但更多时候，我们不得不根据数据来对这个规则进行估计。因果革命最有趣的特点之一就是，在许多情况下，我们可以对这些完全不确定的数学细节置之不理。通常情况下，因果图自身的结构就足够让我们推测出各种因果关系和反事实关系：简单的或复杂的、确定的或概率的、线性的或非线性的。\n\n\n概率与因果关系\n概率能将我们对静态世界的信念进行编码，而因果论则告诉我们，当世界被改变时（无论改变是通过干预还是通过想象实现的），概率是否会发生改变以及如何改变。"
  },
  {
    "objectID": "blog/2020/01/11/index.html",
    "href": "blog/2020/01/11/index.html",
    "title": "kNN改进约会网站的配对效果",
    "section": "",
    "text": "前言\n前面我们已经初步了解了kNN——如何用Python自编k-近邻算法？今天我们试着进行一个实例上kNN的应用。\n海伦一直使用在线约会网站寻找自己心仪的约会对象。经过一番总结，她发现曾交往过三种类型的人：不喜欢的人、魅力一般的人和极具魅力的人。她发现自己无法直接将约会网站推荐的匹配对象归入恰当的上述类别之中，希望我们的分类软件能够更好地帮助她进行确切的分类。此外，她还收集了一些约会网站未曾记录的数据信息，提供给了我们。\n\n\n准备数据：从文本文件中解析数据\n海伦将准备的数据存放在文本文件datingTestSet2.txt中，每个样本数据占据一行，总共有1000行。每个样本主要收集了3种特征：每年飞行里程数、玩视频游戏所耗时间百分比和每周消费的冰淇淋公升数。如下：\n\n\n\ndatingTestSet2\n\n\n而在我们将上述特征数据输入到分类器之前，必须将待处理数据的格式转换为分类器可以接受的格式。我们之前在kNN.py中已经创建了kNN分类器函数，接下来我们创建用于处理输入数据格式的file2matrix函数，此函数的输入为文件名字符串，输出为训练样本矩阵和类标签向量，将文本记录转换为NumPy。\n\n\n\nfile2matrix\n\n\n首先我们以r模式（只读模式）打开要处理的文本文件，readlines函数读取整个文件所有行，保存在一个列表变量中，每行作为一个元素，我们计数文件的行数。然后创建以零填充的矩阵，我们将该矩阵的另一维度设置为固定值3。循环处理文件的每一行数据：使用strip函数截取掉所有的回车字符，使用tab字符（将上一步得到的整行数据分割成一个元素列表，选取前3个元素存储到特征矩阵中，使用索引值-1将文件的最后一列存储到向量classLabelVector中，这里，我们必须明确地通知解释器存储的的元素值为整型，否则Python会将这些元素当作字符串进行处理。\n使用函数file2matrix读取文件数据，必须确保文件存储在我们的工作目录中。重新加载kNN.py模块，以确保更新的内容可以生效，否则Python将继续使用上次加载的kNN模块。\n\n\n\nload data\n\n\n现在我们已经从文本文件导入了数据并将其格式化为想要的格式，接下来我们以图形化的方式直观地展示数据内容，以便辨识出一些数据模式。\n\n\n分析数据：使用Matplotlib创建散点图\n首先我们使用Matplotlib制作原始数据的散点图：\n\n\n\nscatter\n\n\n输出效果如下图，散点图使用特征矩阵的第二、三列数据，分别为玩视频游戏所耗时间百分比和每周消费的冰淇淋公升数。由于没有使用样本类别标签，我们很难看出有用的数据模式信息。\n\n\n\nplot\n\n\n为了更好地理解数据，我们以不同的方式来标记不同的样本类别。Matplotlib库提供的scatter函数支持个性化标记散点图上的点。重新输入上面的代码，调用scatter函数时使用下列参数，利用变量datingLabels存储的类别标签属性，在散点图上绘制色彩不等、尺寸不同的点：\n\n\n\nscatter function\n\n\n\n\n\ncolored plot\n\n\n我们基本上能够看到数据点所属三个类别的区域轮廓，但还不是十分明显，接下来我们使用特征矩阵的第一、二列属性作图：\n\n\n\nclusters\n\n\n此时我们可以看到图中清晰地标示了三个不同的样本类别区域，通过这两个特征更容易区分数据点。\n\n\n准备数据：归一化数值\n观察原始数据我们发现：每年飞行里程数的数量级远大于其余两个特征。在利用kNN计算样本之间的距离时，数值大的该特征会极大地影响最终的结果，也就是说，数量值大小会影响特征对结果影响的权重，而我们这里认为三个特征是同等重要的。\n因而在处理这种情况时，我们通常采用的方法是将数值归一化，如将取值范围处理为0到1或者-1到1之间。下面的公式可将特征值转化为0到1区间内的值：\\(newValue = \\frac{oldValue - min}{max - min}\\)，其中 \\(min\\) 和 \\(max\\) 分别是数据集中的最小特征值和最大特征值。我们需要在脚本 kNN.py 中增加一个函数 autoNorm，自动将数字特征值转化为0到1区间内的值。\n\n\n\nautoNorm\n\n\n我们将每列的最小值放在变量minVals中，每列最大值放在变量maxVals中，其中的参数0使得函数可以从列中选取最小值和最大值。然后函数计算可能的取值范围，并创建新的返回矩阵。\n正如前面给出的公式，为了归一化特征，我们使用当前值减去最小值，然后除以取值范围。而需要注意的是，特征值矩阵有1000*3个值，而minVals和maxVals的值都为1*3。为了解决这个问题，我们使用NumPy库中函数tile将变量内容复制成输入矩阵同等大小的矩阵，然后再利用具体特征值相除得到归一化后的特征矩阵。需要注意的是：对于某些数值处理软件包，/可能意味着矩阵除法，但在NumPy库中，矩阵除法需要使用函数linalg.solve(matA,matB)。\n我们重新加载kNN.py模块，执行函数autoNorm：\n\n\n\nreload autoNorm\n\n\n这里我们也可以只返回normMat矩阵，但是后面我们需要取值范围和最小值来归一化需要测试的新数据。\n\n\n测试算法：作为完整程序验证分类器\n我们已经对数据按照需求进行了处理，下面我们来测试分类器的效果。我们将已有数据的90%作为训练集来训练分类器，使用余下的10%作为测试集，检测分类器的错误率。这里由于海伦提供的数据并没有按照特定目的来排序，因而我们可随意选择10%数据而不影响测试集选择的随机性。\n\n\n\ndatingClassTest\n\n\n转换数据格式并归一化后，我们决定哪些数据用于测试，然后将训练集和测试集输入到kNN分类器classify函数中，计算错误率并输出分类结果。\n\n\n\ntest result\n\n\nkNN分类器在测试集上的错误率为5%。我们可以改变函数datingClassTest内变量hoRatio和变量k的值，看看错误率是否会发生一些变化。\n现在，海伦可以输入未知对象的特征信息，由的分类器来帮助她判定某一对象的可交往程度：讨厌、一般喜欢、非常喜欢。\n\n\n使用算法：构建完整可用系统\n我们会给海伦一小段程序，通过该程序海伦会在约会网站上找到某个人并输入他的信息，程序会给出她对对方喜欢程度的预测值。\n\n\n\nclassifyPerson\n\n\n这里使用input函数获取用户控制台的输入。我们让海伦给出她在约会网站上新找的一个人信息。由于NumPy库提供的数组操作并不支持Python自带的数组类型，因此在编写代码时要注意不要使用错误的数组类型。另外在输入新样本时注意将其归一化处理。\n\n\n\nclassify result\n\n\n这样，我们就完成了kNN对约会网站的配对效果的改进了。"
  },
  {
    "objectID": "blog/2020/01/17/index.html",
    "href": "blog/2020/01/17/index.html",
    "title": "你真的了解自己的电脑吗？",
    "section": "",
    "text": "前言\n我们现在基本上人手一台电脑，无论你是Mac、Windows还是Linux，你真的了解你手头上的电脑么？当你在选购新电脑或者购买部件的时候，是否了解该怎么挑选呢？接下来我们一起了解下最常见也是容易搞不懂的电脑/计算机。\n\n\n什么是计算机\n一般地，接受用户输入命令与数据，经由中央处理器的数学与逻辑单元运算处理后，以产生或存储成有用的信息的机器，我们就称之为计算机。从这个意义上讲，我们日常用的计算器、手机、GPS卫星定位系统、ATM取款机、台式电脑、笔记本电脑、iPad、Apple Watch等都是计算机。我们常说的计算机是其中的台式或笔记本电脑。\n\n\n电脑硬件\n电脑的三大部分（以台式电脑为例）包括：\n\n输入单元：包括键鼠、读卡器、扫描仪、手写板、触控屏幕等\n主机部分：即系统单元，在主机机箱内，里面含有一堆板子、CPU与内存等\n输出单元：屏幕、打印机等\n\n主机里面最重要的就是一块主板，上面安装了中央处理器（Central Processing Unit, CPU）以及内存、硬盘（或存储卡）还有一些适配卡设备。大部分智能手机是将这些组件直接焊接在主板上面而不是插卡。\n\n\nCPU\n整台主机的重点在于CPU，CPU为一个具有特定功能的芯片，里面含有指令集，如果你想要让主机进行什么操作，就得要参考这块CPU是否有相关内置的指令集才可以。由于CPU的工作主要在于管理和运算，因此在CPU内又可分为两个主要的单元，分别是算术逻辑单元和控制单元。其中算术逻辑单元主要负责程序运算与逻辑判断，控制单元则主要协调各周边组件与各单元间的工作。CPU是整个电脑系统的最重要部分。\nCPU依设计理念不同，主要分为：\n\n精简指令集（RISC）系统：ARM公司的ARM CPU系列等。我们常使用的各品牌手机、导航系统、路由器等，几乎都是使用ARM架构的CPU。\n复杂指令集（CISC）系统：AMD、Intel等x86架构的CPU。由于x86架构CPU被大量用于个人电脑，因此个人电脑常被称为x86架构电脑。64位的个人电脑CPU又被统称为x86-64架构。x86架构的称呼来源于Intel最早研发出来的CPU代号。所谓的位（bit），指的是CPU一次读取数据的最大量。64位CPU表示CPU一次可以读写64位的数据，一般32位CPU所能读写的最大数据量大概是4GB。\n\n\n\n电脑上面常用的计算单位（容量、速度等）\n\n容量单位：电脑对数据的判断主要依据有没有通电来记录信息，所以理论上对于每一个记录单位而言，电脑只认识0与1而已。0/1这个二进制的单位我们称之为位（bit，比特）。但位实在太小，因而每份数据都使用8个位来记录，8位为一字节（Byte）。同样的，字节依然太小，因而有K代表1024B，M代表1024K，G代表1024M，T代表1024G，P代表1024T，E代表1024P。一般来说，数据容量使用二进制，所以1GB的文件大小为102410241204B。\n速度单位：CPU的命令周期常使用MHz或GHz之类的单位，这个Hz是“次数/秒”的意思。而在网络传输方面，由于网络使用的是位为单位，因此网络常使用的单位为Mbit/s（每秒多少Mbit）。大家常听到的“20M/5M”光纤传输速度，如果转成数据容量的字节时，其实理论最大传输值为：每秒2.5MB/每秒625KB的下载或上传速度。\n\n假设你今天购买了一块500GB的硬盘，但是格式化完毕后只剩下460GB左右的容量，这是为什么呢？一般硬盘制造商使用十进制的单位，所以500GB代表50010001000*1000B，转成数据的容量单位时使用二进制（1024为基数），所以就成为466GB左右的容量了。并非厂商骗人，只是因为硬盘的最小物理量为512B，最小的组成单位为扇区（sector），通常硬盘容量的计算采用多少个扇区，所以才会使用十进制来处理。\n\n\n内存\nCPU读取的数据完全从内存中来（无论是程序还是一般文件数据），如果要读取硬盘中的数据，也要将数据挪到内存当中，再交由CPU来读取。内存中的数据则是从输入单元所传输进来的，而CPU处理完毕的数据也必须要先写回内存，最后数据才从内存传输到输出单元。\n这就是我们常说的，要加快系统性能，通常将内存容量加大就可以获得相当好的效果。因为所有的数据都是要经过内存的传输，所以内存的容量如果太小，数据读写性能就不足，对性能的影响相当大，尤其在Linux作为服务器操作系统的环境下。这也是为什么在买手机时，人们对可用内存（运行内存）的要求都很高的原因。"
  },
  {
    "objectID": "blog/2020/01/19/index.html",
    "href": "blog/2020/01/19/index.html",
    "title": "谈谈电脑的CPU",
    "section": "",
    "text": "前言\n前面我们已经初步了解了计算机–你真的了解自己的电脑吗？接下来我们继续深入计算机的CPU单元。\n\n\nCPU\n前面我们提过，一般我们常说的电脑指的是x86的个人电脑架构。Linux操作系统最早在发展的时候，就是依据个人电脑的架构来设计的。而在个人电脑架构中，充当“大脑”的无疑是CPU。\n由于CPU负责大量运算，因而它是电脑中具有相当高发热量的组件。现在的所谓多内核CPU，是在一块CPU封装内嵌入两个以上的运算内核，即含有两个以上的CPU单元。\n我们已经知道，CPU内部指令集的不同会导致其工作效率的高低，那么CPU性能的比较还有什么呢？答案是频率。CPU的频率就是CPU每秒钟可以进行的工作次数，频率越高表示这块CPU在单位时间内可以做更多的事情。举例来说，Intel的i7-4790CPU频率为3.6GHz，即表示这块CPU在一秒内可以进行3.6*10的九次方次工作。但是需要注意的是：只能在同款CPU间比较频率的快慢，不同CPU由于指令集、架构、使用的二级缓存及其运算机制的可能不同，单纯看频率没有可比性。\n我们可能听过“超频”这个词，它是什么意思呢？CPU在出厂时，厂商已经设置了这款CPU正常稳定工作的频率，一些电脑硬件玩家要发挥出CPU最大的性能，往往会手动将CPU的外频通过主板提供的设置功能更改成较高频率。现在Intel的CPU会主动帮你超频，以合理利用CPU以及节能。\n最常听见的还有32位与64位电脑，我们可能也一头雾水。其实这个也与CPU相关。我们将CPU每次能够处理的数据量称为字长（word size），字长依据CPU的设计而有32位与64位，而32位与64位电脑主要就是依据这个CPU所能解析的字长而来的。早期的32位CPU中，由于CPU每次解析的数据量有限，因此从内存传来的数据量就有所限制，这也导致32位的CPU最多只能支持最大到4GB的内存。目前的64位CPU统称为x86-64。"
  },
  {
    "objectID": "blog/2020/01/21/index.html",
    "href": "blog/2020/01/21/index.html",
    "title": "上手vim编辑器",
    "section": "",
    "text": "我们平时已经接触了不少的程序编辑器，今天我们要上手一种命令行模式下的文本编辑器——vim编辑器。说它是文本编辑器有点小瞧它的功能，实际上，它也可以作为程序编辑器使用，且功能十分强大。\n在所有的Linux发行版上面都会有一个文本编辑器，那就是vi，vim是高级版的vi。vim不仅可以用不同的颜色显示文字内容，还能够进行诸如shell脚本、C语言等程序编辑，搭配Python也是十分的香，甚至不少人用它来作为写作的专用编辑器。由此可知它的功能有多么的强大。\n如果你在学习Linux，身边的人都会建议你：学习使用命令行模式来处理Linux系统的设置问题，而尽量少去使用图形窗口模式。在配置Linux参数文件时，我们就需要一款强大稳定的文本编辑器。而Linux在命令行模式下的文本编辑器有哪些呢？我们常听到emacs、nano、vim等，而在其中，其实vim并非是对用户最友善的文本编辑器。但是为什么这么多人推荐使用呢？原因有几点：\n\n所有的UNIX-like系统都会内置vi文本编辑器，其他的文本编辑器不一定会存在；\n很多软件的编辑接口都会主动调用vi；\nvim具有程序编辑的能力，可以主动地以字体颜色辨别语法的正确性，方便程序设计；\n编辑速度相当快速。\n\n可以说，如果不上手vim，Linux中很多命令根本无法操作。\n我们提到，vim是高级版本的vi，它可以用颜色或下划线的方式来显示一些特殊的信息，可以依据文件的扩展名或是文件内的开头信息，判断该文件的内容而自动调用该程序的语法判断样式，再以颜色来显示程序代码与一般信息，vim是程序开发者的一项非常好用的工具，就连vim的官方网站（http://www.vim.org）都认为自己是一款程序开发工具而非仅仅是文本处理软件。\n由于是命令行模式下的编辑器，当我们在编辑程序或者制作网页的时候，vim不能做到一般编辑器那样所见即所得，这是它的一个特色。vim同样也有一些非常好用的功能，如支持正则表达式的查找方式、多文件编辑、区块复制等，非常的棒。我们会在日后持续更新vim的使用分享。"
  },
  {
    "objectID": "blog/2020/01/27/index.html",
    "href": "blog/2020/01/27/index.html",
    "title": "计算机概论4",
    "section": "",
    "text": "显卡\n显卡又称为VGA（Video Graphics Array），它对于图形影像的显示扮演着相当关键的角色。一般对于图形影像的显示重点在于分辨率与颜色深度，因为每个图像显示的颜色会占用内存，因此显卡上面会有集成内存并被称为显存，这个显存容量将会影响到你的屏幕分辨率与颜色深度。\n假设你的显示器使用1024*768分辨率，且使用全彩（每个像素占用3B的容量），至少需要多少内存才能使用这样的饱和度？\n因为1024*768分辨率中会有786432个像素，每个像素占用3B，所以总共需要2.25MB以上才行。但如果考虑屏幕的刷新率（每秒钟屏幕的刷新次数），显卡的内存还是越大越好。\n除了显存之外，现在显卡的运算能力也越来越重要，所以显卡厂商直接在显卡上面嵌入一个3D加速的芯片，这就是所谓的GPU称谓的由来。\n显卡主要也是通过GPU的控制芯片来与CPU、内存等通信，也是需要高速运算的一个组件，所以数据的传输也是越快越好。\n显卡与电脑屏幕（或电视）连接的主要接口有：\n\nD-Sub（VGA接口）：较早之前的连接接口，当初设计是针对传统的CRT显示器而来；\nDVI：常见于液晶屏幕的连接；\nHDMI：可同时传输影像与声音，被广泛地使用于电视屏幕中，电脑屏幕目前也经常都会支持HDMI格式；\nDisplayPort：与HDMI相似，可同时传输影像与声音。\n\n\n\n硬盘\n硬盘是由许多的圆形碟片、机械手臂、磁头与主轴马达所组成的。实际的数据都是写在具有磁性物质的碟片上面，而读写主要是通过在机械手臂上的磁头来完成的。实际运行时，主轴马达让碟片转动，然后机械手臂可伸展让磁头在碟片上面进行读写的操作。另外，由于单一碟片的容量有限，因此有的硬盘内部会有两个以上的碟片。\n由于碟片是圆的，且通过机械手臂去读写数据，碟片要转动起来才能够让机器手臂读写，因而通常数据就是以圆圈转圈的方式读写。当初设计在类似碟片同心圆上面切出一个一个的小区块，让磁头去读写，这个小区块就是磁盘的最小物理存储单位，称之为扇区（sector），同一个同心圆的扇区组合成的圆就是所谓的磁道（track）。由于磁盘里可能会有多个碟片，因此在所有碟片上面的同一个磁道可以组合成所谓的柱面（cylinder）。\n我们知道同心圆外圈的圆比较大，占用的面积比内圈多，所以外圈的圆有更多的扇区，通常数据的读写也是默认从外圈开始往内写。原本硬盘的扇区都是设计成512B的大小，目前绝大部分的高容量硬盘已经使用了4KB大小的扇区设计。也因为这个扇区设计，在磁盘分区方面，目前有旧式的MBR模式（MS-DOS兼容模式），以及较新的GPT模式。在较新的GPT模式下，磁盘的分区通常使用扇区号码来划分，和过去旧的MS-DOS是通过柱面号码来划分的方式不同。\n注意：由于硬盘内部机械手臂上的磁头与碟片的接触是很细微的空间，如果有抖动或是污物附着在磁头与碟片之间就会造成数据的损坏或是物理磁盘整个损坏，因而，在电脑通电之后避免震动硬盘。另外，因为机械手臂必须要回归原位，不要随便将电脑电源插头拔掉就以为是顺利关机。\n\n\n固态硬盘\n传统硬盘有个很致命的问题，就是需要驱动马达来转动碟片，这会造成很严重的磁盘读取延迟。因此有厂商拿闪存去制作高容量的设备，而且外形还做的和传统磁盘一样。所以，这类设备已经和传统的机械磁盘（Hard Disk Drive，HDD）不同，我们称之为固态硬盘（Solid State Disk或Solid State Driver，SSD）。\n固态硬盘的最大好处是：它没有马达要去转动，而是通过闪存直接读写的特性，因此除了没数据延迟且快速之外，还很省电。测试磁盘的性能时，有个很特殊的度量单位，称为每秒读写操作次数（Input/Output Operations Per Second，IOPS），这个数值越大，代表可操作次数较高，当然性能也越好。\n目前大家对于HDD和SSD的使用方式大多是：使用SSD作为系统盘，将数据存储放在HDD上，这样系统运行快速，而数据存储量也大。"
  },
  {
    "objectID": "blog/2020/01/29/index.html",
    "href": "blog/2020/01/29/index.html",
    "title": "R语言基础–运算符",
    "section": "",
    "text": "前言\n运算符是一些符号，进行算术运算、比较运算或逻辑运算等。\n\n\n算术运算符\n指数学运算中常用的5种运算符号，有：\n\n^ 幂\n* 乘\n/ 除\n+ 加\n- 减\n%% 模运算\n%/% 整数除法\n\n\n\n比较算符\n建立两个量之间的一种关系，并要求R确定这种关系是否成立。若成立，输出的运算结果为1（TRUE），若不成立，运算结果为0（FALSE）。\n\n== 等于\n!= 不等于\n&gt; 大于\n&lt; 小于\n&gt;= 大于等于\n&lt;= 小于等于\n\n\n\n逻辑算符\n通常用来连接一系列比较式，有：\n\n&& 标量的逻辑“与”运算\n|| 标量的逻辑“或”运算\n& 向量的逻辑“与”运算\n| 向量的逻辑“或”运算\n! 逻辑“非”\n\nR语言表面上没有标量的类型，标量可以看作是含有一个元素的向量，但逻辑运算符对标量和向量有着不同的形式。\n\n\n运算次序\n复杂表达式运算次序的准则：\n\n括号里的表达式先计算；\n较高优先级的运算先执行，具体的优先级的顺序为：\n\n\n第一级（最高级）：^（幂）；!（非）\n第二级：*（乘）；/（除）\n第三级：+（加）；-（减）\n第四级：&lt;；&lt;=；&gt;；&gt;=；==；!=\n第五级：&；&&；|；||\n\n\n对于相同优先级的算符，先做左边的运算。"
  },
  {
    "objectID": "blog/2020/01/31/index.html",
    "href": "blog/2020/01/31/index.html",
    "title": "计算机概论5",
    "section": "",
    "text": "CMOS与BIOS\n前面我们提过CMOS与BIOS的功能：CMOS主要记录主板上面的重要参数，包括系统时间、CPU电压与频率、各项设备的I/O地址与IRQ等。BIOS是写入到主板上某一块flash的程序，它可以在计算机启动的时候执行，以加载CMOS当中的参数，并尝试调用存储设备中的引导程序，进一步进入操作系统当中。BIOS程序可以修改CMOS中的数据，每种主板进入BIOS设置程序的按键都不同，一般桌面电脑常见的是使用[Del]按键进入BIOS设置界面。\n\n\n设备I/O地址与IRQ中断请求\n主板是负责各个电脑组件之间的通信，但是电脑组件实在太多，有输出/输入不同的存储设备等，这个时候主板芯片组就需要用到I/O地址与IRQ进行设备间的通信。I/O地址有点类似于每个设备专属的门牌号码，一般来说，不能有两个设备使用同一个I/O地址，否则系统会不知道该运行哪个设备。IRQ可以想成是各个设备门牌连接到邮件中心（CPU）的专门路径，各设备可以通过IRQ中断请求来告知CPU该设备的工作情况，以方便CPU进行工作分配的任务。\n\n\n数据表示方法\n事实上我们的电脑只认识0与1，记录的数据也是只能记录0与1而已。早期的电脑使用的是利用通电与否的特性制造的电子管，如果通电就是1，没有通电就是0，后来沿用至今，也就是我们说的二进制（binary）。电脑在表示利用数字时，利用二进制的转换进行对数字的处理。\n而对于文字的记录，事实上文本文件也是被记录为0与1，而这个文件的内容在被读取查看时，必须要经过一个编码系统的处理才行。所谓的编码系统可以看成是一个字码对照表。当我们要写入文件的字符数据时，该文字数据会由编码对照表将该字符转成数字后，再存入文件当中。同样，当我们要将文件内容的数据读出时，也会经过编码对照表将该数字转成对应的字符后，再显示到屏幕中。如果编码对照表写错，导致对照的字符产生误差，就会出现乱码。\n常用的英文编码表为ASCII系统，这个编码系统中，每个符号（英文、数字或符号等）都会占用1字节的记录，因此总共会有2的8次方即256种变化。国际组织ISO/IEC制定了所谓的Unicode编码系统，即我们常常说的UTF-8。这个编码打破了所有国家不同的编码之间的限制，因此目前互联网网站大多以此编码系统为主。"
  },
  {
    "objectID": "blog/2020/02/02/index.html",
    "href": "blog/2020/02/02/index.html",
    "title": "计算机概论6",
    "section": "",
    "text": "机器语言程序与编译型程序\n我们在需要CPU工作时，就得要参考其内部指令集的内容，然后编写让CPU能够读得懂的脚本让其去执行，这样CPU才能执行我们所给的任务。\n这就带来了几个问题：\n\n程序开发者必须要了解机器语言：机器只认识0和1，因此开发者必须要学习写机器能直接看懂的语言，而这个毫无疑问，难度很大\n必须要了解所有硬件的相关功能函数：开发者当然需要参考机器本身的功能去编写相应的程序代码，而如果每个开发者都去了解系统的所有硬件，这个工作量实在太大\n程序具有不可移植性：每个CPU都有其独特的指令集，每个硬件都有其功能函数，因此不同平台之间程序代码当然无法直接通用\n\n为了解决这些问题，计算机科学家设计出一种人类能看得懂的程序语言，然后创造一种编译器将这些人类写的程序语言转译成为机器能看得懂的机器语言，如此一来，我们修改与编写程序就容易多了。目前常见的编译器有C、C++、Java、Fortran等。\n高级程序语言的程序代码是较容易查看的，这样我们就将程序的编写修改问题处理完毕。问题是，在这样的环境下面我们还是得要考虑整体的硬件系统，从而来设计编写程序。举个例子，当你需要将运行的数据写入内存中，你就要自行分配一个内存区块出来让这些数据能够填充上去，所以你必须要去了解内存的地址是如何定位的原理，这样一来，程序的编写又会变得麻烦。\n为了要解决硬件方面老师需要重复编写一些程序的问题，所以有了操作系统的出现。"
  },
  {
    "objectID": "blog/2020/02/05/index.html",
    "href": "blog/2020/02/05/index.html",
    "title": "计算机概论7–操作系统",
    "section": "",
    "text": "我们前面提到，在早期想要让计算机执行程序就得要参考一堆硬件功能函数，并且学习机器语言才能够编写程序，同时由于硬件与软件功能不一定一致，每次编写程序时都必须要重新改写，非常的麻烦。\n如果能够将所有的硬件都驱动，并且提供一个软件的参考接口来给工程师开发软件，开发软件无疑会变得简单得多，而这就是操作系统（Operating System，OS）。\n\n操作系统内核（Kernel）\n操作系统其实也是一组程序，这组程序的重点在于管理计算机的所有活动以及驱动系统中的所有硬件。硬件的所有操作都必须要通过操作系统来实现，而这一功能的实现就是靠操作系统的内核完成。你的计算机能不能完成一些任务，都与内核有关。只有内核提供的功能，你的计算机系统才能帮你完成。举例来说，如果你的内核并不支持TCP/IP的网络协议，那么无论你配置什么样的网卡，这个内核都无法提供网络功能。\n内核主要在管理硬件与提供相关的功能（读写硬盘、网络功能、CPU资源分配等），这些管理的操作都非常重要。如果用户能够直接使用到内核的话，一旦不小心将内核程序停止或破坏，将会导致整个系统的崩溃。因此内核程序放置到内存当中的区块是受保护的，并且启动后就一直常驻在内存之中。\n\n\n系统调用\n既然硬件都是由内核管理的，那么如果开发人员想要开发软件的话，自然就得要参考这个内核的相关功能。这样一来，还是从原本的参考硬件函数变成参考内核功能，还是一样的麻烦。\n为了解决这个问题，操作系统通常会提供一套应用程序编程接口（Application Programming Interface，API）即系统调用层给程序员来开发软件，开发人员只要遵守该API公认的系统调用参数就可以比较容易地开发软件了。举例来说，我们学习C语言只要参考C语言的函数即可，不需要再去考虑其他内核的相关功能，因为内核的系统调用接口会主动地将C语言的相关语法转成内核可以了解的任务函数，内核自然就能够顺利地运行该程序。\n内核只会进行计算机系统的资源分配，所以系统还需要有应用程序的提供，才能够供用户使用。应用程序与内核有比较大的关系，与硬件关系则不大；硬件也与内核有比较大的关系，至于与用户直接有关的则是应用程序。\n有几点需要注意：\n\n操作系统的内核层直接参考硬件规格写成，所以同一个操作系统程序不能够在不一样的硬件架构上运行；\n操作系统只是管理整个硬件资源，包括CPU、内存、输入输出设备及文件系统等，如果没有其他的应用程序辅助，操作系统只能让计算机处于准备妥当的状态之中，无法完成用户所想要的功能；\n应用程序的开发都是参考操作系统的API，所以该程序只能在该操作系统当中运行，不能在其他操作系统上运行。所以有些游戏不能够在Linux上安装运行。"
  },
  {
    "objectID": "blog/2020/02/15/index.html",
    "href": "blog/2020/02/15/index.html",
    "title": "方差分析",
    "section": "",
    "text": "方差分析\nt检验和u检验不适用于多个样本均数的比较，而用方差分析比较多个样本均数，可以有效地控制I类错误。\n方差分析（analysis of variance，ANOVA）由英国统计学家R.A.Fisher首先提出，以F命名其统计量，故方差分析又称F检验。\n方差分析的基本思想是根据研究的目的和设计类型，将总变异的离均差平方和SS及其自由度v分别分解成相应的若干部分，然后求各相应部分的变异（数理统计证明，总的离均差平方和等于各部分离均差平方和之和）；再用各部分的变异与组内（或误差）变异进行比较，得出统计量F值；最后根据F值的大小确定p值，作出统计推断。\n方差分析的用途很广，包括两个或多个样本均数间的比较，分析两个或多个因素间的交互作用，回归方程的线性假设检验，多元线性回归分析中偏回归系数的假设检验，两样本的方差齐性检验等。\n方差分析的应用条件为：各样本需是相互独立的随机样本；各样本来自正态分布总体；各总体方差相等，即方差齐性。\n\n\n方差分析基本术语\n实验设计和方差分析都有自己相应的语言。\n以研究某药物对某癌细胞株增殖影响的研究为例，现有两种药物：新研究药物（Treatdrug）和对照组药物（Controldrug）。\n我们提取培养10个某癌细胞株作为研究对象，随机分配一半癌细胞株接受为期96h的Treatdrug治疗，另一半接受为期96h的Controldrug治疗。研究结束时，对两组细胞株的细胞抑制率进行评估。\n在这个实验设计中，治疗方案是两水平（Treatdrug和Controldrug）的组间因子，之所以称作组间因子是因为每个患者都仅被分配到一个组别中，没有患者同时接受Treatdrug和Controldrug。\n细胞抑制率是因变量，治疗方案是自变量。由于在每种治疗方案下观测数相等，因此这种设计也称为均衡设计；若观测数不同，则称为非均衡设计。\n因为仅有一个类别型变量，这种设计又称为单因子方差分析或进一步称为单因子组间方差分析。\n方差分析主要是通过F检验来进行效果评测，若治疗方案的F检验显著，则说明96h后两种药物的细胞抑制率均值不同。\n假设只对Treatdrug的效果感兴趣，则需要将10个癌细胞株都放在Treatdrug组中，然后在治疗24h和96h后分别评估疗效。此时，时间是两水平（24h和96h）的组内因子，因为每个癌细胞株在时间的所有水平下都进行了测量，因此这种设计称为单因子组内方差分析；又由于每个癌细胞株都不止一次被测量，也称作重复测量方差分析。若时间的F检验显著，则说明细胞抑制率在24h和96h间发生了改变。\n现假设对治疗方案差异和它随时间的改变都感兴趣，则可以将两个设计结合起来：随机分配一半癌细胞株到Treatdrug组，另一半到Controldrug组，在24h和96h分别评估它们的细胞抑制率。治疗方案和时间都作为因子时，既可以分析治疗方案的影响和时间的影响，也可以分析治疗方案和时间的交互作用。前两个为主效应，交互部分为交互效应。在这种情况下，需要进行3次F检验，治疗方案因素1次，时间因素1次，两者的交互因素1次。若治疗方案显著，说明Treatdrug和Controldrug对癌细胞的抑制效果不同；若时间显著，表明细胞抑制率在24h和96h间发生了改变；若两种因素交互效应显著，说明两种药物随着时间变化对癌细胞的一直效果不同（即细胞抑制率从24h到96h的改变程度在Treatdrug和Controldrug之间是不同的）。\n当设计中包含两个甚至更多因子时，便是多因子方差分析设计。两个因子时称为双因子方差分析，三因子时称为三因子方差分析。若因子设计包括组内因子和组间因子，又称为混合模型方差分析。\n这里，即使不同的癌细胞株被随机分配到不同的治疗方案中，但在研究开始时两组癌细胞株的增殖速度可能不同，治疗后的差异可能是最初的增殖速度不同导致的，而不是实验方案的影响。增殖速度也可以解释因变量的组间差异，因此它常被称为混杂因素。如果我们在评测治疗方案类型的影响前，对组建的统计学差异进行统计性调整，将初始增殖速度作为协变量，这样的设计称为协方差分析。\n当因变量不止一个时，该设计被称为多元方差分析，若还存在协变量，则称为多元协方差分析。"
  },
  {
    "objectID": "blog/2020/02/18/index.html",
    "href": "blog/2020/02/18/index.html",
    "title": "试验设计与方差分析（2）",
    "section": "",
    "text": "拉丁方设计\n完全随机设计只涉及一个处理因素，随机区组设计涉及一个处理因素、一个区组因素/配伍因素。若实验研究涉及一个处理因素和两个控制因素，每个因素的类别数或水平数相等，此时可采用拉丁方设计来安排试验，将两个控制因素分别安排在拉丁方设计的行和列上。\n将k个不同符号排成k行k列，使得每一个符号在每一行、每一列都只出现一次的方阵，叫做k*k拉丁方。\n拉丁方设计就是将处理从纵横两个方向排列为区组/重复，使每个处理在每一行和每一列中出现的次数相等（通常为1次），即在行和列两个方向都进行局部控制，所以它是比随机区组多一个方向局部控制的随机排列设计，因而具有较高精确性。\n拉丁方设计的特点是处理数、重复数、行数、列数都相等，即处理数=行区组数=列区组数=重复次数，它的每一行和每一列都是一个区组或一次重复，而每一个处理在每一行或每一列都只出现1次\n拉丁方试验设计的步骤：\n\n选择标准方：标准方是指代表处理因素水平的字母。在进行拉丁方设计时，首先要根据试验处理水平数k从标准方表中选定一个k*k的标准方，随后要对选定的标准方的行、列和处理进行随机化排列；\n列随机\n行随机\n处理随机\n\n拉丁方设计的特点是纵横两个方向都设了区组，从而可在两个方向上对土壤等差异（指田间试验时）进行局部控制。试验有k个处理，便有k*k个观测值。进行方差分析时，从总变异方差中除了分解出处理间方差和误差项方差外，还可以分解出纵横两个区组的方差，这样可使误差项方差进一步减小，所以拉丁方试验的精确度比随机区组试验更高。\n拉丁方设计的优点是：精确度高；缺点是：由于重复数与处理数必须相等，使得两者之间相互制约，缺乏伸缩性。因此采用拉丁方设计时试验的处理数不能太多，一般以4～10个为宜。\n\n\n析因设计\n单因素方差分析只涉及一个处理因素，该因素至少有两个水平，只是根据试验对象的属性和控制试验误差的需要，采用的试验设计方法有所不同。如比较注射4种不同剂量雌激素对大白鼠子宫重量的影响，处理因素是注射不同剂量的雌激素，有4个水平。完全随机设计是将n只白鼠随机分4组，随机区组设计是将n只白鼠按出生体重相近的原则，4只一组配成区组后，每个区组内随机分配4个处理水平，拉丁方设计则是在随机区组设计的基础上增加了一个列区组，如白鼠有4个种系（行区组），每个种系的4只白鼠按体重分成4个级别（列区组）。可以看出，完全随机设计、随机区组设计和拉丁方设计的处理因素没有变化，都是比较注射4种不同剂量雌激素带来的差别，只是改变了设计的方法，在同样的试验条件下，通过改进试验设计方法可以大大提高试验的效率，如上述试验，白鼠按体重配成区组后再施加处理（随机区组设计），试验的误差均方通常小于完全随机设计。\n而上述介绍的各种试验设计方法，严格地说，它们仅适用于只有1个处理因素的试验问题之中，其他因素都属于区组因素，即与处理因素无交互作用。若试验所涉及的处理因素的个数为2或以上，当各因素在试验中所处的地位基本平等，且因素之间存在1级（即2因素之间）、2级（即3因素之间）乃至更复杂的交互作用时，则需选用析因设计。\n在评价药物疗效时，除需知道A药和B药各剂量的疗效外（主效应），还需知道两种药同时使用的协同疗效，析因设计及其相应的方差分析用于分析药物的单独效应、主效应和交互效应。\n主效应：某因素各水平的平均效应；单独效应：在每个B水平，A的效应；或在每个A水平，B的效应；交互效应：某因素各水平的单独效应随另一因素水平变化而变化，则称两因素之间存在交互效应，包括协同、拮抗作用。\n析因设计有交互作用的二因子方差分析将总偏差平方和做如下分解：\n\n误差偏差平方和：反映随机误差对试验结果的影响；\n因子A引起的偏差平方和：除含有误差波动外，反映因子A对试验结果的影响；\n因子B引起的偏差平方和：除含有误差波动外，反映因子B对试验结果的影响；\n因子A与B的交互作用的偏差平方和：反映因子A与B的交互作用对试验结果的影响。\n\n如果不存在交互效应，则只需考虑各因素的主效应；在方差分析中，如果存在交互效应，解释结果时，要逐一分析各因素的单独效应，找出最优搭配。在两因素析因设计时，只需考虑一阶交互效应；3个因素及以上时，除一阶交互效应外，还需考虑二阶、三阶等高阶交互效应，解释将更复杂。"
  },
  {
    "objectID": "blog/2020/02/22/index.html",
    "href": "blog/2020/02/22/index.html",
    "title": "试验设计与方差分析（4）",
    "section": "",
    "text": "重复测量设计\n重复测量是指对同一观察对象的同一观察指标在不同的时间点上进行多次测量，用于分析观察指标在不同时间上的变化规律。\n这类测量资料在医学研究中比较常见，例如，药效分析中常分析给药后不同时间的疗效比较，由于同一受试对象在不同时点的观测值之间往往彼此不独立，存在某种程度的相关，因此不能满足常规统计方法所要求的独立性假定，使得其分析方法有别于一般的统计分析方法。\n最常见的重复测量资料是前后测量设计，如高血压患者治疗前后的血压，其设计与配对设计t检验的试验表达完全相同，但却是两种不同类型的设计，其区别在于：\n\n配对设计中同一对子的两个试验单位可以随机分配处理，两个试验单位同期观察试验结果，可以比较处理组间差异，而前后测量设计不能同期观察试验结果；\n配对t检验要求同一对子的两个试验单位的观察结果分别与差值相互独立，差值服从正态分布，而前后测量设计前后两次测量的结果通常与差值不独立；\n配对设计用平均差值推断处理作用，而前后测量设计除了分析平均差值外，还可以进行相关回归分析。\n\n重复测量设计大体有两类，一类是对每个人在同一时间不同因子组合间测量，另外一类是对每个人在不同时间点上重复。前者常见于裂区设计，而后者常见于经典试验设计，即包括前测、处理一次或几次后测的情况，后者比前者要多见。不论沿裂区方向还是沿时间点方向重复，个体内因子无一例外的都是重复测量因子。\n重复测量设计的特点是一定有个体内因子，但不一定有个体间因子，后者是不同处理组合或不同个体组，而且即使有不同组群（如男性和女性），但人人都经历重复测量，而不是一组接受重复测量，另一组不接受。\n具有重复测量的设计，即在给予某种处理后，在几个不同的时间点上从同一个受试对象身上重复获得指标的观测值，有时是从同一个个体的不同部位上重复获得指标的观测值。由于这种设计符合许多医学试验本身的特点，故在医学科研中应用的频率相当高。\n如果试验中共有k个试验因素，其中只有m个因素与重复测量有关，则称为具有m个重复测量的k因素设计。重复测量资料来源于同一受试对象的某一观测值的多次重复测量，常见的重复测量数据的基本格式：N个观测个体，g个处理组，X为观测指标，p为重复测量次数。\n在对重复测量资料进行方差分析时，除了要求样本是随机的，在处理的同一水平上观测是独立的，及每一水平的测定值都来自正态总体外，特别强调协方差的复合对称性或球形性。因此，在进行重复测量资料的方差分析前，应先对资料的协方差阵进行球形性检验。若满足球形性要求，则直接进行方差分析；若不满足球形性要求，则需对与时间有关的F统计量分子、分母的自由度进行校正，以减少犯I类错误的概率，或直接进行多变量方差分析。\n对重复测量试验数据的方差分析，需考虑两个因素的影响：一个因素是处理分组，可通过施加干预和随机分组来实现；另一个因素是测量时间，由研究者根据专业知识和要求确定。因此重复测量资料的变异可分解为处理因素、时间因素、处理和时间的交互作用、受试对象间的随机误差和重复测量的随机误差等5部分。\n重复测量设计的优点：每一个体作为自身的对照，克服了个体间的变异，分析时可更好地集中于处理效应，且被试者自身差异的问题不再存在，即减少了一个差异来源；重复测量设计的每一个体作为自身的对照，研究所需的个体相对较少，因此更加经济。\n重复测量设计的缺点：滞留效应，前面的处理效应有可能滞留到下一次的处理；潜隐效应，前面的处理效应有可能激活原本以前不活跃的效应；学习效应，由于是逐步熟悉试验，因此研究对象的反应能力有可能逐步得到提高。\n对于重复测量资料的分析处理，我们应用较多的是单变量方差分析的一般线性模型方法。在R数据格式中，重复测量资料同一观察单位在各测量点的测量值用一组变量来表示，计算时将这一组变量当作一个整体作为反应变量来处理。"
  },
  {
    "objectID": "blog/2020/03/10/index.html",
    "href": "blog/2020/03/10/index.html",
    "title": "线性代数之向量",
    "section": "",
    "text": "空间是贯穿整个线性代数的主干脉络和核心概念。下面我们学习利用向量这个工具对空间进行定量描述。\n\n关于向量\n直观地说，把一组数字排列成一行或一列，就称为向量。它可以作为对空间进行描述的有力工具。\n如一个简单的二维向量[4,5]⊤，这个向量有两个成分：第一个成分是数字4，第二个成分是数字5。其可以理解为二维平面中x坐标为4，y坐标为5的一个点，也可以理解为以平面中的原点(0,0)为起点，(4,5)为重点的一条有向线段，这就是二维向量的空间表示。一个向量中成分的个数就是该向量的维数。\n不过，向量也不局限于用来直接描述空间中的点坐标和有向线段，也可以凭借基础的数据表示功能，成为一种描述事物属性的便捷工具。\n如你的考试成绩为：语文85分，数学92分，英语89分。由于这三门课具有不同科目属性，因此，可以使用一个三维向量来对其进行表示，即score=[85,92,89]⊤。此时不仅仅可以把向量score看作是一个盛放数据的容器，似乎也可以利用它讲科目考试成绩和空间建立起某种关联。\n又如，在自然语言处理中，也少不了向量这个重要的工具。程序在进行文本阅读时，首先会对文本材料进行分词处理，然后使用向量对词汇进行表示。这是因为向量很适合将对象的属性和特征对应到高维空间中进行定量表达，同时在此基础上进行进一步的后续处理，如判断词汇之间的相似性等。\n我们陆续接触到一些数据处理的基本方法：如投影、降维等，这些方法都是在向量描述的基础上实现的。\n\n\n列向量\n向量对应地就拥有两种表达方式：若元素是纵向排列的，就称为列向量；若元素是横向排列的，就成为行向量。在没有特殊说明的情况下，一般都默认为列向量。为什么会偏爱列向量呢？主要是为了方便后续的向量坐标变换、空间之间的映射等计算过程的处理。\n将一个矩阵A所表示的映射作用于某个向量x上时，习惯上将其写成矩阵乘以向量的表达形式，即Ax。这种写法的数据表示基础就是：向量x必须是一个列向量。"
  },
  {
    "objectID": "blog/2020/03/12/index.html",
    "href": "blog/2020/03/12/index.html",
    "title": "空间中的向量",
    "section": "",
    "text": "我们知道，向量的坐标表示方法并不是唯一的，它的具体表示和空间中基底的选择密切相关。\n\n向量的坐标\n向量的坐标依赖于选取的基底。\n对于二维向量u=[4,5]⊤而言，我们一直以来都理所当然地认定一个事实：它表示一条在x轴上投影为4、y轴上投影为5的有向线段，它的坐标是(4,5)。这其实是基于一个没有刻意强调前提：利用方向为x轴、y轴正方向，且长度为1的两个向量，即Ex=[1,0]⊤，Ey=[0,1]⊤作为上述讨论的基础。因此，对于向量u而言，其完整的写法应该为u=4Ex + 5Ey，进一步展开就是u=4[1,0]⊤ + 5[0,1]⊤，这种形式的表意是最完整的。\n这里被选中作为向量u基准的一组向量是Ex和Ey，它们被称为基底。基底的每一个成员向量被称为基向量，而坐标对应的就是各个基向量前的系数。一般情况下，若不做特殊说明，那么基向量都是选取沿着坐标轴正方向且长度为1的向量，这样方便描述和计算。\n关于向量u的完整准确的说法是：在基底(Ex,Ey)下，其坐标是[4,5]⊤。也就是说，坐标必须依托于指定的基底才有意义。因此，要想准确地描述向量，首先就要确定一组基底，然后通过求出向量在各个基向量上的投影值，最后才能确定在这个基上的坐标值。\n\n\n向量在不同基底上表示为不同坐标\n一个指定的向量可以在多组不同的基底上进行坐标表示，在不同的基底表示下，坐标自然也是不同的。根据一组基底对应的坐标值去求另一组基底所对应的坐标值，这就是以后我们将会反复用到的坐标变换。\n根据我们之前关于向量内积的介绍，最好是事先把基向量的模长转化为1。这样一来，从向量内积的内涵可以看出，若基向量的模长是1，那么就可以用目标向量内积基向量，从而可以直接获得该向量在这个基向量方向上的对应坐标值。实际上，对于任何一个向量，想要找到同方向上模长为1的向量并不是一件难事，只要让向量的各成分分别除以向量的模长即可，就能使向量的模长为单位1。而向量的坐标就是指定基的对应系数。\n\n\n构成基底的条件\n在一个n维空间中，不是随便选取n个向量都能作为一组基底，构成基底的向量必须满足这样的条件：在n维空间中，任意一个向量都可以表示为这一组基向量的线性组合，并且这种线性组合的表示方式（也就是系数）必须是唯一的。\n\n向量数量足够\n若想成为三维空间中的一组基底，首先，其中的每个基向量的维数都必须是3；其次，基向量的个数也必须为3个。若数量不足，如只有两个三维向量a1和a2（假设它们是不共线的两个向量），那么无论对这两个向量怎么进行线性组合，它们都只能表示二者所构成的平面上的任意向量，而三维空间中位于该二维平面上外的任何一个向量，都无法由a1和a2的线性组合进行表示。\n满足线性无关\n如何确保表示方法的唯一性呢？这里我们引入向量线性无关的概念。一组向量需要满足线性无关的条件，即其中任何一个向量都不能通过其余向量的线性组合的形式进行表示。\n换句话说，当且仅当x1=x2=x3=…=xn=0的等式关系成立时，线性组合x1u1 + x2u2 + x3u3 + … + xnun才能生成零向量，若xi中有非零值存在，那么这一组向量就是线性相关的。一组向量满足线性无关的条件等效于满足线性组合表示方法的唯一性（可以从反证法的角度说明线性无关和表示方法的唯一性是等价的）。\n在这个三维空间中，要求所选取的3个基向量线性无关。若它们线性相关，那么x3就可以表示为x1和x2的线性组合，换句话说，备选的3个向量就处在一个平面上了。这样，自然无法通过线性组合的方法来表示三维空间中位于平面外的任何一个向量了，即3个三维向量之间由于彼此线性相关，因此无法张成整个三维空间，只能张成三维空间中的二维平面甚至是退化为一条直线。\n若三维空间中基向量的个数超过3个，则是不行的。如，假设有4个向量试图成为该空间的一组基向量，任选出其中的3个向量，按照前提，假设它们之间满足线性无关性，那么对于第4个向量，由于它也处于三维空间中，则它一定能够被前3个向量的线性组合所表示。那么，三维空间中的这4个向量显示是线性相关的，无法满足向量构成基底的唯一性条件。\n\n\n\n构成基底的条件\n对于一组向量，由它的所有线性组合所构成的空间称为这一组向量的张成空间。张成空间对所讨论向量的线性无关性没有要求，这些向量可以是线性相关的。\n两个线性无关的二维向量，它们构成了二维空间中的一组基底，因此它们的张成空间就是整个二维空间；两个线性相关的共线二维向量，它们的张成空间是一条穿过原点的一维直线；等。\n向量的个数和维数都不是其张成空间维数及形态的决定因素，具体的情况需要结合向量的线性无关性进行整体考量，这就会涉及秩的相关概念。"
  },
  {
    "objectID": "blog/2020/04/25/index.html",
    "href": "blog/2020/04/25/index.html",
    "title": "R语言4.0发布上线",
    "section": "",
    "text": "昨天，R语言的4.0正式版本已经上线，小伙伴们可以去更新下载了，mac版本的地址：https://mirrors.tuna.tsinghua.edu.cn/CRAN/bin/macosx/R-4.0.0.pkg。\n一些小知识点：\n\nR包安装路径：/Library/Frameworks/R.framework/Versions/Current/Resources/library/；\n不同版本的R包会放在不同的路径下：/Library/Frameworks/R.framework/Versions；\n一般软件都会有很好的向下兼容性，高版本普遍能很好地支持低版本中的功能，但R比较特殊，很多人的电脑上会安装多个版本的R。R包的版本是跟着R的版本走的，所以你要是想用新版R环境中的新R包，就必须更新R；要是想用旧版的低版本R包，最好使用旧版R。最好的应对R版本更迭的方式就是：在你的电脑中安装多个版本的R，这样，使用R包的时候，就不用担心受R版本的影响了。"
  },
  {
    "objectID": "blog/2020/04/27/index.html",
    "href": "blog/2020/04/27/index.html",
    "title": "矩阵的运算",
    "section": "",
    "text": "矩阵的加法运算\n矩阵之间的加法运算必须运用在两个相等规模的矩阵之间，即行数和列数都相等的两个矩阵才能做加法运算，原因非常容易理解：因为需要将参与加法运算的两个矩阵对应位置上的元素分别进行相加，才能得到最终的结果矩阵。\n\nimport numpy as np\nA = np.array([[1, 2],\n              [3, 4],\n              [5, 6]])\nB = np.array([[10, 20],\n              [30, 40],\n              [50, 60]])\nprint(A+B)\n\n[[11 22]\n [33 44]\n [55 66]]\n\n\n\n\n矩阵的数量乘法运算\n将参与运算的标量数字分别与矩阵的每一个元素相乘，得到结果矩阵对应的新元素，显然，得到的结果矩阵的大小规模是不变的。\n\nA = np.array([[1, 4],\n              [2, 5],\n              [3, 6]])\nprint(2*A)\n\n[[ 2  8]\n [ 4 10]\n [ 6 12]]\n\n\n\n\n矩阵与矩阵的乘法\n不是随意两个矩阵都可以相乘，乘法运算对两个矩阵的形态是有特定要求的，主要有以下3条：\n\n左边矩阵的列数和右边矩阵的行数必须相等\n左边矩阵的行数决定最终结果矩阵的行数\n右边矩阵的列数决定最终结果矩阵的列数\n\n\nA = np.array([[1, 2],\n              [3, 4],\n              [5, 6],\n              [7, 8]])\nB = np.array([[2, 3, 4, 5],\n              [6, 7, 8, 9]])\nprint(np.dot(A, B))\n\n[[ 14  17  20  23]\n [ 30  37  44  51]\n [ 46  57  68  79]\n [ 62  77  92 107]]\n\n\n\n\n矩阵乘以向量\n矩阵与向量的乘法，一般是将矩阵A写在左边，列向量x写在右边，这种 Ax 形式的写法便于描述向量x的空间位置在矩阵A的作用下进行变换的过程。\n正如前面文章所讲，矩阵与向量的乘法可以看作是矩阵与矩阵乘法的一种特殊形式，只不过位于后面的是一个列数为1的特殊矩阵而已。\n对应前面矩阵与矩阵的乘法规则，矩阵与向量的乘法规则主要有：\n\n矩阵在左，列向量在右，矩阵的列数和列向量的维数必须相等\n矩阵和列向量相乘的结果也是一个列向量\n矩阵的行数就是结果向量的维数\n乘法运算的实施过程就是矩阵的每行和列向量的对应元素分别相乘之后再进行相加\n\n\nA = np.array([[1, 2],\n              [3, 4],\n              [5, 6]])\nx = np.array([[4, 5]]).T\nprint(np.dot(A, x))\n\n[[14]\n [32]\n [50]]\n\n\n从结果来看，原始向量表示二维平面上的一个点，其平面坐标为(4,5)，经过矩阵A的乘法作用，最终将原始点转化为三维空间中的一个新的目标点，其空间坐标为(14,32,50)。\n从这个例子中，我们可以总结出矩阵所发挥的重要作用：在指定矩阵的乘法作用下，原始空间中的向量被映射转换到了目标空间中的新坐标，向量的空间位置由此发生了变化，甚至在映射后，目标空间的维数相较于原始空间都有可能发生改变。\n具体这些空间位置的改变的规律，以及其背后更深层次的内涵，我们会在后面的文章中一起讨论学习。"
  },
  {
    "objectID": "blog/2020/04/29/index.html",
    "href": "blog/2020/04/29/index.html",
    "title": "矩阵乘向量的新视角：空间映射",
    "section": "",
    "text": "我们前面提到：在指定矩阵的乘法作用下，原始空间中的向量被映射转换到了目标空间中的新坐标，向量的空间位置由此发生了变化，甚至在映射之后，目标空间的维数相较于原始空间都有可能发生改变。\n这就引入了矩阵乘向量的一个新视角。\n我们知道，向量需要设定一组具体的基底来进行自身的坐标表示。而实际上，矩阵与向量的乘法，本质上可以看作是对向量坐标表示的基底的一种改变。\n当我们从列的角度来审视矩阵A与向量x的乘法运算，会发现本质上，是对矩阵A的各个列向量进行线性组合的过程，每个列向量的组合系数就是向量x的各个对应成分。\n于是，可以按照列的角度重新把矩阵A写成一组列向量并排的形式，然后将其再与向量x进行乘法运算，这样一看，它所包含的几何意义就更加清楚了。即，一个矩阵和一个列向量相乘的过程可以理解为对位于原矩阵各列的列向量重新进行线性组合的过程。\n向量的坐标表示需要依托于基底的选取，只有明确了基底，向量的坐标表示才有实际意义。如，我们说一个二维列向量的坐标是x和y，我们其实已经默认它的基底是二维平面的x轴和y轴正方向上的单位向量。\n而矩阵与向量相乘后，我们发现，列向量的基底被变换为新的基底，正是矩阵的各列，矩阵的各列称为新的基向量。\n总结一般的情况：矩阵A是m*n的一般矩阵，其中，m不等于n，而向量x是一个n维列向量，没有任何的特殊性，映射前后，列向量x的基向量维数发生了变化：原始的n维列向量x被变换了n个m维列向量线性组合的形式，最终的运算结果是一个m维的列向量。\n由此可以看出，映射后的向量维数和原始向量维数的关系取决于矩阵维数m和n的关系：若m大于n，那么映射后的目标向量维数就大于原始列向量的维数，注意：即使矩阵的n个列向量线性无关，由于n个列向量不能张成m维空间，不能表示m维空间中的所有向量，所以称为m维目标空间的基底是不妥当的；若m小于n，那么目标向量的维数就小于原始列向量的维数，注意：由于列向量实现了降维，显然这n个m维列向量线性相关，因此准确地来说，不能构成基底；若m=n，则列向量的维数保持不变，此时，若n个m维向量线性无关，此时，矩阵的各列向量才能称为目标空间中的一组新基底。\n矩阵A的各个列是列向量x默认基底经过转换后的目标向量，正因为存在维度和线性相关性方面的多种不同情况，所以这组目标向量的张成空间和原始列向量所在的原始空间之间就存在着多种不同的对应关系。\n这也是我们后面所要介绍的空间映射的内容。"
  },
  {
    "objectID": "blog/2024/05/11/optimal_threshold/index.html",
    "href": "blog/2024/05/11/optimal_threshold/index.html",
    "title": "最优分类阈值",
    "section": "",
    "text": "这里我们借助scikit-learn来探讨分类问题中阈值的选择。\n\n数据准备和参数选择\n首先是数据准备：\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nnp.set_printoptions(suppress=True, precision=8, linewidth=1000)\npd.options.mode.chained_assignment = None\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\n\ndata = load_breast_cancer()\nX = data[\"data\"]\ny = data[\"target\"]\n\nXtrain, Xvalid, ytrain, yvalid = train_test_split(X, y, test_size=.20, random_state=516)\n\nprint(f\"Xtrain.shape: {Xtrain.shape}\")\nprint(f\"Xvalid.shape: {Xvalid.shape}\")\n\nXtrain.shape: (455, 30)\nXvalid.shape: (114, 30)\n\n\n模型我们这里选择随机森林。超参的选择，基于GridSearchCV，这里也不赘述。有一个点需要说明，由于使用的是肿瘤数据集，在这种情况下，我们更关注的是recall，即尽量减少假阴性的情况。因而，我们在训练模型时，也是将recall作为评价指标。\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    \"n_estimators\": [100, 150, 250],\n    \"min_samples_leaf\": [2, 3, 4],\n    \"ccp_alpha\": [0, .1, .2, .3]\n    }\n\nmdl = GridSearchCV(\n    RandomForestClassifier(random_state=516), \n    param_grid, \n    scoring=\"recall\", \n    cv=5\n    )\n\nmdl.fit(Xtrain, ytrain)\n\nprint(f\"best parameters: {mdl.best_params_}\")\n\nbest parameters: {'ccp_alpha': 0, 'min_samples_leaf': 4, 'n_estimators': 100}\n\n\n\n\n模型预测\n拿到模型后，自然我们可以开始预测：\n\nypred = mdl.predict_proba(Xvalid)[:,1]\nypred\n\narray([0.005     , 0.82743637, 0.97088095, 0.        , 0.        , 1.        , 0.98020202, 0.67380556, 0.        , 0.99333333, 0.9975    , 0.30048576, 0.9528113 , 0.99666667, 0.04102381, 0.99444444, 1.        , 0.828226  , 0.        , 0.        , 0.97916667, 1.        , 0.99607143, 0.90425163, 0.        , 0.02844156, 0.99333333, 0.98183333, 0.9975    , 0.08869769, 0.97369841, 0.        , 1.        , 0.71100866, 0.96022727, 0.        , 0.71200885, 0.06103175, 0.005     , 0.99490476, 0.1644127 , 0.        , 0.23646934, 1.        , 0.57680164, 0.64901715, 0.9975    , 0.61790818, 0.95509668, 0.99383333, 0.04570455, 0.97575758, 1.        , 0.47115815, 0.92422619, 0.77371415, 0.        , 1.        , 0.26198657, 0.        , 0.28206638, 0.95216162, 0.98761905, 0.99464286, 0.98704762, 0.85579351, 0.10036905, 0.00222222, 0.98011905, 0.99857143, 0.92285967, 0.95180556, 0.97546947, 0.84433189, 0.005     , 0.99833333, 0.83616339, 1.        , 0.9955    , 1.        , 0.99833333, 1.        ,\n       0.86399315, 0.9807381 , 0.        , 0.99833333, 0.9975    , 0.        , 0.98733333, 0.96822727, 0.23980827, 0.7914127 , 0.        , 0.98133333, 1.        , 1.        , 0.89251019, 0.9498226 , 0.18943254, 0.83494391, 0.9975    , 1.        , 0.77079113, 0.99722222, 0.30208297, 1.        , 0.92111977, 0.99428571, 0.91936508, 0.47118074, 0.98467172, 0.006     , 0.05750305, 0.96954978])\n\n\n这个时候，我们要讲的东西就来了。一般地，我们会选择0.50作为分类阈值，即大于0.50的为正类，小于0.50的为负类。\n\nypred = mdl.predict_proba(Xvalid)[:,1].reshape(-1, 1)\nyhat = mdl.predict(Xvalid).reshape(-1, 1)\npreds = np.concatenate([ypred, yhat], axis=1)\nprint(preds)\nprint(confusion_matrix(yvalid, yhat))\n\n[[0.005      0.        ]\n [0.82743637 1.        ]\n [0.97088095 1.        ]\n [0.         0.        ]\n [0.         0.        ]\n [1.         1.        ]\n [0.98020202 1.        ]\n [0.67380556 1.        ]\n [0.         0.        ]\n [0.99333333 1.        ]\n [0.9975     1.        ]\n [0.30048576 0.        ]\n [0.9528113  1.        ]\n [0.99666667 1.        ]\n [0.04102381 0.        ]\n [0.99444444 1.        ]\n [1.         1.        ]\n [0.828226   1.        ]\n [0.         0.        ]\n [0.         0.        ]\n [0.97916667 1.        ]\n [1.         1.        ]\n [0.99607143 1.        ]\n [0.90425163 1.        ]\n [0.         0.        ]\n [0.02844156 0.        ]\n [0.99333333 1.        ]\n [0.98183333 1.        ]\n [0.9975     1.        ]\n [0.08869769 0.        ]\n [0.97369841 1.        ]\n [0.         0.        ]\n [1.         1.        ]\n [0.71100866 1.        ]\n [0.96022727 1.        ]\n [0.         0.        ]\n [0.71200885 1.        ]\n [0.06103175 0.        ]\n [0.005      0.        ]\n [0.99490476 1.        ]\n [0.1644127  0.        ]\n [0.         0.        ]\n [0.23646934 0.        ]\n [1.         1.        ]\n [0.57680164 1.        ]\n [0.64901715 1.        ]\n [0.9975     1.        ]\n [0.61790818 1.        ]\n [0.95509668 1.        ]\n [0.99383333 1.        ]\n [0.04570455 0.        ]\n [0.97575758 1.        ]\n [1.         1.        ]\n [0.47115815 0.        ]\n [0.92422619 1.        ]\n [0.77371415 1.        ]\n [0.         0.        ]\n [1.         1.        ]\n [0.26198657 0.        ]\n [0.         0.        ]\n [0.28206638 0.        ]\n [0.95216162 1.        ]\n [0.98761905 1.        ]\n [0.99464286 1.        ]\n [0.98704762 1.        ]\n [0.85579351 1.        ]\n [0.10036905 0.        ]\n [0.00222222 0.        ]\n [0.98011905 1.        ]\n [0.99857143 1.        ]\n [0.92285967 1.        ]\n [0.95180556 1.        ]\n [0.97546947 1.        ]\n [0.84433189 1.        ]\n [0.005      0.        ]\n [0.99833333 1.        ]\n [0.83616339 1.        ]\n [1.         1.        ]\n [0.9955     1.        ]\n [1.         1.        ]\n [0.99833333 1.        ]\n [1.         1.        ]\n [0.86399315 1.        ]\n [0.9807381  1.        ]\n [0.         0.        ]\n [0.99833333 1.        ]\n [0.9975     1.        ]\n [0.         0.        ]\n [0.98733333 1.        ]\n [0.96822727 1.        ]\n [0.23980827 0.        ]\n [0.7914127  1.        ]\n [0.         0.        ]\n [0.98133333 1.        ]\n [1.         1.        ]\n [1.         1.        ]\n [0.89251019 1.        ]\n [0.9498226  1.        ]\n [0.18943254 0.        ]\n [0.83494391 1.        ]\n [0.9975     1.        ]\n [1.         1.        ]\n [0.77079113 1.        ]\n [0.99722222 1.        ]\n [0.30208297 0.        ]\n [1.         1.        ]\n [0.92111977 1.        ]\n [0.99428571 1.        ]\n [0.91936508 1.        ]\n [0.47118074 0.        ]\n [0.98467172 1.        ]\n [0.006      0.        ]\n [0.05750305 0.        ]\n [0.96954978 1.        ]]\n[[35  3]\n [ 1 75]]\n\n\n但是，这个阈值是可以调整的。我们可以通过调整阈值来达到不同的目的。比如，我们可以通过调整阈值来减少假阴性的情况，这在类别不平衡时尤为重要。\n\n\n阈值的选择\n我们介绍几种常用的方法。\n\n1. 阳性类别prevalance\n我们看下这个数据集中阳性类别的比例：\n\nprint(f\"Proportion of positives in training set: {ytrain.sum() / ytrain.shape[0]:.2f}\")\n\nProportion of positives in training set: 0.62\n\n\n这个toy数据集很夸张哈，达到了0.62。在实际应用中，这个比例可能只有10%或者1%。这里我们只是拿它示例哈，用这个prevalance来作为阈值。\n\nthresh = 1- ytrain.sum() / ytrain.shape[0]\nyhat = np.where(ypred &lt;= thresh, 0, 1)\nprint(confusion_matrix(yvalid, yhat))\n\n[[34  4]\n [ 0 76]]\n\n\n考虑prevalance的方法，可以在类别不平衡的情况下，减少假阴性的情况。\n\n\n2. 最优F1指数\nF1指数是precision和recall的调和平均数。我们可以通过最大F1指数来选择最优的阈值。\n\n\nThreshold using optimal f1-score: 0.471.\n\n\nF1最高为0.471，我们采用它来进行预测：\n\nthresh = .471\nyhat = np.where(ypred &lt;= thresh, 0, 1)\nprint(confusion_matrix(yvalid, yhat))\n\n[[34  4]\n [ 0 76]]\n\n\n\n\n3. ROC曲线\n我们可以通过ROC曲线来选择最优的阈值。ROC曲线下的面积AUC越大，说明模型越好。我们可以选择ROC曲线最靠近左上角的点作为最优阈值。\n\n\n/opt/hostedtoolcache/Python/3.10.18/x64/lib/python3.10/site-packages/sklearn/utils/_plotting.py:175: FutureWarning: `**kwargs` is deprecated and will be removed in 1.9. Pass all matplotlib arguments to `curve_kwargs` as a dictionary instead.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n4. PRC曲线\nPRC曲线是precision-recall曲线。相比于ROC曲线，PRC曲线更适合类别不平衡的情况。我们主要选择PRC曲线最靠近右上角的点作为最优阈值。\n\n\n\n\n\n\n\n\n\nSelected threshold using precision-recall curve: 0.674.\n\n\n\n\n5. 分别关注precision和recall\n我们可以通过调整阈值来分别关注precision和recall。比如，我们可以通过调整阈值来提高recall，减少假阴性的情况。\n\n\n\n\n\n\n\n\n\n\n代码已经放进了星球里。"
  },
  {
    "objectID": "blog/2024/05/17/calibration/index.html",
    "href": "blog/2024/05/17/calibration/index.html",
    "title": "Python中机器学习模型的校准",
    "section": "",
    "text": "calibration\n在我们利用机器学习模型来建模分类预测时，首要关注的指标能力当然是dircrimination，即模型的预测区分能力。常见的指标有sensitivity、specificity、AUROC等。我们在上一篇文章中介绍了如何选择最优分类阈值，这里我们接着介绍在选择了最优阈值后，如何评估模型的校准能力。\n所谓校准能力，即模型预测的概率与实际发生的概率一致。\n通俗来解释这个事情：比如说，我们模型预测某个病人患病的概率是0.8，那么，按照概率定义理解，模型预测概率为0.8时，100个人中应该有80个人最终患病，这个结果体现了模型的校准能力和稳定性。如果模型预测概率为0.8时，实际只有20个人患病，那么，模型的校准能力就不够好，你也不会信任这个模型在实际应用中的预测结果。这就是校准能力的重要性，即你的模型最终输出的概率值要准确反映出事件实际发生的概率。\n\n\n如何评价calibration\n\ncalibration plot\n\n\n\ncalibration curve\n\n\n上图是一个典型的calibration curve，也是我们在文章中常见的图。\n我们将模型预测概率cut或者quantile成5或者10个区间（bin），每个区间预测概率的均值作为x轴，每个区间实际发生的概率作为y轴，然后画出来这个曲线。这个图是评价模型校准能力的一个直观指标。python中可以轻松实现这个工作：\n\nfrom sklearn.calibration import calibration_curve\ny_means, pred_means = calibration_curve(y_true, y_pred, n_bins=10,strategy)\n\n理想情况下，所有点都在对角线上，即模型预测的概率与实际发生的概率完全一致。如果点在对角线上方，说明模型低估，反之，高估。\ncalibration level的定义有：\n\n\n\ncalibration level(Alonzo 2009)\n\n\n\n\n其他指标\n除了calibration plot，我们还可以用其他指标来评价模型的校准能力，比如说Brier score、Hosmer-Lemeshow test、calibration in the large等。这里不做详细介绍。\n我们感兴趣的是，当我们通过上述方法评价了模型的校准能力后，如果发现模型的校准能力不够好，我们应该怎么办？\n\n\n\ncalibrate model\n我们已经发现，模型输出值并不能代表概率。python中一般有predict_proba方法，即这个方法其实并不能保证输出的概率是真实的概率。\n\nfrom sklearn.ensemble import RandomForestClassifier\nmodel= RandomForestClassifier().fit(X_train, y_train)\ny_pred = model.predict_proba(X_test)[:,1]\n\n所以，我们需要对模型进行校准。\n\nPlatt scaling\nPlatt scaling是一种常见的校准方法，其原理是对模型输出的概率以及真实标签，用一个logistic regression模型来拟合，从而实现对模型输出的概率进行校准，拿到最终的概率。\n\nfrom sklearn.calibration import CalibratedClassifierCV\ncalibrated = CalibratedClassifierCV(model, method='sigmoid', cv=5)\ncalibrated.fit(X_train, y_train)\n\n\n\nIsotonic regression\nIsotonic regression是另一种校准方法，其原理是对模型输出的概率以及真实标签，用一个isotonic regression模型来拟合，从而实现对模型输出的概率进行校准。\n\nfrom sklearn.isotonic import IsotonicRegression\nir = IsotonicRegression().fit(y_pred, y_test)\n\n\n\nbayesian binning into quantiles\nBBQ是一种基于贝叶斯的校准方法，其原理是将预测概率分成若干个区间，然后在每个区间内对概率进行校准。该方法结合了分箱（binning）和贝叶斯推断的优点，可以在样本量较小时仍然保持较好的校准效果。\n还有其他方法可以供尝试。\n\n\n\ntake home message\n在利用机器学习模型进行分类预测时，我们不可忽视模型的校准能力。\n代码已经放进了星球里。\n\n\n\n\n\n\n\nReferences\n\nAlonzo, T. A. 2009. “Clinical Prediction Models: A Practical Approach to Development, Validation, and Updating: By Ewout w. Steyerberg.” Generic. Oxford University Press.\n\nCitationBibTeX citation:@online{lu2024,\n  author = {Lu, Zhen},\n  title = {Python中机器学习模型的校准},\n  date = {2024-05-17},\n  url = {https://leslie-lu.github.io/blog/2024/05/17/calibration/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLu, Zhen. 2024. “Python中机器学习模型的校准.” May 17, 2024.\nhttps://leslie-lu.github.io/blog/2024/05/17/calibration/."
  },
  {
    "objectID": "blog/2024/06/23/JCR_2023/index.html",
    "href": "blog/2024/06/23/JCR_2023/index.html",
    "title": "2023年最新JCR影响因子发布",
    "section": "",
    "text": "2023年最新JCR影响因子\n最新的影响因子前几天已经发布了，和去年一样，大家在公众号后台回复”JCR2023”，即可拿到最全的总结excel表格，包括2023年的最新影响因子，以及各个学科的排名，希望对大家有所帮助。\n\n\n关注的一些期刊\n几乎全部的期刊影响因子都回落到了几年前的水平。\n四大神刊中JAMA几近腰斩，柳叶刀系列的多个子刊也是如此。\n\n\n\n柳叶刀系列\n\n\n医工交叉领域也是普遍下滑。medical informatics数字医疗部分，lancet digital health和npj Digital Medicine分别是23.8和12.4分。\n以往动辄二三十分的盛况不再。"
  },
  {
    "objectID": "blog/2024/08/03/propensity_score_weighting/index.html",
    "href": "blog/2024/08/03/propensity_score_weighting/index.html",
    "title": "倾向性评分加权",
    "section": "",
    "text": "背景介绍\n对于非RCT类的观察性研究，由于分组的非随机性，导致了研究偏倚的存在，致使观察到的效应很多时候往往并不可用。为了解决这个问题，研究者们提出了倾向性评分的方法，通过倾向性评分的计算，可以使得试验组和对照组之间的分布更加接近，从而减少了研究偏倚的影响。\n我们公众号以往有过五篇类似的介绍内容，分别是：\n\n倾向性评分分析\n倾向性评分分析的统计学考虑\n倾向性评分匹配的生存分析怎么做\n倾向性评分overlapping weighting的SAS实现（一）\n生存资料倾向性评分OW的SAS实现（二）\n\n其中，后面两篇文章提到了倾向性评分加权中的overlapping weighting方法，这篇文章将对倾向性评分加权的方法进行详细介绍。\n\n\n因果效应\n首先，我们来看下几种因果效应。\nATE即平均处理效应（average treatment effect），是指在试验组和对照组之间的处理效应的差异。理想情况下，随机对照试验估计出来的效应即ATE，但是在实际研究中，由于种种原因，我们往往无法进行随机对照试验。由于ATE的估计人群是试验组和对照组的总体，ATE假设两组受试者是有相同的概率/机会接受某一种处理的，然而，实际研究中，研究者往往更加关注的是ATE的局部估计，即在某一特定的人群中（一般是接受治疗的试验组），处理的效应是多少，而这个效应即为ATT（average treatment effect on the treated）。由于ATT只需要对处理组人群估计因果处理效应，对于RCT而言，潜在的治疗效果和治疗组分配是相互独立的，因此，ATT即为ATE；然而，对于非RCT类研究而言，二者是不同的。\n我们还可以计算ATC（average treatment effect on the control），即对于未接受治疗的人群，如果接受治疗，其效应是多少。此外，还有ATM（average treatment effect among the evenly matchable），即在对照组中，找到与试验组相匹配的人群，计算在这个匹配的总体人群中的治疗效应；ATO（average treatment effect among the overlap population），即在试验组和对照组的重叠人群中，计算治疗效应。相比于ATM，ATO有着更好的方差属性，由于其不像ATM那样匹配要求，转而是选择两组重叠的中间人群，因此，ATO的估计更加稳健。\n\n\n倾向性评分加权\n针对以上五种因果效应，我们可以通过倾向性评分加权的方法来进行相应的估计。这里直接给出五种权重的计算公式：\n\nATE：\\(w_{ATE} = \\frac{Z_i}{e_i} + \\frac{1 - Z_i}{1 - e_i}\\)\nATT：\\(w_{ATT} = \\frac{e_iZ_i}{e_i} + \\frac{e_i(1-Z_i)}{1-e_i}\\)\nATC：\\(w_{ATC} = \\frac{(1-e_i)Z_i}{e_i} + \\frac{(1-e_i) (1-Z_i)}{1 - e_i}\\)\nATM：\\(w_{ATC} = \\frac{(1-e_i)Z_i}{e_i} + \\frac{(1-e_i) (1-Z_i)}{1 - e_i}\\)\nATO：\\(w_{AT0} = (1-e_i)Z_i + e_i(1-Z_i)\\)\n\n以上五种加权的示例以及具体实现的全部代码，我们已经放在了星球里，感兴趣的同学可以自行查看。\n这里我们展示下加权后人群的分布情况。\n\n原始人群的ps分布\n\n\n\nps of original population\n\n\n\n\nATE\n\n\n\nATE\n\n\n\n\nATT\n\n\n\nATT\n\n\n\n\nATC\n\n\n\nATC\n\n\n\n\nATM\n\n\n\nATM\n\n\n\n\nATO\n\n\n\nATO\n\n\n\n\n\n总结\n相信通过以上可视化的展示，大家会更容易理解倾向性评分加权的方法对目标人群的选择以及治疗效应的解释。借助于合适的效应加权，我们可以估计出治疗效应并对于以上五种治疗效应的估计值。"
  },
  {
    "objectID": "blog/2024/08/11/workshop_002/index.html",
    "href": "blog/2024/08/11/workshop_002/index.html",
    "title": "星球第二期workshop上线",
    "section": "",
    "text": "我们星球正式上线第二期workshop啦！\n\n\n\nworkshop\n\n\n本期workshop主题是“Sample Size Calculations in Clinical Research”。本期workshop将从临床研究中不同试验设计的角度出发，介绍如何基于不同设计类型（平行设计、交叉设计、析因设计、成组序贯设计等）、比较类型（非劣效、等效、优效试验）、主要终点指标等因素进行样本量计算。\n\n\n\nscreenshot\n\n\n目前暂定的安排是每周一节一小时，直到本期workshop的全部内容结束。\n\n\n\nused files\n\n\n后续我们会根据星球中同学们的需求，不定时地开展更多主题的workshop，具体时间请关注我们的公众号和星球，我们会在这两个平台上发布最新的信息。"
  },
  {
    "objectID": "blog/2024/08/20/nested_layout/index.html",
    "href": "blog/2024/08/20/nested_layout/index.html",
    "title": "大图嵌小图",
    "section": "",
    "text": "由来\n星球里不断有同学问到如何在一个大图中嵌入小图，这里简单介绍一下。\n\n\nQ1\n\n\n\nQ2\n\n我们使用生存曲线及risk table作为例子，其中生存曲线是大图，risk table是小图。常见的图形为：\n\n\nsurvival curve\n\n想要把risk table嵌入到生存曲线中。\n方法一\n使用grid包，借助于grid包中的viewport函数。viewport用于定义一个绘图区域，可以在一个图形设备中创建多个独立的绘图区域，每个区域都有自己的坐标系和尺寸。\n\nsubvp &lt;- viewport(width = 0.35, height = 0.35, x = 0.75, y = 0.75)\nggsurv$plot\nprint(ggsurv$table, vp = subvp)\n\nviewport创建了一个子视口，它定义了一个相对主视口的区域。效果如下：\n\n\noption 1\n\n方法二\n使用annotation_custom函数，它可以在图形中添加自定义的图形元素。\n\nggsurv$plot + annotation_custom(ggplotGrob(ggsurv$table), xmin=1900, xmax=3000, ymin=0.6, ymax=1)\n\nggplotGrob将ggsurv$table转换为grob对象，以便在图形中使用。效果如下：\n\n\noption 2\n\n方法三\n使用ggpp包。\n\nsub_plot= tibble::tibble(\n    x= .98, y= .98, plot= list(ggsurv$table)\n)\nggsurv$plot + \n    geom_plot_npc(data = sub_plot, aes(npcx = x, npcy = y, label = plot))\n\n使用geom_plot_npc函数将子图添加到主图中，label表示要添加的子图。效果如下：\n\n\noption 3\n\n完整代码已经放在了星球里，感兴趣的同学可以自行查看。"
  },
  {
    "objectID": "blog/2024/08/30/jc/index.html",
    "href": "blog/2024/08/30/jc/index.html",
    "title": "星球JC | 胃癌早期筛查工具",
    "section": "",
    "text": "大家好，这一期预测模型星球Journal Club的分享来自中国医科大学的徐林玉同学，分享的是2019年发表在中科院医学1区的顶级期刊Gut上，题为“Development and validation of a prediction rule for estimating gastric cancer risk in the Chinese high-risk population: a nationwide multicentre study”的研究论文。\n\n\n研究背景\n胃癌是中国第二常见的癌症，早期检测和治疗可以显著降低其死亡率。然而，由于高风险人群庞大，全面的胃镜筛查在经济和操作上都不切实际。当前，中国的国家筛查指南建议对高风险人群从40岁开始进行筛查，但由于高风险人群估计超过3亿人，全面的胃镜筛查并不可行。因此，迫切需要一种风险分层工具，作为胃镜检查前的初步筛查工具，以进一步识别真正的高风险个体；且当前中国国内尚无类似的工具。\n现有的筛查工具主要基于已知的胃癌风险因素，如萎缩性胃炎和幽门螺杆菌感染。虽然有一些方法如ABC方法在日本被开发用于预测未来胃癌的发生，但其在中国高风险人群中的适用性仍存在疑问。此外，现有的生物标志物组合方法虽然在某些研究中表现良好，但其结果可能不适用于中国的高风险人群。因此，这项全国多中心横断面研究的目标是开发一种新的预测规则，用于二级预防（早发现、早诊断、早治疗），作为初步筛查工具，用于在中国无症状人群中识别高风险个体，以便进一步进行诊断性胃镜检查。\n\n\n研究方法\n\n研究类型\n全国、多中心、横断面研究。\n\n\n研究人群\n年龄在40至80岁之间、无胃肠道症状的个体，符合中国胃癌高风险标准，并前往医院进行胃镜筛查。\n\n\n数据收集\n通过问卷调查、血清学检测（PG I、PG II、G-17、抗幽门螺杆菌IgG抗体）、胃镜检查和组织学检查收集数据。\n\n\n统计分析\n文章对数据的管理采取了中心化的管理，尽可能确保数据质量。参与者按2:1比例被随机分为开发队列和验证队列。开发队列用于模型开发，验证队列用于外部验证。文章利用Logistic回归模型开发预测规则。在开发队列中，通过univariate and multivariate analyses评估危险因素与胃癌的关联，其中，univariate analyses阈值设为p&lt;0.25，且multivariate analyses采用backward stepwise进行进一步的变量筛选。文章预测规则的设定基于regression coefficient-based scoring method（Points were assigned by dividing the regression coefficients by the absolute value of the smallest coefficient in the model and rounding up to the nearest integer）。模型整体性能通过R²和Brier评分进行衡量，区分能力通过AUC和discrimination slope评估，校准能力通过Hosmer-Lemeshow χ2统计量和calibration in the large进行评估。同时，评估模型预测的敏感度、特异度、准确率、阳性预测值（positive predictive value）、阴性预测值（negative predictive value）、阳性似然比（positive likelihood ratio）、阴性似然比（negative likelihood rario）和number needed to screen（defined as the number of participants who would need to undergo gastroscopy for one patient with GC to be identified）。文章对开发队列进行bootstrap抽样1000次作为内部验证，验证队列上进行外部验证。同时，基于u test比较模型在开发队列、验证队列上的AUC表现。此外，文章额外做了一部分工作，即将自身模型与当前现有文献中的预测模型进行效果的对比。数据分析使用IBM SPSS和R软件进行。\n\n\n\n研究结果\n此研究收集了2015年6月至2017年3月期间来自中国115家医院的数据，最终纳入14,929名合格参与者。\n\n\n\n图1 研究对象纳入及定义\n\n\n\n预测规则的开发\n基于分数的预测规则的开发：采用单因素和多因素分析，确定了包括年龄、性别、PG I/II比率、G-17水平、幽门螺杆菌感染和饮食习惯（腌制和油炸食品消费）在内的7个预测因素。\n\n\n\nTable 1\n\n\n按照0–25分评分范围进行风险分层，将个体分为低风险（≤11）、中风险（12–16）和高风险（17–25）组。\n\n\n\n图2 logistic回归模型中胃癌的预测因素及相关预测规则\n\n\n其中，中高风险组的胃镜检查检测到70.8%的胃癌病例和70.3%的早期胃癌病例。低风险组的胃镜检查需求减少了66.7%。\n\n\n\nTable 4\n\n\n预测性能：开发的预测规则具有良好的性能表现。\n\n\n\n图3 预测模型的性能指标\n\n\n\n\n与其他预测模型的性能比较\n该预测规则表现良好，并显示出显著优于其他三种替代预测方法（即Miki等人的ABC方法、中国的基于ELISA的ABC方法以及Tu等人的五种生物标志物方法）在识别胃癌患者方面的区分能力。\n\n\n\n图4 验证队列中预测模型和替代预测模型的比较\n\n\n\n\n\nTake home message\n\n该预测规则在识别中国人群中胃镜检查前的高风险个体方面表现良好。可以作为一种准确且具有成本效益的大规模初步筛查工具，以提高胃癌（包括早期胃癌）的检测率，从而改善胃癌的二级预防。\n文章不仅比较了自身模型在多中心外部验证中的效果，更是对比了现有模型，且表现均优于现有模型。\n采用回归赋分的方式，对于人工智能算法解释性较好，能较好地在医院中进行推广应用；此外，采用赋分划分亚组的方式，关注到了模型实际可能带来的收益。"
  },
  {
    "objectID": "blog/2024/09/06/jc/index.html",
    "href": "blog/2024/09/06/jc/index.html",
    "title": "星球JC | 心脏功能障碍诊断模型",
    "section": "",
    "text": "大家好，这一期预测模型星球Journal Club的分享来自中国中医科学院西苑医院的小格格同学。\n\n今天分享的是2024年发表在中科院医学1区的顶刊JACC: CardioOncology上，题为 A Biomarker-Based Diagnostic Model for Cardiac Dysfunction in Childhood Cancer Survivors (Leerink et al. 2024) 的研究论文。\n\n\n\nfigure abstract\n\n\n\n研究背景\n心血管疾病越来越多地在长期儿童癌症幸存者（CCS, childhood cancer survivors）中受到关注。在接受蒽环类药物、米托蒽醌和/或胸部定向放疗治疗的CCS患者中，约11%在癌症诊断后40年内发生心力衰竭。目前推荐终生超声心动图监测，通过早期发现左心室功能障碍（LV功能障碍）来预防或延缓心衰，监测间隔根据蒽环类药物、米托蒽醌和胸部定向放疗的累积剂量确定。\n而心脏生物标志物在CCS长期监测中的作用仍不确定。生物标志物可能能够作为一种具有成本效益的鉴别检测，帮助确定是否继续或延迟应用超声心动图。如果血液生物标志物检测可以有效排除LV功能障碍，超声心动图可以推迟到下一个计划的监测时间点。尽管以往研究报告N末端B型利钠肽前体（NT-proBNP）和高敏心肌肌钙蛋白T（hs-cTnT）在长期CCS中检测LV功能障碍的诊断准确性有限，以致不建议使用心脏生物标志物进行CCS监测，但仍存在一些未解决的问题：\n\n尽管先前的研究将心脏生物标志物作为检测LV功能障碍的独立诊断检测进行了研究，但若将其与临床信息结合可能会提高诊断性能。\nCCS相关研究尚未探索用于rule out或rule in LV功能障碍的特定生物标志物的cutoff浓度，而这是提高诊断性能的潜在因素。\n目前尚不清楚 CCS 中心脏生物标志物是否可能更有效地诊断更为严重的LV功能障碍，这一点在之前的一项针对普通人群的研究中得到了提示。\n\n基于横断面多中心研究，文章致力于开发和内部验证将心脏生物标志物与临床特征相结合的诊断模型，主要目的是确定其在先前未诊断出心肌病的 CCS 患者中rule in或rule out LV功能障碍的有效性。\n\n\n研究方法\n\n研究类型\n多中心、横断面研究。\n\n\n研究人群\nCCS及其兄弟姐妹，其中，兄弟姐妹作为CSS的对照。\n\n\n数据收集\n通过问卷调查、血清学检测（NT-proBNP、hs-cTnT和肌酐水平、肾小球滤过率）、超声心动图检查收集数据。\n\n\n统计分析\n缺失值处理：探索了缺失值的可能模式，假设缺失值随机缺失（MAR），使用predictive mean matching进行重复20次的多重填补，继而基于每个填补的数据集进行独立分析，使用Rubin’s rules进行汇总，此外，文章做了一步填补结果与complete case analysis结果的比较。\n生物标志物与LVEF的关系：通过local polynomial regression拟合CCS中生物标志物浓度与LVEF之间的关系。\n预定义诊断标准：基于先前对呼吸困难患者心力衰竭诊断的研究，建立预定义的标准来rule out（阴性预测值NPV&gt;=98%，敏感度&gt;=90%）和rule in（阳性预测值PPV&gt;=75%，特异度&gt;=90%）LV功能障碍。\n评价诊断准确性：包括AUC、敏感度、特异度、PPV和NPV；校准曲线进行评价；NRI（net reclassification improvement）。\n预测模型的开发：基于logistic。第一个模型仅考虑临床预测因素，使用后向筛选关键变量，阈值设为0.05，由于使用了多重填补，最终选择出现在一半以上模型的变量进入最终模型；第二个模型加入NT-proBNP和hscTnT，使用pooled Wald test检验这两个变量给模型拟合带来的改善，此外，考虑到生物标志物与终点的非线性关系，文章进行了限制性立方样条分析。通过500次bootstrap进行内部验证。\n\n\n\n研究结果\n此研究共纳入1334例CCS，共有278名兄弟姐妹作为对照。\n\n\n\nTable 1\n\n\n与兄弟姐妹相比，CCS患者的心脏生物标志物：NT-proBNP水平的中位数和异常比例都高于他们的兄弟姐妹，hs-cTnT浓度无显著差异。\n\n仅使用生物标志物建模\nlocal polynomial regression发现随着LVEF的降低，CCS患者中的NT-proBNP和hs-cTnT浓度呈现升高的趋势，特别是当LVEF下降到低于50%时，这种趋势更为明显。但单独使用NT-proBNP异常或hs-cTnT不足以rule out或rule in任何类型的LV功能障碍。\n\n\n\nTable 2\n\n\n\n\n结合临床特征与生物标志物\n将心脏生物标志物NT-proBNP和hs-cTnT结合到临床特征中，可以显著提高模型能力，特别是用于rule out LV功能障碍。\n\n\n\nTable 3\n\n\n\n\n\nNRI\n\n\n且校准度较好。\n\n\n\nFigure 2\n\n\n\n\n\nTake home message\n\n文章在关注常规模型评价指标的基础上，特别关注biomarker对模型rule out结局的影响，从rule in和rule out两方面来评价多个模型的不同用途。\n文章通过建立多个生物标志物结合临床特征的诊断模型，展示了其最优cutoff值下可能的预测价值，同时对最新指南提到的高风险人群更新了分析。\n\n\n\n\n\n\n\n\nReferences\n\nLeerink, Jan M, Elizabeth AM Feijen, Esmee C de Baat, Remy Merkx, Helena JH van der Pal, Wim JE Tissing, Marloes Louwerens, et al. 2024. “A Biomarker-Based Diagnostic Model for Cardiac Dysfunction in Childhood Cancer Survivors.” Cardio Oncology 6 (2): 236–47."
  },
  {
    "objectID": "blog/2024/09/09/einsum/index.html",
    "href": "blog/2024/09/09/einsum/index.html",
    "title": "爱因斯坦求和",
    "section": "",
    "text": "在物理学、机器学习、科学计算等领域，张量的操作是常见而又复杂的。而在张量操作中，有一种优雅的数学简化工具，即爱因斯坦求和约定（Einstein Summation Convention），它极大地减少了公式的复杂性，特别是在高维张量的运算中。\n\n什么是爱因斯坦求和？\n爱因斯坦求和约定是由著名物理学家阿尔伯特·爱因斯坦提出的一种简化公式的符号约定。根据这一约定，当一个表达式中有重复的索引变量时，默认对该索引进行求和运算，而不需要显式地写出求和符号。\n例如，两个矩阵 (A) 和 (B) 的乘法可以通过爱因斯坦求和简化。通常，矩阵乘法公式为：\n\\[\nC_{ik} = \\sum_{j} A_{ij} B_{jk}\n\\]\n在爱因斯坦求和的符号下，重复出现的索引 (j) 表示对其求和，因此公式可以写为：\n\\[\nC_{ik} = A_{ij} B_{jk}\n\\]\n我们用torch实现这个例子：\n\nimport torch\na = torch.arange(6).reshape(2, 3)\nb = torch.arange(15).reshape(3, 5)\ntorch.einsum('ik,kj-&gt;ij', [a, b])\n\ntensor([[ 25,  28,  31,  34,  37],\n        [ 70,  82,  94, 106, 118]])\n\n\n\ncolumn sum\n\na = torch.arange(6).reshape(2, 3)\ntorch.einsum('ij-&gt;j', [a])\n\ntensor([3, 5, 7])\n\n\n\n\nrow sum\n\ntorch.einsum('ij-&gt;i', [a])\n\ntensor([ 3, 12])\n\n\n\n\nsum\n\ntorch.einsum('ij-&gt;', [a])\n\ntensor(15)\n\n\n\n\ntranspose\n\ntorch.einsum('ij-&gt;ji', [a])\n\ntensor([[0, 3],\n        [1, 4],\n        [2, 5]])\n\n\n\n\nbatch matrix multiplication\n\na = torch.randn(3,2,5)\nb = torch.randn(3,5,3)\ntorch.einsum('ijk,ikl-&gt;ijl', [a, b])\n\ntensor([[[ 1.2735,  4.1871, -6.4466],\n         [ 5.8136, -2.4947,  2.1005]],\n\n        [[ 1.7047,  0.2791,  2.1914],\n         [ 0.5477, -1.5492, -0.7990]],\n\n        [[ 3.8645,  0.1712, -0.6204],\n         [-4.1222,  0.4563,  2.9974]]])\n\n\n\n\n\n为什么爱因斯坦求和如此重要？\n爱因斯坦求和的核心优势在于它可以极大地简化复杂的张量运算，特别是在涉及高维度张量时。这种简洁的符号不仅减少了书写和理解的困难，还可以避免出错，因为我们不再需要反复地书写求和符号。\n在物理学中，尤其是在广义相对论和量子力学中，张量的运算无处不在，而爱因斯坦求和约定为这些运算提供了极大的便利。"
  },
  {
    "objectID": "blog/2024/09/16/beta_dist/index.html",
    "href": "blog/2024/09/16/beta_dist/index.html",
    "title": "解析 Beta 分布与贝叶斯",
    "section": "",
    "text": "看到小白学统计发了一篇文章介绍 Beta 分布，我们也更新一篇文章解析 Beta 分布与贝叶斯。\n\n1. 什么是 Beta 分布？\nBeta 分布是一种定义在 (0,1) 区间的连续概率分布，用来描述比例或概率等数据。例如，应用于研究某事件发生的概率。它由两个参数 \\(\\alpha\\) 和 \\(\\beta\\) 决定：\n\n\\(\\alpha\\) = \\(\\beta\\) = 1：均匀分布，即所有概率值等可能。\n\\(\\alpha\\) &gt; 1 和 \\(\\beta\\) &gt; 1：生成钟形分布，集中于中间。\n\\(\\alpha\\) &lt; 1 或 \\(\\beta\\) &lt; 1：分布会更靠近 0 或 1。\n\n这种灵活性使得 Beta 分布特别适合于建模那些约束在 (0,1) 之间的变量，比如成功概率、比率等。其概率密度函数为：\n\\[\nf(x; \\alpha, \\beta) = \\frac{x^{\\alpha-1} (1-x)^{\\beta-1}}{B(\\alpha, \\beta)}\n\\]\n其中，B(\\(\\alpha\\), \\(\\beta\\)) 为 Beta 函数，相当于归一化因子，使得概率密度函数的积分等于 1。\nBeta 分布广泛用于表示成功概率的先验分布，是 贝叶斯推断 中的常用工具。通过观察数据，不断更新 Beta 分布的 \\(\\alpha\\) 和 \\(\\beta\\) 参数，进而得到后验分布。\n\n\n2. 贝叶斯视角下的 Beta 分布\n在贝叶斯统计中，Beta 分布通常作为 二项分布 的共轭先验分布。\n\n当先验分布和后验分布属于同一族分布时，我们说这个先验分布是似然函数的共轭先验。共轭分布的主要优点是它们简化了后验分布的计算。\n\n例如，如果我们有某事件的观测数据，并且希望估计成功概率 p，假设 p 的先验分布为 Beta 分布，则观测到 n 次成功和 m 次失败后，新的后验分布仍为 Beta 分布，其更新规则为：\n\\[\n\\alpha_{\\text{后验}} = \\alpha_{\\text{先验}} + n\n\\]\n\\[\n\\beta_{\\text{后验}} = \\beta_{\\text{先验}} + m\n\\]\n这意味着 Beta 分布可以在贝叶斯分析中通过数据观察逐步更新。这也是小白说统计文章里例子的原理来源。\n\n\n3. betareg 包的功能\nbetareg 包是 R 语言中用于处理 Beta 回归模型的工具，适用于 (0,1) 区间的比例数据建模。这些数据通常不适合使用传统的线性回归模型，因为响应变量的范围受到限制。Beta 回归模型假设响应变量服从 Beta 分布，提供了一种灵活的方式来处理这些约束性数据。主要功能包括：\n\n处理边界值：如接近 0 或 1 的极端数据点。\n偏差校正：特别在样本量小的情况下，能够进行精确估计。\n扩展模型：支持有限混合模型，用于处理具有不同组别特征的数据。\n\n\n\n4. Beta 回归的应用场景\nBeta 回归在实际数据分析中有许多应用，尤其在以下几类问题中：\n\n生物统计学：如疾病发病率、药物疗效等。\n金融：如股票收益率、风险度量等。\n市场营销：如用户转化率、广告点击率等。"
  },
  {
    "objectID": "blog/2024/10/02/oop/index.html",
    "href": "blog/2024/10/02/oop/index.html",
    "title": "R 中面向对象编程系统选择",
    "section": "",
    "text": "我们在很久以前稍微介绍过 R 语言的面向对象编程，但是没有深入对比。今天我们将讨论 R 语言中的面向对象编程系统，包括 S3、S4 和 R6。\n\n在R语言中，面向对象编程（OOP，object oriented programming）提供了多种系统供用户选择。每种系统都有其独特的特点、优缺点以及适用场景。\nOOP系统概述\nR主要有三种面向对象编程系统：\n\nS3\nS4\nR6\n\nS3 系统\nS3 是一种轻量级的面向对象系统，主要通过使用属性来实现。它的优点在于简单易用，适合快速开发。\n特点\n\n灵活性: S3 允许任意对象成为类。\n简洁性: 类定义和方法定义非常简单。\n动态性: 可以在运行时添加方法和属性。\n示例代码\n\nmy_s3_class &lt;- function(x) {\n  structure(list(value = x), class = \"my_class\")\n}\n\nprint.my_class &lt;- function(obj) {\n  cat(\"The value is:\", obj$value, \"\\n\")\n}\n\nobj &lt;- my_s3_class(10)\nprint(obj)\n\nThe value is: 10 \n\n\nS4系统\nS4 系统比 S3 更严格，支持正式的类定义和方法。适合需要高可靠性的应用。\n特点\n\n严格性: 类和方法都需要明确定义。\n类型检查: 提供更好的类型安全性。\n多重继承: 支持多重继承，增加灵活性。\n示例代码\n\nsetClass(\"myS4Class\", slots = list(value = \"numeric\"))\n\nsetMethod(\"show\", \"myS4Class\", function(object) {\n  cat(\"The value is:\", object@value, \"\\n\")\n})\n\nobj &lt;- new(\"myS4Class\", value = 20)\nshow(obj)\n\nThe value is: 20 \n\n\nR6 系统\nR6 是最新的 OOP 系统，支持封装、私有字段和方法，使得代码更模块化。\n特点\n\n封装性: 支持私有字段和方法。\n简洁性: 语法与其他 OOP 语言相似。\n高效性: 对性能有较好的优化。\n示例代码\n\nlibrary(R6)\n\nMyR6Class &lt;- R6Class(\"MyR6Class\",\n  public = list(\n    value = NULL,\n    initialize = function(value) {\n      self$value &lt;- value\n    },\n    print_value = function() {\n      cat(\"The value is:\", self$value, \"\\n\")\n    }\n  )\n)\n\nobj &lt;- MyR6Class$new(30)\nobj$print_value()\n\nThe value is: 30 \n\n\n选择合适的系统\n在选择 R 中的 OOP 系统时，考虑项目的复杂性、可维护性及团队的熟悉程度是关键。\n\nS3: 适合快速开发和简单应用。适合初学者或小型项目。\nS4: 适合需要严格类型检查和复杂结构的应用。常用于研究领域或需要高可靠性的项目。\nR6: 适合需要封装和私有成员的场景。适合大型项目和复杂数据结构的开发。\n\n完整代码已经放在了星球里，感兴趣的同学可以自行查看。"
  },
  {
    "objectID": "blog/2024/11/26/LD/index.html",
    "href": "blog/2024/11/26/LD/index.html",
    "title": "连锁不平衡",
    "section": "",
    "text": "连锁不平衡（Linkage Disequilibrium, LD）\n连锁不平衡（LD） 是群体遗传学和基因组学中的一个基本概念，用于描述两个或多个遗传标记（通常是单核苷酸多态性，SNP）之间的非随机关联。当某些 SNP 的等位基因 （allele）的组合在一个群体中出现的频率高于根据单独等位基因频率计算的预期频率时，这种现象被称为连锁不平衡。LD 是基因组关联分析（GWAS）和基因组选择等研究的重要基础，用于识别与复杂性状相关的遗传变异，并帮助推断遗传位点之间的关系。\n连锁是指基因组中靠得很近的基因或遗传标记倾向于一起遗传的现象，这是由染色体重组的随机性和空间上的接近性引起的。而 LD 则描述了在一个群体中等位基因组合的统计学偏差，如果遗传标记之间的组合频率偏离预期（非随机关联），就表明存在 LD。\n\n\nLD的数学描述\n连锁不平衡通常用两个指标来描述：\\(D'\\) 和 \\(r^2\\)。\n\n1. 不平衡系数（D）\n定义两个标记 \\(A\\) 和 \\(B\\) 的四种可能等位基因组合及其频率： - \\(A_1B_1\\), \\(A_1B_2\\), \\(A_2B_1\\), \\(A_2B_2\\)，分别表示两位点上的不同等位基因组合。\n设： - \\(P(A_1B_1)\\)：观察到 \\(A_1\\) 和 \\(B_1\\) 同时出现的频率。 - \\(P(A_1)\\)：\\(A_1\\) 的边际频率。 - \\(P(B_1)\\)：\\(B_1\\) 的边际频率。\n则不平衡系数 \\(D\\) 定义为：\n\\[\nD = P(A_1B_1) - P(A_1)P(B_1)\n\\]\n\n当 \\(D = 0\\) 时，标记之间没有 LD，即独立遗传。\n当 \\(D \\neq 0\\) 时，标记之间存在 LD。\n\n然而，由于 \\(D\\) 的值依赖于等位基因的频率，其绝对值没有固定的范围，因此通常会对其进行标准化。\n\n\n2. 标准化不平衡系数（\\(D'\\)）\n\\[\nD' = \\frac{D}{D_{\\text{max}}}\n\\]\n其中 \\(D_{\\text{max}}\\) 是 \\(D\\) 的可能最大值或最小值，取决于等位基因频率。\n\n\\(D'\\) 的范围为 [-1, 1]。\n\\(D' = 1\\) 表示完全连锁不平衡。\n\n\n\n3. 相关系数（\\(r^2\\)）\n但在群体遗传学中，其实我们更常用 \\(r^2\\) 来衡量 LD：\n\\[\nr^2 = \\frac{D^2}{P(A_1)P(A_2)P(B_1)P(B_2)}\n\\]\n\n\\(r^2\\) 的范围为 [0, 1]。\n\\(r^2 = 0\\)：标记之间完全独立。\n\\(r^2 = 1\\)：标记之间完全关联。\n\\(r^2\\) 的值通常用于评估 GWAS 研究中的 SNP 冗余。"
  },
  {
    "objectID": "blog/2024/11/29/TRIPOD_AI/index.html",
    "href": "blog/2024/11/29/TRIPOD_AI/index.html",
    "title": "星球 JC | 解读 TRIPOD-AI",
    "section": "",
    "text": "大家好，这一期预测模型星球 Journal Club 的分享来自中国医科大学的陈奕含同学。\n\n\nTRIPOD-AI 的制定\nTRIPOD 协作组于 2015 年公布了《个体预后或诊断的多变量预测模型透明报告》（Transparent Reporting of a Multivariable Prediction Model for Individual Prognosis or Diagnosis，TRIPOD），用以规范预测模型的报告过程；随后为进一步提高报告质量评价的客观性及一致性，协作组于 2019 年公布了 TRIPOD 报告规范依从性评判标准，此标准也可作为提高研究报告准确性和完整性的指导文件。\n由于人工智能与机器学习的发展，更多研究将人工智能算法应用于预测模型的开发，因此在前期工作的基础上，TRIPOD 协作组于 2024 年发表了名为 TRIPOD-AI：updated guidance for reporting clinical prediction models that use regression or machine learning methods《TRIPOD-AI：报告使用回归或机器学习方法的临床预测模型的更新指南》的文章，取代 TRIPOD 2015，以此来指导预测模型类文章的报告，提高报告规范性。协作组成员在 TRIPOD 2015 的基础上，结合其他报告中的条目与系统评价的结果，构建了一个唯一候选条目的最终列表，进而进行德尔菲专家修订。通过两轮德尔菲专家会议，包含不同领域的专家。此外，还进行了公众参与会议和共识会议，最终确定了 TRIPOD+AI 声明。\n\n\nTRIPOD+AI 声明条目\nTRIPOD+AI 清单包括标题、摘要、引言、方法、开放科学实践、患者和公众参与、结果和讨论等 27 个主要条目。部分条目包含多个子条目，共计 52 个清单子条目。其涵盖了描述预测模型开发、预测模型性能评估(验证)或两者兼而有之的研究。其中，任何项记为 D；E 适用于所有研究，无论是开发预测模型还是评估预测模型的性能。检查表中的 D 项适用于描述预测模型开发的研究，而 E 项适用于评估预测模型性能的研究。对于开发和评估预测模型性能的研究，所有检查表项都适用。\nTRIPOD+AI 也为预测模型研究的期刊或会议摘要提供了单独的核查表，并与 TRIPOD+AI 保持一致。另外，PROBAST+AI 用来评估预测模型的质量和偏倚风险。需要注意的是，TRIPOD+AI 中包含的建议是最低限度的报告建议，作者还应提供额外的信息，如在补充材料中报告和引用一些所要求的或补充的信息。TRIPOD+AI 强调了整个清单的公平性，其意味着预测模型的设计和使用方式不会对任何特定群体的个人产生不利的歧视，也不会造成或加剧现有的医疗服务或患者结果方面的不平等。公平性的一个重要方面是确保用于开发或评估预测模型的数据具有代表性和多样性（这些差异应该能够代表预测模型所要使用的人群），并在模型开发过程中承认、处理和缓解数据偏差的限制。为了实现公平目标，清单在背景(项目 3c)、方法(项目 5a、7、8a、8b、9c、12f、14)、结果(项 20b、23a)和讨论(第 25、26 项)中都嵌入了相关内容。\nTRIPOD+AI 也添加了开放科学实践的内容，有助于促进透明度、可重复性和研究人员之间的合作。通过注册研究并使研究材料(如协议、数据、代码和预测模型)公开，使其他研究人员可以在新数据中验证研究结果并评估模型性能，以确保模型准确，并评估模型的安全性。在人工智能的背景下，开放科学部分涵盖了资助申报、利益冲突、协议可获得性、研究注册、数据共享、代码共享等方面。\nTable 2 预测模型研究报告的 TRIPOD+AI 清单\n\n\n\n\n\n\n\n\n\n段落/主题\n条目\n开发/评估\n清单条目\n\n\n\n\n标题\n\n\n\n\n\n标题\n1\nD；E\n将研究确定为开发或评估多变量预测模型的性能、目标人群和待预测结果\n\n\n摘要\n\n\n\n\n\n摘要\n2\nD；E\n摘要检查表见 TRIPOD+AI\n\n\n介绍\n\n\n\n\n\n背景\n3a\nD；E\n解释医疗保健背景(包括无论是诊断性还是预后性)以及开发或评估预测模型的理由，包括对现有模型的参考\n\n\n\n3b\nD；E\n描述照护路径情境下预测模型的目标人群和意向目的，包括其意向用户(例如,医疗保健专业人员、患者、公众等)\n\n\n\n3c\nD；E\n描述任何已知的社会人口群体之间的健康不平等\n\n\n目的\n4\nD；E\n明确研究目的，包括研究是否描述了预测模型的开发或验证(或两者兼而有之)\n\n\n方法\n\n\n\n\n\n数据\n5a\nD；E\n分别描述开发和评估数据集(例如,随机试验,队列,常规护理或登记数据)的数据来源、使用这些数据的理由以及数据的代表性\n\n\n\n5b\nD；E\n指定收集到的参与者数据的日期，包括参与者计提的开始日期和结束日期；并且，如果适用，则结束随访\n\n\n研究对象\n6a\nD；E\n指定研究设置(例如,初级保健、次级保健、普通人群)的关键要素，包括中心的数量和位置\n\n\n\n6b\nD；E\n描述研究参与者的合格标准\n\n\n\n6c\nD；E\n给出接受的任何治疗的细节，以及在模型开发或评估过程中如何处理，如果相关\n\n\n数据准备\n7\nD；E\n描述任何数据预处理和质量检查，包括在相关的社会人口统计群体中是否类似\n\n\n结局\n8a\nD；E\n明确定义正在预测的结果和时间范围，包括如何评估和何时评估，选择该结果的理由，以及结果评估的方法是否在社会人口统计学群体中保持一致\n\n\n\n8b\nD；E\n如果结果评估需要主观解释，请描述结果评估者的资质和人口学特征\n\n\n\n8c\nD；E\n报告任何盲目评估预测结局的行为\n\n\n预测因子\n9a\nD\n描述初始预测因子(例如,文献,以前的模型,所有可用的预测器)的选择，以及在模型建立之前对预测因子的任何预选择\n\n\n\n9b\nD；E\n明确定义所有预测因子，包括如何和何时测量(以及对结果和其他预测因素进行盲法评估的任何行动)\n\n\n\n9c\nD；E\n如果预测器测量需要主观解释，则描述预测器评估师的资格和人口统计学特征\n\n\n样本量\n10\nD；E\n说明研究规模是如何达到(分别进行开发和评估)的，并证明研究规模足以回答研究问题。包括任何样本量计算的细节\n\n\n缺失数据\n11\nD；E\n描述缺失数据是如何处理的。提供省略任何数据的理由\n\n\n分析方法\n12a\nD\n描述数据在分析中如何使用(例如,模型性能的开发和评估)，包括是否对数据进行分区，考虑任何样本量要求\n\n\n\n12b\nD\n根据模型的类型，描述在(函数形式,重新标度,转换,或任何标准化)分析中如何处理预测因子\n\n\n\n12c\nD\n指定模型的类型，基本原理，所有模型构建步骤，包括任何超参数的调整，以及内部验证的方法\n\n\n\n12d\nD；E\n描述是否以及如何在集群(例如,医院、国家)中处理和量化模型参数值和模型性能的估计中的任何异质性。其他考虑参见 TRIPOD-Cluster\n\n\n\n12e\nD；E\n指定所有用于评估模型性能的(及其理论基础)和用于比较多个模型的(如区分度、校准度、临床实用性等) (如果相关)的指标和图\n\n\n\n12f\nE\n描述由于模型评估而产生的任何模型更新(例如,重新校准)，无论是总体还是针对特定的社会人口群体或设置\n\n\n\n12g\nE\n对于模型评估，描述如何计算模型预测的(例如,公式、代码、对象、应用程序编程接口等)\n\n\n类别不均衡\n13\nD；E\n如果使用了类别不平衡方法，说明为什么这样做，如何这样做，以及后续任何重新校准模型或模型预测的方法\n\n\n公平性\n14\nD；E\n描述任何用于解决模型公平性的方法及其原理\n\n\n模型输出\n15\nD\n指定预测模型(例如,概率,分类)的输出。为任何分类以及如何确定阈值提供详细信息和理由\n\n\n训练与评估\n16\nD；E\n确定开发和评估数据在医疗保健环境、合格标准、结果和预测因素之间的任何差异\n\n\n伦理批准\n17\nD；E\n命名批准该研究的机构研究委员会或伦理委员会，并描述参与者知情同意或伦理委员会放弃知情同意的情况\n\n\n开放科学\n\n\n\n\n\n资助\n18a\nD；E\n给出本研究的资金来源和资助者的角色\n\n\n利益冲突方\n18b\nD；E\n声明所有作者的利益冲突和财务披露\n\n\n计划书\n18c\nD；E\n说明研究方案可在何处获取或声明方案未准备好\n\n\n注册\n18d\nD；E\n为研究提供注册信息，包括注册名和注册号，或者说明研究未进行注册\n\n\n数据共享\n18e\nD；E\n详细说明研究数据的可获得性\n\n\n代码共享\n18f\nD；E\n提供分析代码可用性的详细信息\n\n\n患者和公众参与\n\n\n\n\n\n患者和公众参与\n19\nD；E\n在研究的设计、进行、报告、解释或传播过程中提供任何患者和公众参与的详细信息，或陈述没有参与的情况\n\n\n结果\n\n\n\n\n\n参与者\n20a\nD；E\n描述研究过程中参与者的流动情况，包括有结果和没有结果的参与者人数，如果适用，则对随访时间进行总结。一张图可能会有帮助\n\n\n\n20b\nD；E\n报告每个数据源或设置的总体和适用的特征，包括关键日期、关键预测因素(包括人口统计特征)、接受的治疗、样本量、结果事件数、随访时间和缺失数据的数量。一张表可能是有帮助的。报告关键人口群体之间的差异\n\n\n\n20c\nE\n对于模型评估，展示了与开发数据分布的重要预测因子(人口统计学,预测因素和结果)的比较\n\n\n模型开发\n21\nD；E\n指定每个分析(例如,模型开发、超参数调优、模型评估等)中参与者和结果事件的数量\n\n\n模型规范\n22\nD\n提供完整预测模型(例如,公式、代码、对象、应用程序编程接口等)的详细信息，允许在新的个体中进行预测，并允许第三方评估和实施，包括访问或重用(例如,免费的,专有的)的任何限制\n\n\n模型表现\n23a\nD；E\n报告模型性能估计的置信区间，包括对任何关键子组的(例如,社会人口学)。考虑情节来辅助呈现\n\n\n\n23b\nD；E\n如果检查，报告了跨集群的模型性能的任何异质性的结果。详情见 TRIPOD-Cluster\n\n\n模型更新\n24\nE\n报告任何模型更新的结果，包括更新后的模型和随后的性能\n\n\n讨论\n\n\n\n\n\n解释\n25\nD；E\n对主要结果进行总体解释，包括目标和先前研究背景下的公平问题\n\n\n局限性\n26\nD；E\n讨论研究(如非代表性样本、样本量、过拟合、缺失数据等)的任何局限性及其对任何偏倚、统计不确定性和可推广性的影响\n\n\n该模型在当前护理背景下的可用性\n27a\nD\n描述在实现预测模型时，应该如何评估和处理质量差或不可用的输入数据(例如,预测值)\n\n\n\n27b\nD\n指定在处理输入数据或使用模型时是否需要用户进行交互，以及需要用户具备何种水平的专业知识\n\n\n\n27c\nD；E\n讨论了未来研究的下一步工作，特别关注模型的适用性和通用性\n\n\n\nTable 3 期刊或会议摘要中预测模型研究的必要报告项目\n\n\n\n\n\n\n\n段落和条目\n清单表条目\n\n\n\n\n标题\n\n\n\n1\n将研究确定为开发或评估多变量预测模型的性能、目标人群和待预测结果\n\n\n背景\n\n\n\n2\n对医疗保健背景进行简要解释，并为开发或评估所有模型的性能提供理论依据\n\n\n目的\n\n\n\n3\n明确研究目标，包括研究是否描述模型开发、评价，或者两者兼而有之\n\n\n方法\n\n\n\n4\n描述数据的来源\n\n\n5\n描述数据收集的合格标准和地点\n\n\n6\n明确模型所要预测的结果，包括预后模型情况下预测的时间范围\n\n\n7\n说明模型类型，总结模型建立步骤，以及内部验证的方法\n\n\n8\n明确用于评估模型性能的度量指标(如区分度、校准度、临床实用性等)\n\n\n结果\n\n\n\n9\n报告参与人数和结果事件\n\n\n10\n汇总最终模型中的预测因子\n\n\n11\n报告模型性能估计(带置信区间)\n\n\n讨论\n\n\n\n12\n对主要结果进行了整体解释\n\n\n注册\n\n\n\n13\n给出注册表或存储库的注册号和名称\n\n\n\n\n\n实例\n\n1 标题：将研究确定为开发或评估多变量预测模型的性能、目标人群和待预测结果。D；E\n信息丰富的标题有助于潜在读者和系统审阅者识别预测模型研究；报告信息丰富的标题，提供有关目标人群和模型预测结果的关键信息。\n实例 1 的标题为《Multimodal Machine Learning-Based Marker Enables Early Detection and Prognosis Prediction for Hyperuricemia：基于多模态机器学习的标志物可以实现高尿酸血症的早期检测和预后预测》，不能直接从标题得知是模型的开发、验证还是二者兼有的研究，“预测”表明该研究是一篇临床预测模型类文章，“高尿酸血症”显示预测结局为发生高尿酸血症，但并未在标题中表明研究的目标人群，根据正文内容可知其目标人群为医院就医的患者，无其他特殊限定。但若研究人群为罹患某一疾病或具备某一特征的人群，则应在标题中体现，能进一步反映出模型的适用范围和研究的临床价值。\n\n\n2 摘要：摘要检查表见 TRIPOD+AI 摘要清单。D；E\n报告针对TRIPOD+AI for Abstracts检查表中每个项目的摘要。\n根据 TRIPOD+AI for Abstracts，在采用结构式摘要，报告研究背景、目的、方法（数据来源、收集情况、预测结果、模型类型及性能度量指标）、结果（样本量和结果事件、预测因子和模型性能）、讨论及注册情况。实例 1 对研究背景略做阐述，并报告数据情况（英国生物库与南方医院队列）、结局事件（ISHUA）、模型性能（报告了相关数值），简略提及基因与临床数据两大类预测因子、但未提及度量指标类别及注册情况。\n\n\n3 背景\n\n3.1：解释医疗保健背景(包括无论是诊断性还是预后性)以及开发或评估预测模型的理由，包括对现有模型的参考。D；E\n描述打算使用或需要模型的医疗保健环境；在现有预测模型可用的情况下，提供开发新模型的明确理由；对于评估现有模型的研究，提供评估的理由，并提供所有正在评估的模型的参考资料。\n实例 2《Use of Machine Learning Models to Predict Death After Acute Myocardial Infarction：利用机器学习模型预测急性心肌梗死后死亡》中对于准备开发的模型需求和明确理由做出了清晰叙述。\n\n\n3.2：描述照护路径情境下预测模型的目标人群和意向目的，包括其意向用户(例如,医疗保健专业人员、患者、公众等)。D；E\n描述谁是开发或评估模型的目标人群，例如，特定年龄、特定国家/地区或患有特定疾病的人；描述模型的预期目的，包括模型旨在支持的临床决策或指导（例如，转诊进行进一步检测或入院，分诊、开始治疗或改变生活方式）以及该模型在护理路径中的预期使用点；描述该模型的预期用户是谁，以及该模型是否适用于医疗保健专业人员、患者、公众或其他。\n实例 3《Explainable machine-learning predictions for the prevention of hypoxaemia during surgery：可解释的机器学习预测用于预防手术中的低氧血症》一文中在背景中阐述了希望应用的医疗群体与应用场景。\n\n\n3.3：描述任何已知的社会人口群体之间的健康不平等。D；E\n在打算使用该模型的医疗保健环境中，描述目标人群中社会人口群体之间的任何已知健康不平等（以及支持健康不平等的引文）。\n\n\n\n4 目的：明确研究目的，包括研究是否描述了预测模型的开发或验证(或两者兼而有之)。D；E\n提供研究所有目标的明确陈述，描述研究是开发预测模型、评估预测模型的性能，还是同时评估两者。\n实例4《Construction of a risk prediction model for detecting postintensive care syndrome—mental disorders：重症监护后综合征-精神障碍风险预测模型的构建》在背景部分明确提及该研究构建模型的类型。\n\n\n5 数据\n\n5.1：分别描述开发和评估数据集(例如,随机试验,队列,常规护理或登记数据)的数据来源、使用这些数据的理由以及数据的代表性。D；E\n提供用于模型开发和模型性能评估的数据来源的描述，包括数据是否（例如）来自随机试验、队列、登记处或电子常规医疗记录；说明研究是使用现有数据还是前瞻性地收集新数据以用于预测模型研究；使用现有数据的位置（即它们最初是出于不同的目的收集的），提供使用这些数据的理由，并评论这些数据的适用性（特别是如果数据是从不同的环境或国家/地区使用到预期目标人群的）和这些数据相对于预期目标人群和背景的代表性；应为所有数据集提供数据源的描述，并单独用于开发和评估；如果使用了任何合成数据，请说明原因，并提供有关如何创建合成数据（和代码，参见第 18f 项）并在研究中使用的所有详细信息。\n实例1《Multimodal Machine Learning-Based Marker Enables Early Detection and Prognosis Prediction for Hyperuricemia：基于多模态机器学习的标志物可以实现高尿酸血症的早期检测和预后预测》。\n\n\n5.2：指定收集到的参与者数据的日期，包括参与者计提的开始日期和结束日期；并且，如果适用，则结束随访。D；E\n指定选择参与者或使用数据的期间的开始和结束日期；对于预测预后的模型，随访持续时间很重要，因此请报告随访结束的日期。\n实例5《Machine learning-based prediction of composite risk of cardiovascular events in patients with stable angina pectoris combined with coronary heart disease: development and validation of a clinical prediction model for Chinese patients：基于机器学习的稳定型心绞痛合并冠心病患者心血管事件复合风险预测：中国患者临床预测模型的开发与验证》。\n\n\n\n6 参与者\n\n6.1：明确研究环境的关键要素（例如，初级保健、二级保健、一般人群），包括中心的数量和位置。D；E\n描述医疗保健场景，以及研究参与者的招募地点；报告研究的地理位置（至少是国家/地区）和中心（包括中心的数量）。\n实例 6《Development and validation of a prediction rule for estimating gastric cancer risk in the Chinese high-risk population: a nationwide multicentre study：在中国高危人群中建立和验证胃癌风险预测规则：一项全国性多中心研究》。\n\n\n6.2：描述研究参与者的合格标准。D；E\n应报告参与者的资格标准，以了解预测模型的潜在适用性和普遍性；这包括报告连续变量的任何限制，例如，用于定义纳入参与者资格的年龄范围。\n实例 4《Construction of a risk prediction model for detecting postintensive care syndrome—mental disorders：重症监护后综合征-精神障碍风险预测模型的构建》。\n\n\n6.3：给出接受的任何治疗的细节，以及在模型开发或评估过程中如何处理，如果相关。D；E\n应报告在随访之前或开始时接受的任何治疗，以及在预测模型的开发或评估期间是否以及如何处理这些治疗（如果相关）；在使用预测模型和测量结果之间接受的任何治疗，这可能会改变结果的概率，应报告（如果相关）。\n实例 7《Utility of Machine Learning Algorithms in Predicting Preoperative Lymph Node Metastasis in Patients With Rectal Cancer Based on Three-Dimensional Endorectal Ultrasound and Clinical and Laboratory Data：基于三维直肠腔内超声和临床及实验室数据的机器学习算法在预测直肠癌患者术前淋巴结转移中的应用》。\n\n\n\n7 数据准备：描述任何数据预处理和质量检查，包括在相关的社会人口统计群体中是否类似。D；E\n描述任何数据清理步骤，包括任何特征工程、原始数据转换、特征缩减和数据质量检查。所有用于数据清理的代码都应该可用（参见第18f项）；对于使用来自多个来源的数据（例如，来自不同研究、队列或注册库的数据）的分析，请描述任何协调（例如，结果和预测因子）；确认关键社会人口群体的所有数据预处理/数据清理步骤是否相似（如果相关）；如果数据预处理/数据清理步骤广泛，考虑在补充材料中报告此信息。\n实例 8《Machine Learning for Predicting Risk and Prognosis of Acute Kidney Disease in Critically Ill Elderly Patients During Hospitalization: Internet-Based and Interpretable Model Study：机器学习预测老年危重症患者住院期间急性肾脏病的风险和预后：基于互联网和可解释模型的研究》。\n\n\n8 结局\n\n8.1：明确定义正在预测的结果和时间范围，包括如何评估和何时评估，选择该结果的理由，以及结果评估的方法是否在社会人口统计学群体中保持一致。D；E\n对于诊断预测模型，应明确定义结果，包括是否使用（广泛接受的）参考标准（真实情况）来确定结果的存在与否；对于预后模型，即预测未来结果的模型，作者应报告结果预测的时间范围。例如，预测心胸手术后28天的死亡风险，或骨质疏松症患者10年的骨折风险。此外，应报告随访期间结果评估的频率；如果使用标准定义，例如使用ICD1代码，则应说明和引用；应报告社会人口群体之间结果评估的任何差异；在某些情况下，可能需要确认没有使用预测因子来定义结果或作为结果的代理。\n实例 5《Machine learning-based prediction of composite risk of cardiovascular events in patients with stable angina pectoris combined with coronary heart disease: development and validation of a clinical prediction model for Chinese patients：基于机器学习的稳定型心绞痛合并冠心病患者心血管事件复合风险预测：中国患者临床预测模型的开发与验证》。\n\n\n8.2：如果结果评估需要主观解释，请描述结果评估者的资质和人口学特征。D；E\n对于需要主观解释的结局（例如，解释影像学检查的结果，描述结局评价者的数量、资格和人口统计学特征）；如果结果的测量和解释需要（额外的）培训或具体说明，则应报告这些；如果内容广泛，请考虑在补充材料中报告此信息。\n实例 7《Utility of Machine Learning Algorithms in Predicting Preoperative Lymph Node Metastasis in Patients With Rectal Cancer Based on Three-Dimensional Endorectal Ultrasound and Clinical and Laboratory Data：基于三维直肠腔内超声和临床及实验室数据的机器学习算法在预测直肠癌患者术前淋巴结转移中的应用》。\n\n\n8.3：报告任何盲目评估预测结局的行为。D；E\n预测的结果应该对预测因子的信息不知情地进行评估——特别是与需要主观解释的结果相关，从而避免数据（标签）泄漏；如果合适，作者应描述结果评估者可以获得哪些信息，并报告任何对结果评估盲法的具体行动。\n\n\n\n9 预测因子\n\n9.1：描述初始预测因子(例如,文献,以前的模型,所有可用的预测因子)的选择，以及在模型建立之前对预测因子的任何预选择。D\n提供有关如何考虑将初始预测因子列表包含在模型构建中的详细信息，以及它们是根据文献的（系统）回顾、临床输入（领域专家）选择的，或者只是使用可用数据中的所有预测因子；如果在模型构建之前进行了任何预测因子的预选，然后提供此操作的详细信息。例如，由于大量缺失数据而在模型构建中遗漏了预测变量，或者预测变量被认为与预测结果不合理（临床）相关；初始预测变量的列表可能很广泛，在这些情况下，建议在补充材料中报告这些预测变量。\n\n\n9.2：明确定义所有预测因子，包括如何和何时测量(以及对结果和其他预测因素进行盲法评估的任何行动)。D；E\n应明确定义建模中包含的所有预测变量，以及度量单位和分类预测变量的所有类别，以便读者和其他人可以复制、实施或评估模型的性能；有关如何以及何时测量预测变量值的详细信息。请注意，预测变量应在打算使用模型之前或之时进行测量；对于需要主观解释的预测变量，将其盲解释为建模中考虑的其他预测变量的值可能很重要（例如，避免数据泄漏）。作者应报告任何使预测变量测量的评估对其他预测变量不知情的行为；特别是对于诊断模型，预测变量的测量应在不了解个体结果的情况下进行，因为这可能会人为地夸大预测变量与结果之间的关联。作者应报告任何使预测因子测量值的评估对结果值视而不见的操作；在某些情况下，预测因子的数量可能非常大，因此在主要手稿中报告它们都是无益的，在这些情况下，明确定义所有预测因子仍然很重要，应考虑在补充材料中报告这一点。\n实例 9《A clinical prediction model based on interpretable machine learning algorithms for prolonged hospital stay in acute ischemic stroke patients: a real-world study：基于可解释性机器学习算法的急性缺血性卒中患者住院时间延长的临床预测模型：真实世界研究》。\n\n\n9.3：如果预测因子测量需要主观解释，则描述预测因子评估者的资格和人口统计学特征。D；E\n对于需要主观解释的预测因子（例如，解释影像学检查的结果），应报告预测因子评估者的资格和人口统计学特征；如果测量和解释需要（额外）培训或具体说明，则应报告这些。这可以在补充材料中报告。\n\n\n\n10 样本量：说明研究规模是如何达到(分别进行开发和评估)的，并证明研究规模足以回答研究问题。包括任何样本量计算的细节。D；E\n描述样本量是如何确定的—这应该单独进行，以确定模型开发所需的样本量和评估模型性能所需的样本量，无论数据是前瞻性收集的还是使用现有数据；提供任何样本量计算中使用的详细信息和所有估计值；如果没有进行正式的样本量计算，例如，使用了所有可用数据，提供数据大小是否足以回答研究问题的理由。\n\n\n11 缺失数据：描述缺失数据是如何处理的。提供省略任何数据的理由。D；E\n数据缺失是一个无处不在的问题。作者应报告正在考虑包含在模型中的每个预测变量的缺失值数量；应报告缺失值的处理情况，包括对缺失原因的任何假设；如果个体（或预测变量）由于缺失值而被遗漏，则应报告此情况，并给出原因；如果已估算缺失值，那么应报告插补任何缺失值的方法的完整详细信息；如果已插补缺失值，请确认它是针对训练和任何测试数据单独完成的（即避免泄漏）。\n实例 2《Use of Machine Learning Models to Predict Death After Acute Myocardial Infarction：利用机器学习模型预测急性心肌梗死后死亡》。\n\n\n12 分析方法\n\n12.1：描述数据在分析中如何使用(例如,模型性能的开发和评估)，包括是否对数据进行分区，考虑任何样本量要求。D\n描述如何使用可用数据来开发模型和评估模型性能，包括是否以及如何对数据进行分区，以及对数据进行分区的原因（例如，模型开发、超参数调整、评估模型性能、内部和外部交叉验证）；如果数据已分区，请报告在分区过程中是否考虑了样本量要求（参见第10项）， 以及分区数据的大小是否足以进行分析和回答研究问题；如果数据已经被划分为训练（包括任何超参数调优数据）和测试数据，请确认没有数据泄漏；如果数据包含来自同一个体的多个记录或样本，并且数据已分区为训练（包括任何超参数优化数据）和测试数据，请确认任何分区数据中没有个体泄漏，如果没有，请如何描述在分析中如何处理这种情况（参见第12c项）。\n实例 9《Machine Learning for Predicting Risk and Prognosis of Acute Kidney Disease in Critically Ill Elderly Patients During Hospitalization: Internet-Based and Interpretable Model Study：机器学习预测老年危重症患者住院期间急性肾脏病的风险和预后：基于互联网和可解释模型的研究》。\n\n\n12.2：根据模型的类型，描述在(函数形式,重新标度,转换,或任何标准化)分析中如何处理预测因子。D\n对于在分析过程中转换（即重新缩放或标准化）的任何预测变量，请描述这是如何完成的；对于任何已进行类别折叠的分类预测变量，例如，由于样本量小/结果事件太少，请提供详细信息和原因。\n\n\n12.3：指定模型的类型，基本原理，所有模型构建步骤，包括任何超参数的调整，以及内部验证的方法。D\n明确说明正在开发的模型类型（例如，逻辑回归、Cox回归、随机森林、神经网络），并提供使用每种模型构建方法的基本原理——考虑预测结果的类型以及预测模型在实践中将如何实施；对于每个模型，清楚地描述模型构建中的所有步骤，包括任何超参数优化、已优化的超参数以及调整方式。如果应用了许多模型构建方法，并且字数限制禁止完整描述，则使用补充材料提供详细信息；对于正在开发多个模型的研究（例如，使用不同的模型构建方法），清楚地描述选择哪个模型的标准（如果有），请参阅关于模型性能的第12e项和第23项）；模型开发过程中的内部验证方法（评估模型性能）应该清楚地描述，例如，是否使用了k折叠交叉验证或引导。阐明在内部评估方法期间是否重放了所有模型构建步骤（包括超参数调整）；清楚地描述用于检查模型稳定性（例如，bootstrapping）（例如，在预测因子选择、预测性能和个体预测方面））；如果数据包含来自同一个体的多个记录或样本，描述在模型构建和内部验证中如何处理此问题（例如，如果使用K折交叉验证，请确认某个人的所有记录/样本是否都包含在同一折中（例如，避免数据泄露）。\n实例 10《Construction of a clinical prediction model for complicated appendicitis based on machine learning techniques：基于机器学习技术的复杂性阑尾炎临床预测模型构建》。\n\n\n12.4：描述是否以及如何在跨集群(例如,医院、国家)中处理和量化模型参数值和模型性能的估计中的任何异质性。其他考虑参见TRIPOD-Cluster。D；E\n如果在模型开发或模型性能评估期间，分析考虑了数据中的任何聚类（例如，通过组合来自多项研究的个体参与者数据，或按医疗中心/医院或国家聚类的数据），则应清楚地描述用于解释聚类的基本原理和方法；对于预测模型研究的具体报告建议，这些研究考虑了模型参数值的聚类和异质性，并且性能，作者应查阅TRIPOD-Cluster检查表。\n\n\n12.5：指定所有用于评估模型性能的(及其理论基础)和用于比较多个模型的(如区分度、校准度、临床实用性等) (如果相关)的指标和图。D；E\n报告用于评估模型性能的所有度量。通常预计至少会提供模型判别和校准（包括校准图）；如果预测模型预测的是事件发生时间结果，则清楚地描述用于解释事件发生时间性质（即删失）的措施和方法。同样，还应说明对任何竞争风险的处理（如果适用）；对于预后模型，报告评估模型预测性能的所有时间点；报告用于图形显示模型性能的方法，例如校准图（具有平滑的校准曲线）和决策曲线；如果正在比较多个模型，即，与现有模型进行比较或比较多种建模方法，那么应清楚地解释用于比较这些模型的方法，以及对卓越性能做出任何判断的标准。\n实例 5《Machine learning-based prediction of composite risk of cardiovascular events in patients with stable angina pectoris combined with coronary heart disease: development and validation of a clinical prediction model for Chinese patients：基于机器学习的稳定型心绞痛合并冠心病患者心血管事件复合风险预测：中国患者临床预测模型的开发与验证》。\n\n\n12.6：描述由于模型评估而产生的任何模型更新(例如,重新校准)，无论是总体还是针对特定的社会人口群体或设置。E\n如果模型在验证后更新，例如重新校准或重新拟合——无论是在整个队列中还是在特定的社会人口群体中，请提供有关用于更新模型的方法的详细信息。\n\n\n12.7：对于模型评估，描述如何计算模型预测的(例如,公式、代码、对象、应用程序编程接口等)。E\n对于在单独的数据集中评估现有模型的研究（即外部验证研究），请提供有关如何计算模型的单个预测的详细信息。如果模型不是免费/公开可用的，请解释预测是如何获得的；如果正在评估回归模型方程，请提供该方程的详细信息（例如，考虑提出此方程，提供对开发该方程的原始研究的引用）；对于评估没有方程的预测模型（例如，神经网络、随机森林）的研究，请提供有关如何进行预测的详细信息，例如，代码、软件对象、API，以及在哪里可以找到这些信息（即URL、DOI）；如果模型的单个预测用于创建风险组或分类（在模型开发中未指定），则应报告有关如何以及为何执行此操作的详细信息（参见第15项）。\n实例 5《Machine learning-based prediction of composite risk of cardiovascular events in patients with stable angina pectoris combined with coronary heart disease: development and validation of a clinical prediction model for Chinese patients：基于机器学习的稳定型心绞痛合并冠心病患者心血管事件复合风险预测：中国患者临床预测模型的开发与验证》。\n\n\n\n13 类别不均衡：如果使用了类别不平衡方法，说明为什么这样做，如何这样做，以及后续任何重新校准模型或模型预测的方法。D；E\n如果使用了类不平衡方法（例如，欠/超采样、SMOTE），则提供这样做的理由，以及如何做到这一点——考虑到对样本量的任何影响（例如，对于欠采样方法）；不平衡校正对模型校准有影响，得出的概率估计值太高（这也对定义任何风险组有影响），描述了用于重新校准模型或模型预测的方法。\n实例 11《机器学习基于不平衡数据预测急性新发缺血性卒中患者院内死亡模型研究》。\n\n\n14 公平性：描述任何用于解决模型公平性的方法及其原理。D；E\n公平性是指确保预测模型不歧视个人或群体，例如基于种族、性别、年龄等个人属性，所有用于解决公平性的方法都应清楚地解释其基本原理；在开发模型和评估其性能时，确保数据包含代表性群体（目标人群）非常重要，研究人员应尝试证明这一点；如果预测模型是使用代表性不足的群体或未包括的特定群体的数据开发的，那么需要在代表性数据中对这些群体进行评估，以评估这些群体中的模型，以提高对发展和评价数据之外的更多个体群体的普遍性。\n\n\n15 模型输出：指定预测模型(例如,概率,分类)的输出。为任何分类以及如何确定阈值提供详细信息和理由。D\n大多数模型输出个体的概率估计，而一些模型将输出转化为分类（例如，分为低风险或高风险组），这应该明确说明。如果已经创建了分类或风险组，则应在护理路径的背景下这样做的基本原理以及这些风险组如何为任何临床决策提供信息；对于生成分类或风险组的模型，应明确报告，并应指定任何阈值（例如，定义组的估计概率范围）（这些是否基于文献， 临床指南、统计考虑或 ad-hoc）；如果已经提供了单个预测模型输出的不确定性区间，则提供有关如何完成此操作的详细信息（例如，使用参数估计的方差-协方差矩阵或共形预测）。\n实例 1《Multimodal Machine Learning-Based Marker Enables Early Detection and Prognosis Prediction for Hyperuricemia：基于多模态机器学习的标志物可以实现高尿酸血症的早期检测和预后预测》。\n\n\n16 训练与评估：确定开发和评估数据在医疗保健环境、合格标准、结果和预测因素之间的任何差异。D；E\n在一个环境、中心或国家开发的预测模型不一定对不同的环境、中心或国家有用。来自不同来源的数据之间的资格标准、结果和预测因子定义可能（故意）不同。描述开发数据与用于评估模型性能的数据之间的任何差异，有助于理解和解释模型在原始模型开发数据上下文中的性能和泛化性。\n\n\n17 伦理批准：命名批准该研究的机构研究委员会或伦理委员会，并描述参与者知情同意或伦理委员会放弃知情同意的情况。D；E\n如果该研究没有机构研究委员会或伦理批准，那么请明确说明，并说明原因。\n例 7《Utility of Machine Learning Algorithms in Predicting Preoperative Lymph Node Metastasis in Patients With Rectal Cancer Based on Three-Dimensional Endorectal Ultrasound and Clinical and Laboratory Data：基于三维直肠腔内超声和临床及实验室数据的机器学习算法在预测直肠癌患者术前淋巴结转移中的应用》。\n\n\n18 开放科学\n\n18.1 资助：给出本研究的资金来源和资助者的角色。D；E\n提供有关研究是否获得资助的详细信息，并提供有关资助者在研究中的作用的任何详细信息；为所有作者提供任何其他资金来源。\n实例 5《Machine learning-based prediction of composite risk of cardiovascular events in patients with stable angina pectoris combined with coronary heart disease: development and validation of a clinical prediction model for Chinese patients：基于机器学习的稳定型心绞痛合并冠心病患者心血管事件复合风险预测：中国患者临床预测模型的开发与验证》。\n\n\n18.2 利益冲突方：声明所有作者的利益冲突和财务披露。D；E\n披露读者可能认为相关或可能影响研究设计、实施、解释或报告的任何作者关系或活动。\n实例 5《Machine learning-based prediction of composite risk of cardiovascular events in patients with stable angina pectoris combined with coronary heart disease: development and validation of a clinical prediction model for Chinese patients：基于机器学习的稳定型心绞痛合并冠心病患者心血管事件复合风险预测：中国患者临床预测模型的开发与验证》。\n\n\n18.3 计划书：说明研究方案可在何处获取或声明方案未准备好。D；E\n提供有关研究方案可用性的所有详细信息，包括可以找到研究方案的位置（例如，出版物详细信息、补充材料、在存储库中公开提供，例如在开放科学框架上），包括URL或DOI；明确说明是否未制定或未公开提供研究方案（以及原因）；如果与研究方案中规定的内容有任何显着偏差， 提供偏差的摘要和原因。\n实例 12《Development and validation of a multimodal feature fusion prognostic model for lumbar degenerative disease based on machine learning: a study protocol：基于机器学习的腰椎退行性疾病多模态特征融合预后模型的开发与验证：研究方案》。\n\n\n18.4 注册：为研究提供注册信息，包括注册名和注册号，或者说明研究未进行注册。D；E\n如果研究已注册（例如，在clinicaltrials.gov上，开放科学框架），请提供有关注册号、注册名称和注册链接（包括任何DOI）的详细信息；明确说明研究是否尚未注册。\n\n\n18.5 数据共享：详细说明研究数据的可获得性。D；E\n提供有关研究数据可用性的详细信息，包括可以找到数据的位置（例如，公共存储库、URL、DOI）、如何检索数据、获取和使用数据的任何条件或限制。任何共享数据都应附带数据字典；如果无法共享数据，请提供原因；避免使用陈词滥调，例如“根据合理请求提供数据”，而不指定构成合理请求的条件。\n\n\n18.6 代码共享：提供分析代码可用性的详细信息▲。D；E\n提供有关分析代码可用性的所有详细信息（以及如何运行代码的文档），包括可以找到代码的位置（例如，代码存储库、DOI、链接）、如何检索代码、应报告获取和使用代码的任何条件或许可（和版本）；分析代码是复制（原则上）研究的所有报告结果和发现所需的所有代码（包括数据清理）。应报告（原则上）重现研究结果所需的软件和任何软件包（包括任何版本号）。在某些情况下，可能需要报告有关计算环境的更多详细信息（例如，硬件、操作系统、CPU、RAM）。\n\n\n\n19 患者和公众参与：在研究的设计、进行、报告、解释或传播过程中提供任何患者和公众参与的详细信息，或陈述没有参与的情况。D；E\n描述患者或公众如何参与研究及其结果的规划、设计、实施、报告或传播；研究结果是否向患者或公众展示？；考虑使用 GRIPP2 声明来报告患者和公众对研究的参与情况；如果没有患者或公众参与研究的任何方面，请明确说明。\n\n\n20 参与者\n\n20.1：描述研究过程中参与者的流动情况，包括有结果和没有结果的参与者人数，如果适用，则对随访时间进行总结。一张图可能会有帮助。D；E\n流程图可用于描述参与者通过研究的流程，其中流程图的入口点是参与者的来源，然后后续步骤可以与资格标准、后续行动（如果适用）相关。和数据可用性；流程图中要呈现的其他有用信息包括缺失值的参与者数量和结果事件的数量；对于延迟参考测试的预后或诊断研究，应报告随访时间的摘要（例如，中位随访和范围）。\n实例 9《Machine Learning for Predicting Risk and Prognosis of Acute Kidney Disease in Critically Ill Elderly Patients During Hospitalization: Internet-Based and Interpretable Model Study：机器学习预测老年危重症患者住院期间急性肾脏病的风险和预后：基于互联网和可解释模型的研究》。\n\n\n20.2：报告每个数据源或设置的总体和适用的特征，包括关键日期、关键预测因素(包括人口统计特征)、接受的治疗、样本量、结果事件数、随访时间和缺失数据的数量。一张表可能是有帮助的。报告关键人口群体之间的差异。D；E\n报告（可能使用表格）使用的所有数据集的摘要，包括结果的分布、预测因子（例如，平均值/中位数、标准差/四分位数范围、频率）、接受的任何治疗、样本量（和结果事件的数量、随访时间的摘要，以及每个预测因子的缺失值的数量和比例；如果相关， 报告感兴趣的关键人口统计群体之间的任何差异可能很有用。\n实例 7《Utility of Machine Learning Algorithms in Predicting Preoperative Lymph Node Metastasis in Patients With Rectal Cancer Based on Three-Dimensional Endorectal Ultrasound and Clinical and Laboratory Data：基于三维直肠腔内超声和临床及实验室数据的机器学习算法在预测直肠癌患者术前淋巴结转移中的应用》。\n\n\n20.3：对于模型评估，展示了与开发数据分布的重要预测因子(人口统计学,预测因素和结果)的比较。E\n对于评估现有模型性能的研究（包括模型开发研究中的模型），提供重要变量（例如，平均值/中位数、标准差/四分位数范围、频率）分布的比较，例如人口统计学、模型中的预测变量和结果，包括缺失值的比例。这可能最好以表格形式呈现，并考虑按结果状态报告。\n实例 6《Development and validation of a prediction rule for estimating gastric cancer risk in the Chinese high-risk population: a nationwide multicentre study：在中国高危人群中建立和验证胃癌风险预测规则：一项全国性多中心研究》。\n\n\n\n21 模型开发：指定每个分析(例如,模型开发、超参数调优、模型评估等)中参与者和结果事件的数量。D；E\n应为每次分析（即每个模型开发、每个模型评估）报告样本量（包括结果事件的数量），因为它们在预测模型研究的不同分析中通常会有所不同（例如，在数据分区、模型超参数调整之后），尤其是在存在缺失数据的情况下；如果数据包含单个报告的多个样本或记录，也报告个体数量。\n\n\n22 模型规范：提供完整预测模型(例如,公式、代码、对象、应用程序编程接口等)的详细信息，允许在新的个体中进行预测，并允许第三方评估和实施，包括访问或重用(例如,免费的,专有的)的任何限制★。D\n预测模型开发研究的“产品product”是预测模型。因此，提供有关模型的详细信息以及如何使用它来预测新个体非常重要。例如，为回归模型提供方程，对于使用模型无法“写下”为方程的方法开发的模型，提供有关代码、软件对象或API可用性的详细信息，以便其他人可以在自己的数据中评估此模型，或在日常实践中实施它；如果开发了多个模型， 然后提供有关所有模型可用性的详细信息；解释如何使用该模型允许其他人对新个体进行预测；提供任何硬件要求和软件（和软件包）的详细信息，以实现第三方测试、实施和监控；如果模型无法公开可用（例如，出于商业原因），则应明确报告，并报告访问模型以计算新个体和第三方评估的预测的任何条件。\n实例 6《Development and validation of a prediction rule for estimating gastric cancer risk in the Chinese high-risk population: a nationwide multicentre study：在中国高危人群中建立和验证胃癌风险预测规则：一项全国性多中心研究》。\n\n\n23 模型表现\n\n23.1：报告模型性能估计的置信区间，包括对任何关键子组的(例如,社会人口学)。考虑情节来辅助呈现。D；E\n应将第 12e 项中描述的所有模型性能度量的估计值与置信区间一起呈现；报告总体和任何感兴趣关键群体（例如，性别、种族）（例如，作为公平性检查的一部分）的模型性能估计值，以及置信区间；使用图表来呈现和辅助评估，例如校准图（具有平滑的校准曲线和预测值的分布）和决策曲线；报告所有评估的性能估计值（例如，在开发数据中;在评估数据中;来自内部验证过程等），包括检查的每个时间点（用于预后模型）；报告模型稳定性的任何检查，例如，在自举样本中开发的模型中的性能估计和单个预测的可变性方面；清楚地指出哪些数据已被用于呈现每个性能估计。\n实例 13《Interpretable machine learning-based clinical prediction model for predicting lymph node metastasis in patients with intrahepatic cholangiocarcinoma：可解释的基于机器学习的临床预测模型用于预测肝内胆管癌患者的淋巴结转移》。\n\n\n23.2：如果检查，报告了跨集群的模型性能的任何异质性的结果。详情见TRIPOD-Cluster。D；E\n如果模型性能的评估考虑了数据中的任何聚类（例如，来自组合来自多项研究的个体参与者数据，或按中心/医院或国家聚类的数据），则应报告结果以及置信区间（参见第23a项）；关预测模型研究的具体报告建议，这些研究考虑了模型性能的聚类和异质性， 作者应查阅TRIPOD-cluster清单。\n\n\n\n24 模型更新：报告任何模型更新的结果，包括更新后的模型和随后的性能。E\n如果预测模型在验证后进行了更新（例如，重新校准、重新拟合），则应报告更新后的预测模型的详细信息，以便进行第三方评估和实施，包括对访问或重复使用的任何限制（见第22项）；应报告更新模型的性能（参见第23a项，可能第23b项）。\n\n\n25 解释：对主要结果进行总体解释，包括目标和先前研究背景下的公平问题。D；E\n对研究结果的解释将研究结果置于其他证据的背景下。如果存在现有模型，则在这些现有研究的背景下讨论结果；对于评估现有预测模型性能的研究，如果现有研究已经评估了模型的性能，那么讨论和总结这些结果并将其置于上下文中非常重要；确保对结果的解释不会超出模型开发和评估报告的结果，以防止过度解释或“旋转”；了解模型在评估数据中的性能与该模型的任何其他评估研究中模型的性能进行比较是有用的。当结果出现分歧时，应讨论模型性能差异的可能原因。\n实例 13《Interpretable machine learning-based clinical prediction model for predicting lymph node metastasis in patients with intrahepatic cholangiocarcinoma：可解释的基于机器学习的临床预测模型用于预测肝内胆管癌患者的淋巴结转移》。\n\n\n26 局限性：讨论研究(如非代表性样本、样本量、过拟合、缺失数据等)的任何局限性及其对任何偏倚、统计不确定性和可推广性的影响。D；E\n承认局限性是任何科学论文的一个重要方面——可以指研究设计、实施或分析的任何方面。对研究局限性进行有意义的讨论，考虑与分析中使用的数据的代表性、样本量、过度拟合和缺失数据/数据质量相关的任何问题。\n实例 13《Interpretable machine learning-based clinical prediction model for predicting lymph node metastasis in patients with intrahepatic cholangiocarcinoma：可解释的基于机器学习的临床预测模型用于预测肝内胆管癌患者的淋巴结转移》。\n\n\n27 该模型在当前照护背景下的可用性\n\n27.1：描述在实现预测模型时，应该如何评估和处理质量差或不可用的输入数据(例如,预测值)。D\n作者应该评论如何在模型打算用作日常实践中护理路径的一部分时处理不可用的预测变量值。还应评估在打算使用模型时估算缺失值的任何策略（因此在方法和结果中提到）；同样，在实施时，作者应讨论（如果相关）处理质量差的输入数据（例如，图像分辨率、数据格式）。\n实例 14《Risk factors for severe respiratory syncytial virus infection during the first year of life: development and validation of a clinical prediction model：生命第1年发生严重呼吸道合胞病毒感染的危险因素：临床预测模型的建立与验证》。\n\n\n27.2：指定在处理输入数据或使用模型时是否需要用户进行交互，以及需要用户具备何种水平的专业知识。D\n提供有关预期或要求用户如何与预测模型交互以按预期使用模型的详细信息，例如处理输入数据的任何注意事项；使用模型、处理或收集输入数据是否需要或需要任何专业知识或培训，如果需要，请提供详细信息。\n\n\n27.3：讨论未来研究的任何后续步骤，具体了解模型的适用性和可推广。E\n是否需要对模型进行进一步评估，例如，在不同的人群或亚组中，或者模型是否准备好在临床试验中进行评估，或作为护理途径的一部分实施。\n实例 14《Risk factors for severe respiratory syncytial virus infection during the first year of life: development and validation of a clinical prediction model：生命第1年发生严重呼吸道合胞病毒感染的危险因素：临床预测模型的建立与验证》。"
  },
  {
    "objectID": "blog/2024/12/03/DNAnexus_log/index.html",
    "href": "blog/2024/12/03/DNAnexus_log/index.html",
    "title": "Swiss Army Knife 工具运行 R 的 log 记录",
    "section": "",
    "text": "用 DNAnexus 平台上的 Swiss Army Knife 工具运行了下 R，记录下运行的 log。\n这个日志记录了任务的执行过程，其中涉及到多个文件的下载、解压和执行。下面是对这个日志的详细解释：\n\n1. 初始化日志记录\nLogging initialized (priority)\nLogging initialized (bulk)\n以上表示日志记录系统已经初始化，准备开始记录任务的执行情况。priority 和 bulk 表示不同级别或不同类型的日志记录。\n\n\n2. 下载和解压文件\nDownloading bundled file resources.tar.gz\n&gt;&gt;&gt; Unpacking resources.tar.gz to /\ntar: Removing leading `/' from member names\n省略了一些类似的 log 输出，表明有多个文件（如 resources.tar.gz, qctool.tar.gz, plink.tar.gz 等）被依次下载并解压。tar: Removing leading '/' from member names：这是 tar 解压时的标准行为，表示会删除文件路径中的首个 /，避免解压到根目录（/）而覆盖系统文件。\n\n\n3. 设置 SSH 公钥\nSetting SSH public key\n这里系统设置了一个 SSH 公钥，可能用于与远程服务器的连接。\n\n\n4. 安装和运行环境信息\ndxpy/0.385.0 (Linux-5.15.0-1072-aws-x86_64-with-glibc2.29) Python/3.8.10\nbash running (job ID job-xxxx)\ndxpy 是用于与 DNAnexus 平台交互的 Python 库。运行环境是 Linux（版本 5.15.0-1072-aws），使用 Python 3.8.10。作业 ID 是 job-xxxx，表示当前任务在平台上的唯一标识符。\n\n\n5. 下载文件到本地文件系统\ndownloading file: file-xxxx to filesystem: /home/dnanexus/in/in/0/hello_world.r\n从 DNAnexus 平台下载了名为 hello_world.r 的文件，并将其保存到 /home/dnanexus/in/in/0/ 目录。\n\n\n6. dxfuse 版本信息和文件系统守护进程\nUsing dxfuse version v1.4.0\nThe log file is located at /root/.dxfuse/dxfuse.log\nstarting fs daemon\nwait for ready\nDaemon started successfully\n使用 dxfuse （一个用于将 DNAnexus 文件系统挂载到本地文件系统的工具）版本 1.4.0。启动了文件系统守护进程，用于与 DNAnexus 文件系统进行交互。\n\n\n7. 下载文件使用多个线程\nDownloading files using 4 threads+ [[ '' == '' ]]\n文件的下载是并行进行的，使用了 4 个线程来加速下载。\n\n\n8. 执行 R 脚本\n+ eval 'Rscript hello_world.r'\n++ Rscript hello_world.r\n[1] \"Hello, world!\"\n执行了 R 脚本 hello_world.r，并成功输出了 “Hello, world!”，表明脚本运行成功。\n\n\n9. 完成任务\n+ set +x\n这是一个 Shell 命令，表示关闭命令回显模式。+x 表示在执行时打印每条命令的详细信息，关闭后将不再显示。"
  },
  {
    "objectID": "blog/2024/12/11/table_exporter/index.html",
    "href": "blog/2024/12/11/table_exporter/index.html",
    "title": "DNAnexus 表型数据提取",
    "section": "",
    "text": "自从 UK Biobank 数据必须在 DNAnexus 上操作后，我们不得不使用 UKB 的付费平台。DNAnexus 是一个功能强大的云平台，提供了多种工具来处理、分析和可视化大规模基因组数据。在研究中，表型数据（如疾病状态、实验测量值、临床记录等）是理解基因与环境因素相互作用的重要资源。DNAnexus 提供了 Table Exporter 工具以及 dx extract_dataset，帮助研究人员从数据库中高效提取表型数据，并将其导出为可分析的格式。\n在这篇文章中，我们将介绍如何在 DNAnexus 上使用 Table Exporter 提取表型数据。\n\nTable Exporter 简介\n\n\n\nTable Exporter\n\n\nTable Exporter 是 DNAnexus 提供的一项功能，允许用户从项目中提取存储的表型数据，并根据需要导出为 CSV、TSV 或 SQL 格式的文件。这些文件可以包含参与者的各种表型信息，如年龄、性别、疾病状态、基因型等。该工具的灵活性使得它不仅适用于小规模数据提取，还能够处理大规模的数据集，支持定制化字段的选择，以及针对特定需求的格式化输出。\n\n\n如何使用 Table Exporter 提取数据\n\n步骤 1：登录 DNAnexus 平台\n首先，确保你已经拥有 DNAnexus 帐号并登录。若没有帐户，可以在 DNAnexus 官方网站申请注册。登录后，你将进入 Project Dashboard，即你的项目仪表板。\n\n\n步骤 2：选择项目\n在仪表板中，选择包含你需要提取表型数据的项目。你可以选择已有的项目，或者新建一个项目。\n\n\n步骤 3：配置 Table Exporter\n在项目中找到 Table Exporter 工具，并点击进入配置界面。\n\n\n\nTable Exporter 配置\n\n\n在此界面中，你需要选择要导出的表型数据字段。这些字段通常是与研究目标相关的变量，例如： 年龄、性别、疾病状态、身高、体重等基本信息。Table Exporter 支持将数据导出为不同格式：CSV：适用于大多数数据分析工具（如 Excel、R、Python 等），能够以表格形式查看。TSV：适合更大数据量的处理，且与数据库中的表格数据兼容性更高。SQL：如果你希望导出为 SQL 查询格式，用于后续数据库查询，可以选择此格式。选择适合你需求的导出格式后，点击 Next 进入下一步。在导出时，Table Exporter 还提供了一些额外的输出选项：字段名称（Field Names）：你可以选择导出数据时是否包括字段名称作为表头。数据过滤（Data Filtering）：根据需要，可以设置过滤条件，仅导出符合条件的数据。\n\n\n\nadvanced option\n\n\n配置完成后，点击 Start Export 按钮，DNAnexus 将开始生成导出的数据文件。根据数据量的不同，可能需要一些时间来处理。你可以在 Job History（作业历史）中查看任务进度。\n\n\n\njob history"
  },
  {
    "objectID": "blog/2024/12/19/vcf/index.html",
    "href": "blog/2024/12/19/vcf/index.html",
    "title": "深入了解 VCF/VCF.GZ/VCF.GZ.TBI 文件",
    "section": "",
    "text": "在生物信息学研究中，变异数据的存储和分析是核心任务之一。而 VCF 文件（Variant Call Format）作为广泛使用的文件格式，几乎贯穿了从测序数据到变异注释和分析的整个流程。这里，我们将深入探讨 VCF 及其相关的压缩文件（VCF.GZ）和索引文件（VCF.GZ.TBI），帮助大家更好地理解这些文件及其应用场景。\n\n什么是 VCF 文件？\nVCF（Variant Call Format）文件是一种专为存储基因组变异数据设计的文件格式，它可以记录不同个体或样本中的突变信息，包括单核苷酸变异（SNP）、插入和缺失（InDels）以及结构变异（SVs）。\nVCF文件具有以下主要特点： - 文本格式：易于读取和解析，便于集成到各种分析工具中。 - 可扩展性：支持通过 INFO 字段存储额外的变异注释。 - 标准化：通过一致的格式定义，保证跨研究和工具的兼容性。\n\n\nVCF 文件的结构\n一个典型的 VCF 文件由两部分组成：\n\nHeader（头部）\n以#开头，包含文件的元信息，如 VCF 版本、参考基因组、字段定义等。例如：\n#fileformat=VCFv4.2\n##reference=GRCh38\n##INFO=&lt;ID=AF,Number=A,Type=Float,Description=\"Allele Frequency\"&gt;\n#CHROM POS ID REF ALT QUAL FILTER INFO FORMAT SAMPLE1 SAMPLE2\n#CHROM：染色体编号；POS：变异在基因组上的位置；ID：变异的标识符（如 rsID）；REF 和 ALT：参考等位基因和替代等位基因。\n\n\nData（数据部分）\n存储实际的变异信息，每行记录一个变异。例如：\n1  123456  rs123456  A  G  50  PASS  AF=0.1  GT:AD:DP  0/1:5,10:15  0/0:20,0:20\n这里记录了第 1 号染色体上第 123456 位的一个 SNP，参考碱基是 A，突变碱基是 G。\n\n\n\nVCF.GZ 文件：为什么要压缩？\n由于全基因组测序数据的规模庞大，原始的 VCF 文件往往非常大，这给存储和传输带来了不小的负担。因此，生物信息学领域通常会对 VCF 文件进行压缩，生成 VCF.GZ 文件。\nVCF.GZ的特点：\n\n基于 Gzip 压缩：VCF.GZ 文件采用 Gzip 算法压缩，具有良好的压缩率。\n兼容性强：绝大多数生物信息学工具（如 bcftools、GATK、vcftools 等）都支持直接读取 VCF.GZ 文件。\n便于存储和共享：相比未压缩的 VCF 文件，VCF.GZ 文件体积更小，减少了存储成本，并加快了传输速度。\n\n要生成VCF.GZ文件，可以使用以下命令：\nbgzip -c input.vcf &gt; input.vcf.gz\n\n\nVCF.GZ.TBI 文件：加速随机访问的关键\n对于全基因组数据，快速定位某个染色体区域的变异是一项基本需求。索引文件（VCF.GZ.TBI）正是为了解决这一问题而设计的。\nVCF.GZ.TBI的作用：\n\n快速检索：VCF.GZ.TBI 文件为 VCF.GZ 文件生成了索引，使得工具可以快速定位到特定染色体区域的变异记录。\n支持分区分析：在多样本和多染色体的情况下，索引文件显著提升了并行处理的效率。\n\n生成索引文件可以使用以下命令：\ntabix -p vcf input.vcf.gz\n完成后，会生成一个名为 input.vcf.gz.tbi 的索引文件。\n\n\nVCF/VCF.GZ/VCF.GZ.TBI 在实际中的应用\n\n存储与共享变异数据\n未压缩的 VCF 文件便于直接查看，但更适用于小规模数据集。VCF.GZ 文件更适合大规模数据的存储和共享。\n\n\n加速基因组区域检索\n索引文件（VCF.GZ.TBI）在大规模队列分析中尤为重要。如使用 bcftools view 仅提取某条染色体上的特定区域：\nbcftools view -r chr1:100000-200000 input.vcf.gz\n\n\n兼容主流生信工具\n几乎所有主流的变异分析工具都支持 VCF 格式及其压缩和索引版本。例如：\n\nGATK：进行变异过滤和注释。\nvcftools：计算等位基因频率、进行样本过滤。\nbcftools：快速子集提取和格式转换。\n\n\n\n云环境中的应用\n通过压缩后的 VCF.GZ 文件和索引，配合分布式计算框架（如 Spark SQL），可以在云端快速处理大规模变异数据。"
  },
  {
    "objectID": "blog/2024/12/21/plink/index.html",
    "href": "blog/2024/12/21/plink/index.html",
    "title": "理解 PLINK格式",
    "section": "",
    "text": "在基因组学研究中，PLINK 是一个广泛使用的工具，用于关联分析、基因型数据处理以及数据格式转换。PLINK 以多种文件格式存储基因型数据，每种格式适用于不同的应用场景。这里，本文将介绍 PLINK 的几种主要数据格式及其特点，帮助大家更好地理解和使用这些文件。\n\n\n\nPLINK\n\n\n\nPLINK 数据的基本组成\n在 PLINK 中，基因型数据的存储由三部分信息组成：\n\n个体信息：每个研究对象的基本信息，如样本 ID、性别等。\n变异信息：每个 SNP 的基本信息，如染色体位置和碱基变化。\n基因型矩阵：每个样本在每个位点上的基因型数据。\n\n为了存储和处理这些数据，PLINK 提供了多种格式集合，包括 ped/map、fam/bim/bed 和 psam/pvar/pgen。\n\n\n数据格式及其特点\n\nped/map 格式\n这是 PLINK 的原始标准文本格式，用于存储完整的基因型数据表。\n\n.ped 文件\n.ped 文件包含每个样本的详细基因型数据，无标题行，每行代表一个样本。每行的前六列是样本的基本信息，后续每两个字段表示一个 SNP 的等位基因。\n\n0 HG00403 0 0 0 -9 G G T T A A G A C C\n0 HG00404 0 0 0 -9 G G T T A A G A T C\n\n字段说明：第 1-6 列：样本信息（家庭 ID、样本 ID、父母 ID、性别、表型等）。第 7 列及之后：每个 SNP 的两个等位基因（如 G G 表示基因型为 GG）。\n\n\n.map 文件\n.map 文件存储每个变异位点的信息，无标题行，每行代表一个 SNP。\n\n1       1:13273:G:C     0       13273\n1       1:14599:T:A     0       14599\n\n字段说明：第 1 列：染色体编号。第 2 列：变异 ID（格式为染色体:位置:参考等位基因:替代等位基因）。第 3 列：遗传距离（默认为 0）。第 4 列：物理位置（以碱基为单位）。\n优点：直观易读，便于检查数据。缺点：文件体积大，尤其是对于大规模数据。\n\n\n\nfam/bim/bed 格式\n这是 PLINK 的二进制格式，包含与 ped/map 相同的信息，但文件更小，适合大数据处理。\n\n.fam 文件\n样本基本信息，内容与 .ped 文件的前六列一致。\n\n0 HG00403 0 0 0 -9\n0 HG00404 0 0 0 -9\n\n\n\n.bim 文件\nSNP 基本信息，与 .map 文件内容相似，但增加了等位基因列。\n\n1       1:13273:G:C     0       13273   C       G\n1       1:14599:T:A     0       14599   A       T\n\n\n\n.bed 文件\n二进制格式的基因型矩阵，每个位点的数据存储为二进制编码。\n\n00 6c 1b 01 ff ff bf bf ff ff ff ef fb ff ff ff fe\n\n优点：存储效率高，适合大规模基因型数据的分析。缺点：文件内容不可直接阅读，需要通过软件解析。\n\n\n\npsam/pvar/pgen 格式\n这是 PLINK2 引入的新格式，提供更灵活的数据管理和高效的处理能力。\n.psam 文件：存储个体信息。.pvar 文件：存储变异信息，与 .bim 文件相似。.pgen 文件：存储基因型矩阵的二进制文件。\n\n\n\n格式的适用场景\n\n\n\n\n\n\n\n\n格式集合\n特点\n适用场景\n\n\n\n\nped/map\n文本格式，直观易读，但占用空间大\n数据检查、数据格式初学者\n\n\nfam/bim/bed\n二进制格式，存储高效，需借助软件解析\n大规模数据分析，如GWAS\n\n\npsam/pvar/pgen\nPLINK2格式，兼容性强，处理速度快\n需要复杂分析和高性能计算时"
  },
  {
    "objectID": "blog/2024/12/24/genotype_data_QC/index.html",
    "href": "blog/2024/12/24/genotype_data_QC/index.html",
    "title": "预 GWAS 阶段的基因型数据 QC 流程",
    "section": "",
    "text": "在进行全基因组关联研究（GWAS）前，数据质量的控制（QC）是至关重要的一步。\n预处理和质量控制可以确保我们使用的数据集干净、可靠，避免潜在的偏倚和错误。这里，我们将介绍如何在 GWAS 前进行基因型数据的 QC，确保数据的准确性和可靠性。\n\n1. 计算缺失率（Missing Rate）与呼叫率（Call Rate）\n在 GWAS 分析中，缺失数据可能会影响结果的准确性。基因型数据的质量首先需要评估每个样本和每个 SNP 的缺失率。\n缺失率表示某个样本在所有标记位点的基因型缺失的比例；呼叫率表示某个 SNP 在所有样本中的基因型被成功识别的比例。样本缺失率：样本的基因型数据可能因技术问题、平台差异等因素缺失，需要计算每个样本的缺失率，并根据预设的阈值剔除缺失过多的样本。SNP 呼叫率：类似地，每个SNP的呼叫率需要计算。如果某些 SNP 在大多数样本中都没有成功呼叫，可以考虑排除这些 SNP。\n在 PLINK 中，可以使用以下命令来计算样本的缺失率和 SNP 的呼叫率：\nplink --bfile ${genotypeFile} --missing --out plink_missing\n\n\n2. 计算等位基因频率（Allele Frequency）\n等位基因频率是 GWAS 中重要的统计量，它表示某个特定等位基因在样本中的出现频率。为了确保我们分析的是常见的变异，我们通常会设置最小等位基因频率（MAF）阈值。通常，筛选出 MAF 低于 1% 的 SNP 是合理的，因为低频变异可能会增加假阳性。\n在 PLINK 中，计算等位基因频率非常简单：\nplink --bfile ${genotypeFile} --freq --out plink_freq\n\n\n3. Hardy-Weinberg 平衡精确检验（Hardy-Weinberg Equilibrium Test）\nHardy-Weinberg 平衡（HWE）检验是 GWAS 中常用的一种质量控制手段。根据 HWE 定律，如果一个群体处于均衡状态，则基因型的分布应该符合预期的频率。如果某个 SNP 的基因型偏离了 HWE，可能是因为样本污染、系统性错误或者是与某些表型相关的变异。通常，对于病例对照研究，HWE检验的p值阈值设定为\\(10^{-6}\\)。\n使用 PLINK 执行 HWE 检验：\nplink --bfile ${genotypeFile} --hwe 1e-6 --out plink_hwe\n\n\n4. LD-Pruning\n连锁不平衡（LD）修剪是为了去除那些高度相关的 SNP，减少冗余，确保分析中使用的 SNP 具有独立性。若 SNP 之间存在高度的 LD，它们可能会影响 GWAS 结果的准确性。通常使用 LD-pruning 来去除相关性高的 SNP。\n在 PLINK 中，常用的 LD-pruning 命令如下：\nplink --bfile ${genotypeFile} --indep-pairwise 50 5 0.2 --out plink_results\n该命令会根据给定的窗口大小（50 个 SNP）、步长（每次移动 5 个 SNP）和 \\(r^{2}\\) 阈值（0.2），进行 LD 修剪。\n\n\n5. 计算近交系数 F（Inbreeding F coefficient）\n近交系数 F（F coefficient）用于衡量样本中的近交程度。较高的 F 值可能表示样本存在近交，而较低的 F 值可能表示样本污染。计算 F 值时，可以使用 PLINK 的 --het 命令，它会生成每个样本的观测和期望纯合子基因型数量，并计算 F 值。\nplink --bfile ${genotypeFile} --het --out plink_results\n输出结果中，F 值较高的样本可以考虑进一步检查或排除。常见的处理方法是将 F 值超过 3 个标准差（SD）的样本排除。\n\n\n6. 数据管理（make-bed / recode）\n为了便于后续分析，PLINK 提供了多种数据格式转换工具。最常用的格式是 BED 格式，它是 PLINK 的二进制格式，能够大大提高存储和计算效率。make-bed 将原始的 PED/MAP 文件转换为 BED 格式。通常在预处理数据时使用该命令。\nplink --bfile ${genotypeFile} --make-bed --out plink_bed\nbash\nrecode 如果你需要将数据转回 PED 格式或其他格式，可以使用 recode 命令。\nplink --bfile ${genotypeFile} --recode --out plink_recode\n这些命令帮助将数据从一种格式转换为另一种格式，确保数据可以被其他分析工具有效读取。\n\n\n总结\n基因型数据的 QC 是 GWAS 分析中的基础步骤，它确保数据的可靠性和准确性。通过执行上述步骤，你可以在 GWAS 分析前排除掉缺失值过多、低频变异、偏离 Hardy-Weinberg 平衡的 SNP，以及冗余的高度 LD SNP。高质量的数据是可靠分析的前提，只有确保数据质量，才能得出科学、可靠的结果。"
  },
  {
    "objectID": "blog/2024/12/26/smartpca/index.html",
    "href": "blog/2024/12/26/smartpca/index.html",
    "title": "使用 Eigensoft 中的 smartPCA 进行 PCA 分析",
    "section": "",
    "text": "昨天我们介绍了如何使用 plink 进行 pca 分析，这里，我们将介绍如何使用 Eigensoft 包中的工具 smartPCA 进行 PCA 分析。\n\nsmartPCA\nsmartPCA 是 Eigensoft 包中的一个工具，专门用于基因型数据的 PCA 分析。它可以处理大规模的遗传数据集，输出主成分和相应的特征值，帮助我们识别数据中最重要的变异模式。\n\n1. 准备数据\n首先，需要准备三个输入文件，这些文件包含了基因型数据的核心信息： - SNP 文件（.snp）：包含 SNP 位置信息，如 SNP 名称、染色体、遗传位置、参考等位基因和替代等位基因。 - 个体文件（.ind）：包含个体信息，如个体名称、性别、种群信息等。 - 基因型文件（.geno）：包含实际的基因型数据，每个个体对应一个SNP位点的基因型数据。\n这些文件通常采用 EIGENSTRAT 格式，智能地将数据组织成易于处理和分析的形式。\n\n\n2. 编写参数文件\n为了运行 smartPCA，需要准备一个参数文件（.par），该文件指定了输入文件和输出文件的路径、PCA 分析的相关设置。一个典型的参数文件示例如下：\ngenotypename: &lt;GENOTYPE_DATA&gt;.geno\nsnpname: &lt;GENOTYPE_DATA&gt;.snp\nindivname: &lt;GENOTYPE_DATA&gt;.ind\nevecoutname: &lt;OUT_FILE&gt;.evec\nevaloutname: &lt;OUT_FILE&gt;.eval\npoplistname: &lt;POPULATION_LIST_FILE&gt;.txt\nlsqproject: YES\nnumoutevec: 4\nnumthreads: 1\n\ngenotypename: 基因型数据文件路径。\nsnpname: SNP位点数据文件路径。\nindivname: 个体数据文件路径。\nevecoutname: 输出的主成分文件路径（.evec）。\nevaloutname: 输出的特征值文件路径（.eval）。\npoplistname: 种群列表文件路径，指定用于计算主成分的种群。\nlsqproject: 是否对缺失数据进行投影。\nnumoutevec: 计算的主成分数量。\nnumthreads: 使用的线程数量。\n\n\n\n3. 运行 PCA 分析\n创建好参数文件后，可以通过命令行运行 smartPCA 进行分析。假设参数文件名为 params.par，可以使用以下命令启动 PCA 分析：\nsmartpca -p params.par\n运行时间通常在 15 到 30 分钟之间，具体时间取决于数据集的大小。\n\n\n4. 查看输出结果\nsmartPCA 会生成两个主要的输出文件： - .evec 文件：包含每个个体在各个主成分上的坐标。可以使用这些坐标绘制主成分图（例如，二维或三维散点图），以可视化个体或种群之间的关系。 - .eval 文件：包含每个主成分的特征值。这些特征值反映了每个主成分对数据方差的贡献。\n例如，特征值较大的主成分通常解释了数据中更大的变异，因此我们可以根据特征值的大小来判断是否保留某个主成分。\n\n\n5. 可视化 PCA 结果\nPCA 的最终目的是将数据降到二维或三维空间，便于我们观察数据的分布和结构。常见的做法是将第一主成分和第二主成分的坐标作为横纵坐标，绘制二维散点图。\n例如，使用 R 或 Python 中的可视化工具，可以将 .evec 文件中的数据提取出来，并绘制主成分图，来分析不同群体、种群或个体在遗传空间中的分布。\n\n\n6. 投影其他个体\n如果我们使用的是包含现代和古代个体的混合数据集，可以使用 smartPCA 的投影功能。这个功能允许我们将不在种群列表中的个体投影到已计算的主成分上。这样，可以将古代 DNA 样本投影到现代群体的 PCA 结果中，进一步分析古代遗传变异在现代遗传空间中的位置。\n\n\n\n总结\n通过使用 Eigensoft 中的 smartPCA 工具，我们可以方便地对大规模的基因型数据进行 PCA 分析，识别数据中的遗传结构，了解群体之间的遗传关系。通过合理选择主成分，结合特征值和可视化技术，PCA 能够帮助我们从高维遗传数据中提取出有价值的信息，进行种群间比较、演化历史分析等。"
  },
  {
    "objectID": "blog/2025/01/01/export_plink/index.html",
    "href": "blog/2025/01/01/export_plink/index.html",
    "title": "使用 Hail 输出 PLINK 文件：一步到位",
    "section": "",
    "text": "在现代基因组学研究中，数据的处理和转换是至关重要的一环。PLINK 文件（包括 .bed, .bim, .fam 三个文件）是遗传学研究中的标准文件格式之一，经常用于 GWAS（全基因组关联研究）等分析。对于研究人员来说，将原始数据转换为 PLINK 文件格式是分析过程中的关键步骤。而 Hail，作为一个处理大规模基因数据的高效工具，提供了方便的方法来导出这些文件。\n前面，我们介绍了如何掌握 Hail，这里，我们将介绍如何使用 Hail 的 export_plink 功能，轻松导出 PLINK 文件，并深入探讨如何在数据预处理和分析过程中使用它。\n\n使用 Hail 导出 PLINK 文件\nHail 提供了一个非常便利的函数 export_plink()，能够将 Hail MatrixTable 格式的数据导出为 PLINK 所需的三种文件格式：.bed、.bim 和 .fam。这些文件是 PLINK 软件包用于处理和分析基因型数据的标准文件格式。\n\n1. 载入数据\n首先，我们需要加载基因型数据，通常这些数据存储在 VCF 格式的文件中。使用 Hail 的 import_vcf() 函数，可以轻松导入 VCF 文件。\n\nimport hail as hl\n\nmt = hl.import_vcf('file://path_to_vcf_data.vcf', force_bgz=True, reference_genome='GRCh38')\n\n\n\n2. 数据注释\n为了确保导出的 PLINK 文件包含完整的信息，我们通常需要对数据进行注释。例如，标注样本的性别、家族信息等。这可以通过 annotate_cols() 函数完成。\n\nmt = mt.annotate_cols(\n    pat_id='0',         # 父代 ID\n    mat_id='0',         # 母代 ID\n    is_female=True,     # 是否为女性\n    pheno=-9            # 表型（-9 表示缺失值）\n)\n\n在这个例子中，我们为每个样本添加了父母 ID、性别和表型信息。\n\n\n3. 导出为 PLINK 文件\n现在，数据已经准备好，可以使用 export_plink() 函数将其导出为 PLINK 所需的 .bed, .bim, .fam 文件。\n\nhl.export_plink(\n    mt, 'output/example', \n    fam_id=mt.s,       # 样本的 ID\n    ind_id=mt.s,       # 样本的 ID\n    pat_id=mt.pat_id,  # 父代 ID\n    mat_id=mt.mat_id,  # 母代 ID\n    is_female=mt.is_female, # 性别信息\n    pheno=mt.pheno     # 表型信息\n)\n\n参数解析： - fam_id: 样本的家族 ID，通常使用样本的 ID（即 mt.s）。 - ind_id: 样本的个体 ID，这里使用 mt.s 作为样本 ID。 - pat_id 和 mat_id: 分别代表父母的 ID，通常可以使用默认值 0 表示没有父母信息，或者根据数据 - is_female: 样本的性别信息，Hail 会根据性别表达为 1（男性）或 2（女性）。这里通过注释 is_female 字段来指示性别。 - pheno: 表型数据，可以是布尔值（如是否患病）或者是数值型数据（如身高、体重等）。\n导出的文件会包含以下三部分：\n\noutput/example.bed: 存储基因型数据（二进制格式）。\noutput/example.bim: 包含变异信息，如染色体、位置、参考和变异等。\noutput/example.fam: 包含样本信息，如家族 ID、个体 ID、父母 ID、性别和表型。\n\n\n\n\n进阶功能：批量导出与错误处理\n当处理大量基因数据时，可能会涉及多个数据批次。在这种情况下，使用批处理来导出 PLINK 文件变得尤为重要。例如，我们可以使用 Python 脚本批量处理多个数据集，并通过适当的异常处理机制确保过程顺利进行。\n\nimport subprocess\n\n# 批量导出并上传\nbatch_files = ['batch1.vcf', 'batch2.vcf', 'batch3.vcf']  # 示例批次\nfor batch_file in batch_files:\n    # 导入 VCF 文件并处理\n    mt = hl.import_vcf(f'file://{batch_file}', force_bgz=True, reference_genome='GRCh38')\n    mt = mt.annotate_cols(\n        pat_id='0',\n        mat_id='0',\n        is_female=True,\n        pheno=-9\n    )\n    \n    # 导出 PLINK 文件\n    hl.export_plink(mt, f'output/{batch_file.split(\".\")[0]}', fam_id=mt.s, ind_id=mt.s, pat_id=mt.pat_id, mat_id=mt.mat_id, is_female=mt.is_female, pheno=mt.pheno)\n\n    # 上传到远程存储\n    subprocess.run(['dx', 'upload', f'output/{batch_file.split(\".\")[0]}.bed'])\n    subprocess.run(['dx', 'upload', f'output/{batch_file.split(\".\")[0]}.bim'])\n    subprocess.run(['dx', 'upload', f'output/{batch_file.split(\".\")[0]}.fam'])"
  },
  {
    "objectID": "blog/2025/01/06/terminal_commands/index.html",
    "href": "blog/2025/01/06/terminal_commands/index.html",
    "title": "10 个常用终端命令",
    "section": "",
    "text": "终端是开发者与操作系统之间的重要桥梁，通过它，我们可以快速进行各种操作。无论是查找文件、修改权限，还是处理文本文件，熟练掌握一些基础的终端命令可以显著提高工作效率。\n这里，我们来介绍 10 个每个开发者都应该知道的终端命令，它们能帮助我们在日常开发工作中更加高效地使用终端。\n\n1. grep - 查找内容\ngrep 是一个强大的搜索命令，用于在文件中查找指定的文本内容。它常用于查找特定的关键词，支持正则表达式，功能非常强大。\n常见用法：\ngrep \"let's find something\" file.[txt,json,js,md,etc]\n在 file.txt 文件中查找匹配 pattern 的行。\n# case-insensitive search\ngrep -i \"pattern\" file.txt\n# count occurrences\ngrep -c \"pattern\" file.txt\n# search for multiple patterns\ngrep -e \"pattern1\" -e \"pattern2\" file.txt\n# recursive search in directories\ngrep -o -r \"pattern\" /path/to/directory | wc -l\n\n\n2. man - 查看命令帮助\nman 是 manual 的缩写，用于查看命令的帮助文档。当我们对某个命令不熟悉时，可以通过 man 命令查看其用法、选项和参数等详细信息。\nman grep\n查看 grep 命令的帮助文档。\n\n\n3. cat - 查看文件内容\ncat 是一个用于查看文件内容的命令，它可以一次性显示整个文件的内容。如果文件较大，可以结合其他命令来分页显示内容。\ncat file.txt\n显示 file.txt 文件的所有内容。\n# combine multiple files\ncat file1.txt file2.txt &gt; combined.txt\n# create a new file\ncat &gt; newfile.txt\n\n\n4. head - 查看文件开头\nhead 命令用于显示文件的前几行内容。默认情况下，它会显示文件的前10行，但可以通过参数指定显示行数。\nhead -n 20 file.txt\n显示文件 file.txt 的前20行。\n\n\n5. awk - 文本处理工具\n我们在这篇文章探索 AWK已经有过介绍。\n\n\n6. sed - 流编辑器\nsed 是一个流编辑器，用于处理和修改文件内容。它通过匹配模式和替换操作，可以高效地修改文本文件。\n# replace a word or pattern in a file\nsed -i '' 's/old/new/g' file.md\n-i 选项表示就地编辑（in-place editing），即直接修改文件内容。如果不使用 -i，sed 会将结果输出到标准输出，而不会修改原文件。-i 选项后面通常需要一个备份文件的扩展名。如果提供一个空字符串 ’’，表示不创建备份文件。如果不提供这个参数，sed 会默认创建一个备份文件。s 表示替换操作，g 表示全局替换（global），即替换每一行中所有匹配的 old，而不仅仅是第一个匹配项。\n# print specific lines\nsed -n '10,20p' file.json\n-n 选项表示静默模式（silent mode），只有通过 p 命令显式打印的行才会输出。10,20p 表示打印第 10 到第 20 行。\n# regular expression\nsed 's/[0-9]*/X/g' file.csv\n将文件中的数字替换为 X。\n# rename files in bulk\nfor file in *.txt; do \n  mv \"$file\" \"$(echo \"$file\" | sed 's/.txt$/.md/')\"\ndone\necho “$file” 是为了将文件名传递给 sed 命令，然后将 .txt 后缀替换为 .md。\n\n\n7. tail - 查看文件末尾\ntail 命令用于查看文件的最后几行内容，特别适合查看日志文件的最新信息。与 head 命令相反，tail 显示的是文件的结尾部分。\ntail -f logfile.log\n实时查看日志，-f 选项表示跟踪（follow），即实时显示文件的更新内容。\n\n\n8. chmod - 修改文件权限\n每个文件和目录都有权限属性，用于控制用户对文件的访问权限。权限属性包括读、写、执行权限，分别对应 r、w、x。用户组包括所有者、所属组和其他用户，分别对应 owner、group、others。chmod 命令用于修改文件或目录的访问权限，可以通过符号模式或数字模式来设置权限。\n符号模式包括 u（所有者）、g（所属组）、o（其他用户）、a（所有用户），加号 + 表示添加权限，减号 - 表示删除权限，等号 = 表示设置权限。\nchmod +x script.sh\n给 script.sh 脚本添加执行权限。\n数字模式包括 0-7，分别对应 rwx 权限。r=4，w=2，x=1。例如，755 表示所有者可读写执行，所属组和其他用户可读执行；777 表示所有用户可读写执行。\nchmod 755 file.txt\n递归修改目录权限：\nchmod -R 755 /path/to/directory\n\n\n9. xargs - 将输出作为参数传递\nxargs 命令常与其他命令结合使用，它可以将标准输入转换为命令行参数，并将其传递给后续的命令。\necho \"file1 file2 file3\" | xargs rm\n将 file1 file2 file3 传递给 rm 命令，删除这三个文件。\n结合 find 使用：\nfind . -name \"*.txt\" | xargs rm\n压缩文件：\nls *.log | xargs tar -czvf logs.tar.gz\n\n\n10. find - 查找文件\nfind 是一个非常强大的文件查找命令，它可以根据不同的条件查找文件。可以按文件名、大小、修改时间等进行搜索。\n常见用法：\nfind /path/to/search -name \"astro\"\n在指定路径下查找所有 astro 文件。\n# clean up log files\nfind /var/log -type f -name \"*.log\" -mtime +7 -delete\n-type f 表示只查找文件，-mtime +7 表示修改时间在 7 天前的文件，-delete 表示删除这些文件。\n# backup files\nfind /path/to/files -name \"*.txt\" -exec cp {} /path/to/backup \\;\n-exec 选项用于执行其他命令，{} 表示查找到的文件，; 表示命令结束。\n\n\ntakeaway\n掌握这些常用的终端命令能帮助我们在开发和运维工作中事半功倍。每个命令都有其特定的功能，熟练运用它们，我们将能够更高效地处理文件、查找信息、修改权限，甚至进行复杂的文本处理。"
  },
  {
    "objectID": "blog/2025/01/19/renv_in_r/index.html",
    "href": "blog/2025/01/19/renv_in_r/index.html",
    "title": "R 中项目环境管理",
    "section": "",
    "text": "在 R 语言的项目开发和数据分析过程中，管理包的依赖关系一直是一个挑战。每个项目可能依赖不同版本的R包，而不同项目间的包依赖往往互相冲突。为了避免这些问题，renv 包应运而生，其相当于 python 中的 virtualenv，帮助 R 用户高效地管理和隔离项目的依赖环境。\n这里，我们将介绍 renv 包的基本功能及使用方法，帮助大家更好地管理 R 项目的包依赖。\n\n\n\nrenv\n\n\n\n什么是 renv 包\nrenv（R Environment）是一个用于管理 R 项目环境的工具。它允许为每个 R 项目创建一个独立的虚拟环境，从而确保每个项目拥有独立且隔离的 R 包依赖。通过使用 renv，我们可以避免包版本冲突，并且可以轻松地复制和共享项目环境，确保项目在不同的机器或不同的时间点上能够重现。\n\n\nrenv 的核心优势\n\n隔离环境：每个项目有独立的库，包的版本不会相互干扰。即使我们在一个项目中更新了某个包的版本，其他项目依然可以使用旧版本。\n项目可重现性：通过保存和共享项目的依赖信息（如包的版本），其他开发者可以使用相同的环境重新创建项目，确保研究的可重现性。\n便捷的依赖管理：renv 会自动创建和维护 renv.lock 文件，记录项目依赖的包及其版本。这个文件是重现项目环境的关键，确保在不同机器或时间点运行时，依赖的版本完全一致。\n\n\n\n如何在 R 中使用 renv 包\n\n1. 初始化项目\n首先，我们需要安装并加载 renv 包。在一个新的 R 项目中，我们可以使用 renv::init() 来初始化 renv 环境。这将会创建一个新的 renv 文件夹并生成 renv.lock 文件。初始化后，项目中会有一个专门的库（renv/library）存放该项目所需的所有 R 包，同时生成 renv.lock 文件，这个文件记录了所有依赖包的具体版本。\n\n\n2. 安装依赖包\n当我们开始开发项目时，可以通过 install.packages() 安装所需的 R 包，renv 会将这些包安装到项目的独立环境中。\n例如，安装 ggplot2` 包后，renv 会自动更新 renv.lock 文件，记录包的安装信息及其版本。\n\n\n3. 恢复项目环境\n如果我们或者其他人需要在另一个环境中重新运行该项目，renv 允许我们通过 renv::restore() 命令恢复项目的所有依赖包。这个命令会根据 renv.lock 文件自动安装所有所需的包及其指定版本，从而确保每次运行时的包版本一致，避免了环境问题。\n\n\n4. 查看并更新项目依赖\n如果我们需要查看项目当前依赖的所有包，可以使用 renv::status() 命令，它会列出所有包及其状态。而当我们需要更新项目中的某个包时，可以使用 renv::update() 命令，它会根据当前最新的包版本进行更新，并更新 renv.lock 文件。\n\n\n5. 分享项目\n当我们完成项目后，可以将 renv.lock 文件和源代码一起通过 github 分享给其他人，其他人只需要使用 renv::restore() 来恢复项目环境，无需担心包依赖的问题。\n\n\n\nrenv 与 packrat 的对比\n在 renv 之前，R 中有一个较为类似的包叫做 packrat，它也提供环境管理的功能。然而，renv 相比于 packrat 有几个优势：\n\n更简洁：renv 比 packrat 更轻量，易于使用和理解。\n更强的兼容性：renv 与 R 版本的兼容性更好，适应性更强，能够更好地处理依赖关系。\n性能更高：renv 在项目初始化、恢复和更新时速度更快。\n\n因此，renv 在目前 R 项目环境管理中得到了更广泛的使用。"
  },
  {
    "objectID": "blog/2025/01/21/intelligent_reply/index.html",
    "href": "blog/2025/01/21/intelligent_reply/index.html",
    "title": "公众号智能回复功能终于上线",
    "section": "",
    "text": "公众号智能回复\n\n\n腾讯终于在公众号后台上线了智能回复功能。\n公众号以往发表的文章成为 AI 学习的知识库，用于自动回复用户在后台提出的问题，这也是鹅厂的腾讯元器做的事情。不过，我自己试了一下，鹅厂大模型效果一般，有时候回答的问题不是很准确，还是需要人工干预。\n\n\n\ntest\n\n\n欢迎大家后台体验。"
  },
  {
    "objectID": "blog/2025/02/11/dosage/index.html",
    "href": "blog/2025/02/11/dosage/index.html",
    "title": "一文读懂 Dosage 文件",
    "section": "",
    "text": "在基因组关联分析（GWAS）中，基因型数据的准确性和高效处理是核心挑战，准确的基因型数据是发现遗传变异与性状关联的关键。传统方法使用硬判型（Hard Call）数据（如 0/1/2 编码），但随着测序技术的发展，Dosage 文件因其对基因型不确定性的量化能力，逐渐成为 GWAS 分析的新标准。而 SAIGE 作为混合模型 GWAS 的明星工具，对 Dosage 文件的支持更是备受关注。\n\nDosage文件：基因型数据的概率化革命\n\n1.1 什么是 Dosage 文件？\nDosage 文件记录的是每个样本在某个位点的剂量（Dosage），即基因型为杂合（如 0/1）或风险等位基因（如 1/1）的期望值（Expected Value）。其取值范围通常为 0.0~2.0，表示从无风险等位基因到两个风险等位基因的概率分布。\n数学定义：\n\\[\nDosage = P(0/0) \\times 0 + P(0/1) \\times 1 + P(1/1) \\times 2\n\\]\n其中 \\(P\\) 表示基因型的概率。基于这些概率，可以计算出 dosage 值，也就是参考等位基因的期望拷贝数。\n\n\n1.2 Dosage vs Hard Call：为什么更科学？\n\n\n\n对比维度\nHard Call\nDosage\n\n\n\n\n数据本质\n离散（0/1/2）\n连续（0.0~2.0）\n\n\n低深度测序处理\n易丢失信息（强制二分类）\n保留不确定性（概率加权）\n\n\n统计功效\n可能低估关联信号\n提高检测灵敏度\n\n\n\n举个例子，若某位点的测序深度低，Hard Call 可能强制判为 0/0，而 Dosage 可记录为 0.2（更接近真实生物学状态）。\n\n\n\nSAIGE为何偏爱 Dosage 文件？\nSAIGE采用基于混合线性模型（Mixed Linear Model, MLM）的算法，通过引入遗传关系矩阵（GRM）控制群体结构。而 Dosage 文件的优势在于，其连续型变量可直接作为协变量输入，避免离散化导致的信息损失，提高模型对微弱信号的捕捉能力。相比于简单的 hard call，dosage 数据充分利用了 imputation 的概率信息，使得关联分析更敏感、更准确。不过需要注意的是，dosage 文件在使用前必须经过严格的质量控制，确保 imputation 的准确性，以避免低质量数据对分析结果的干扰。\n\n\nDosage 文件的生成与使用\n生成 Dosage 文件的常用工具有 PLINK、BEAGLE、IMPUTE2 等。以 PLINK 为例，通过以下命令将 VCF 文件转换为 Dosage 文件：\n# 从VCF转换为Dosage格式\nplink --vcf input.vcf --dosage DS --write-snplist --out output\n生成的 output.dosage 文件格式如下：\nCHR SNP POS A1 A2 DOSAGE_1 DOSAGE_2 ... DOSAGE_N\n1 rs123 1000 A T 0.98 1.76 ... 0.02\n\n\nTake-Home Message\ndosage 文件作为基因型 imputation 的产物，在 GWAS 分析中发挥着至关重要的作用。通过充分利用 dosage 数据中的概率信息，SAIGE 等工具能够更准确地捕捉到基因型与性状之间的微妙关联，为我们揭示遗传机制提供了有力支持。在数据质量和格式得到充分保障的前提下，正确使用 dosage 文件将大大提升 GWAS 分析的精度和效率。"
  },
  {
    "objectID": "blog/2025/02/17/zotero_onedrive_deepseek/index.html",
    "href": "blog/2025/02/17/zotero_onedrive_deepseek/index.html",
    "title": "Zotero + OneDrive + DeepSeek：构建个人文献阅读管理系统",
    "section": "",
    "text": "最近 DeepSeek 大火，看到有人分享用它来阅读管理文献，非常好用。我自己一直用的是 EndNote，但是似乎不支持接入 Deepseek，所以我决定暂时转投 Zotero 阵营，实现文献管理和阅读的一体化。\n\nZotero + OneDrive\nZotero 是一个开源的文献管理工具，但是它有一个缺点，只有 300MB 的免费存储空间。\n\n\n\nStorage\n\n\n我之前的 EndNote 没有存储空间限制的焦虑，为了防止以后 Zotero 的存储空间不够用，我决定顺手把 Zotero 的文献库放到 OneDrive 云上，这样就可以在不同设备上同步，且不适用 Zotero 自带的存储空间。\n对于 Zotero 放到云上的设置，可以参考：\n\n\n\nSync\n\n\n\n\n\nFiles and Folders\n\n\n只需要将 Zotero 的 Storage 文件夹软链接到 OneDrive 云上，然后在 Zotero 的设置中设置数据文件夹的路径即可。\n这样，对于有 Microsoft 365 家庭版订阅的用户，就可以免费使用 1TB 的 OneDrive 存储空间来存储 Zotero 的文献库了。\n\n\nZotero + DeepSeek\nDeepSeek 最近已经火到不需要我再多做介绍了。Deepseek 之前是给每位用户送了 10元 的 API：\n\n\n\nDeepSeek\n\n\n但是是一个月的有效期，且目前 DeepSeek 的 API 已经无法购买了。我们转向使用硅基流动提供的 API，它也赠送了 14元：\n\n\n\nSiliconCloud\n\n\n那接下来就是在 Zotero 中设置硅基流动中模型的 API 了，这里我们选择的是模型 deepseek-ai/DeepSeek-R1-Distill-Qwen-32B：\n\n\n\nDeepSeek-R1-Distill-Qwen-32B\n\n\n浏览下它的 API 文档，我们在 Zotero 的 Awesome GPT 插件中设置好 API 即可，这个插件可以通过 GitHub 下载：\n\n\n\nAPI\n\n\n通过了 test 之后（我这里图片上的 error 可以忽略），我们就可以使用 DeepSeek 来阅读文献了。\n首先必须读一篇经典文献：Attention is All You Need。\n\n\n\nTransformer\n\n\n感兴趣的同学可以参考我们的教程开始使用 DeepSeek 助力自己的科研了。"
  },
  {
    "objectID": "blog/2025/03/07/pairedttest/index.html",
    "href": "blog/2025/03/07/pairedttest/index.html",
    "title": "配对样本t检验的统计学效率",
    "section": "",
    "text": "复习独立样本 t 检验和配对样本 t 检验的统计量，并证明配对样本 t 检验的统计学效率高于独立样本 t 检验。\n统计学效率通常指在相同的显著性水平和样本量下，检验检测真实差异（效应量）的能力（即检验功效 power）更高，或者在达到相同power时所需样本量更小。"
  },
  {
    "objectID": "blog/2025/03/07/pairedttest/index.html#结论",
    "href": "blog/2025/03/07/pairedttest/index.html#结论",
    "title": "配对样本t检验的统计学效率",
    "section": "结论",
    "text": "结论\n通过估计量方差和非中心参数的比较，证明了当配对观测值正相关时，配对样本t检验的统计学效率高于独立样本t检验。这也是为什么在实验设计中，当可以控制个体差异时，优先选择配对设计的原因。"
  },
  {
    "objectID": "blog/2025/03/10/balanced_design/01_index.html",
    "href": "blog/2025/03/10/balanced_design/01_index.html",
    "title": "证明平衡设计与非平衡设计的统计学效率",
    "section": "",
    "text": "引言\n比较两独立样本在主要疗效指标为定量指标的前提下，平衡设计和非平衡设计的统计学效率。\n\n\n方差相等背景\n证明：在两独立样本的主要疗效指标方差相等的前提下，平衡设计的统计学效率高于非平衡设计。\n假设有两个独立样本：\n\n样本1：均值为 \\(\\mu_1\\)，方差为 \\(\\sigma^2\\)，样本量为 \\(n_1\\)；\n样本2：均值为 \\(\\mu_2\\)，方差为 \\(\\sigma^2\\)（假设两样本结局变量的方差相等），样本量为 \\(n_2\\)。\n\n总样本量固定为 \\(N = n_1 + n_2\\)。\n设计类型如下：\n\n平衡设计：\\(n_1 = n_2 = \\frac{N}{2}\\)。\n非平衡设计：\\(n_1 \\neq n_2\\)，但 \\(n_1 + n_2 = N\\)。\n\n假设 \\(N\\) 足够大，可以采用基于标准正态分布的 \\(Z\\) 检验代替 \\(t\\) 检验。\n\n估计量的方差（精度）\n对于两个独立样本，均值差的估计量为 \\(\\bar{x}_1 - \\bar{x}_2\\)。由于样本独立且方差相等，其方差为：\n\\[\n\\text{Var}(\\bar{x}_1 - \\bar{x}_2) = \\text{Var}(\\bar{x}_1) + \\text{Var}(\\bar{x}_2) = \\frac{\\sigma^2}{n_1} + \\frac{\\sigma^2}{n_2} = \\sigma^2 \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)\n\\]\n我们需要证明，在 \\(n_1 + n_2 = N\\) 固定的情况下，\\(\\frac{1}{n_1} + \\frac{1}{n_2}\\) 在 \\(n_1 = n_2 = \\frac{N}{2}\\) 时取得最小值。\n令 \\(n_2 = N - n_1\\)，则：\n\\[\n\\frac{1}{n_1} + \\frac{1}{n_2} = \\frac{1}{n_1} + \\frac{1}{N - n_1} = \\frac{N}{n_1 (N - n_1)}\n\\]\n定义函数：\n\\[\nf(n_1) = \\frac{N}{n_1 (N - n_1)}\n\\]\n其中 \\(N\\) 为常数，\\(n_1\\) 的取值范围为 \\(0 &lt; n_1 &lt; N\\)。目标是找到使 \\(f(n_1)\\) 最小的 \\(n_1\\)。\n为了简化，考虑最大化分母 \\(g(n_1) = n_1 (N - n_1)\\)，因为 \\(f(n_1) = \\frac{N}{g(n_1)}\\)，当 \\(g(n_1)\\) 最大时，\\(f(n_1)\\) 最小。\n计算 \\(g(n_1)\\) 的导数：\n\\[\ng(n_1) = n_1 N - n_1^2\n\\]\n\\[\ng'(n_1) = N - 2n_1\n\\]\n令 \\(g'(n_1) = 0\\)：\n\\[\nN - 2n_1 = 0 \\implies n_1 = \\frac{N}{2}\n\\]\n计算二阶导数以验证极值：\n\\[\ng''(n_1) = -2 &lt; 0\n\\]\n由于二阶导数为负，\\(n_1 = \\frac{N}{2}\\) 时 \\(g(n_1)\\) 取得最大值，因此 \\(f(n_1)\\) 取得最小值。\n代入 \\(n_1 = n_2 = \\frac{N}{2}\\)：\n\\[\n\\frac{1}{n_1} + \\frac{1}{n_2} = \\frac{1}{\\frac{N}{2}} + \\frac{1}{\\frac{N}{2}} = \\frac{2}{\\frac{N}{2}} = \\frac{4}{N}\n\\]\n在 \\(n_1 = n_2\\) 时方差最小，表明平衡设计下估计量的精度最高。\n\n\n\n方差不相等背景\n证明：在两独立样本的主要疗效指标方差不相等的前提下，非平衡设计的统计学效率高于平衡设计。 估 ### 估计量的方差（精度）\n延用上面的推导，假设两组的方差分别为 \\(\\sigma_1^2\\) 和 \\(\\sigma_2^2\\)，则两组均值差的 估计量 \\(\\bar{x}_1 - \\bar{x}_2\\) 的方差为：\n\\[\n\\text{Var}(\\bar{x}_1 - \\bar{x}_2) = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\n\\]\n设 \\(n_2 = N - n_1\\)，目标函数：\n\\[\nf(n_1) = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{N - n_1}\n\\]\n求导并令 \\(f'(n_1) = 0\\)：\n\\[\n-\\frac{\\sigma_1^2}{n_1^2} + \\frac{\\sigma_2^2}{(N - n_1)^2} = 0\n\\]\n\\[\nn_1 = \\frac{N \\sigma_1}{\\sigma_1 + \\sigma_2}, \\quad n_2 = \\frac{N \\sigma_2}{\\sigma_1 + \\sigma_2}\n\\]\n我们可以看到，最优分配比例为 \\(n_1 : n_2 = \\sigma_1 : \\sigma_2\\) 。这也表明，当方差不相等时，最优设计是非平衡的，而上面方差相等的情况是一种特例。\n\n非平衡设计方差：\n\n\\[\n\\text{Var}_{\\text{opt}} = \\frac{(\\sigma_1 + \\sigma_2)^2}{N}\n\\]\n\n平衡设计方差：\n\n\\[\n\\text{Var}_{\\text{bal}} = \\frac{2 (\\sigma_1^2 + \\sigma_2^2)}{N}\n\\]\n二者差值：\n\\[\n\\text{Var}_{\\text{bal}} - \\text{Var}_{\\text{opt}} = \\frac{(\\sigma_1 - \\sigma_2)^2}{N} &gt; 0 \\quad (\\text{当} \\sigma_1 \\neq \\sigma_2)\n\\]\n这也表明非平衡设计方差更小，效率更高。\n当在某些情况下为了更多获得新药的信息，可能会在设计时考虑非平衡设计，此种情况下，提高方差较大组受试者的样本量，可以提高试验的统计学效率。\n\n\n结论\n综合以上分析：\n\n在两独立样本的方差相等且总样本量固定的前提下，平衡设计的统计学效率高于非平衡设计。\n在两独立样本的方差不相等的前提下，非平衡设计的统计学效率高于平衡设计。\n\n更加一般地，除了上面的 \\(Z\\) 检验和 \\(t\\) 检验，包括 \\(Wald \\quad test\\) 和 \\(score \\quad test\\) 等，均符合上述结论。"
  },
  {
    "objectID": "blog/2025/03/11/WGS_GWAS/index.html",
    "href": "blog/2025/03/11/WGS_GWAS/index.html",
    "title": "全基因组测序GWAS vs. 传统GWAS",
    "section": "",
    "text": "引言\n全基因组关联分析（Genome-Wide Association Study, GWAS）是遗传学领域研究复杂性状和疾病遗传基础的强大工具。传统GWAS主要依赖SNP芯片技术，而随着测序技术的飞速发展，全基因组测序（Whole-Genome Sequencing, WGS）逐渐成为GWAS研究的新选择。\nWGS-GWAS相比传统GWAS到底有哪些优势呢？\n\n\n\nInterconnected techniques in genome-wide association studies (GWAS) research(Omidiran et al. 2024)\n\n\n图片来自：Interconnected techniques in genome-wide association studies (GWAS) research\n\n\n传统GWAS的局限性\n传统GWAS使用SNP芯片技术，虽然在过去取得了巨大成功，但也存在一些明显的局限性：\n\n变异覆盖有限：SNP芯片只能检测预先设计的常见变异位点（SNP），无法发现新的或罕见的遗传变异。\n分辨率低：由于芯片探针数量有限，难以精确定位与疾病相关的具体变异。\n罕见变异检测能力弱：主要针对常见变异（MAF &gt; 0.05），对罕见变异（MAF &lt; 0.01）的捕捉能力不足。\n因果变异难识别：通常只能找到与疾病相关的标记SNP，而非真正的致病变异。\n结构变异检测受限：SNP芯片专注于单核苷酸变异，对插入、删除等结构变异的检测能力较差。\n群体特异性问题：芯片设计基于特定人群数据，可能不适用于其他群体。\n数据质量依赖技术：依赖杂交技术，易受杂交误差影响。\n灵活性低：分析局限于芯片预设的变异类型和范围。\n\n这些局限性使得传统GWAS在研究复杂疾病的遗传机制时受到一定制约。\n\n\nWGS-GWAS的优势\n相比之下，使用全基因组测序数据进行GWAS（WGS-GWAS）带来了革命性的改进，以下是其主要优势：\n\n1. 全面的遗传变异覆盖\nWGS能够测定整个基因组的DNA序列，覆盖所有类型的遗传变异，包括SNP、罕见变异以及插入、删除等结构变异。\n\n\n2. 更高的分辨率\nWGS的分辨率可以精确到单个碱基对级别，帮助研究者更准确地定位与性状或疾病相关的遗传变异，避免传统GWAS中模糊定位的问题。\n\n\n3. 罕见变异的检测能力\n罕见变异（MAF &lt; 0.01）在复杂疾病中可能扮演重要角色。WGS能够全面捕捉这些低频变异，为研究提供新的突破口。\n\n\n4. 直接识别因果变异\n传统GWAS通常找到的是与疾病关联的标记位点，而WGS可以直接检测潜在的因果变异，提升对疾病机制的理解。\n\n\n5. 结构变异的捕捉\nWGS不仅限于SNP，还能检测插入、删除、倒位等结构变异，这些变异可能对性状和疾病有重要影响。\n\n\n6. 群体特异性变异的研究\nWGS不受芯片设计的群体偏见限制，能发现特定人群特有的变异，为跨人群遗传研究提供支持。\n\n\n7. 更高的数据质量\nWGS通过直接测序DNA生成数据，相比SNP芯片的杂交技术，减少了技术误差，结果更可靠。\n\n\n8. 分析灵活性\nWGS数据包含整个基因组信息，研究者可以根据需要选择不同的变异类型和分析策略，灵活性远超传统GWAS。\n\n\n\nTake-Home Message\n总的来说，使用全基因组测序数据进行GWAS在变异覆盖、分辨率、罕见变异检测、因果变异识别、结构变异分析、群体特异性、数据质量和灵活性等方面都显著优于传统GWAS。这些优势使得WGS-GWAS在探索复杂性状和疾病遗传基础时展现出更大的潜力，为精准医学和遗传学研究开辟了新的可能性。\n\n\n\n\n\n\n\nReferences\n\nOmidiran, Oluwaferanmi, Aashna Patel, Sarah Usman, Ishani Mhatre, Habiba Abdelhalim, William Degroat, Rishabh Narayanan, Kritika Singh, Dinesh Mendhe, and Zeeshan Ahmed. 2024. “GWAS Advancements to Investigate Disease Associations and Biological Mechanisms.” Journal Article. Clinical and Translational Discovery 4 (3). https://doi.org/10.1002/ctd2.296."
  },
  {
    "objectID": "blog/2025/03/25/broadcasting_in_numpy/index.html",
    "href": "blog/2025/03/25/broadcasting_in_numpy/index.html",
    "title": "NumPy 广播机制详解",
    "section": "",
    "text": "引言\n这里我们介绍 NumPy 中的广播机制，这一功能是 NumPy 库的核心特性之一，特别适用于数据科学和数值计算领域，其允许不同形状的数组进行算术操作，显著简化代码并提升计算效率。\n\n\n背景与定义\nNumPy 是 Python 中用于高效数组操作的库，广泛应用于科学计算、机器学习和数据分析。广播机制是指 NumPy 在算术操作中处理不同形状数组的能力，通过扩展较小数组的维度，使其与较大数组匹配，从而进行逐元素操作。\n例如，将标量 2 加到数组 [1, 2, 3] 上，结果为 [3, 4, 5]，标量 2 被概念上扩展为 [2, 2, 2]。这种机制不仅节省内存（不实际复制数据），还避免了手动循环，显著提高了计算效率，尤其在处理大型数据集时。\n\n\n广播规则详解\n广播的实现依赖于以下规则，确保数组形状兼容：\n\n维度对齐：比较两个数组的形状，从右向左（尾部维度开始）。如果维度数不同，较小数组会在左侧补齐维度 1。例如，形状 (3,) 的数组可视为 (1, 3)。\n形状兼容性：如果对应维度大小相等，直接操作。如果一个维度为 1，NumPy 会拉伸该维度，使其匹配另一个维度的大小。例如，形状 (1, 3) 的数组与 (2, 3) 相加，前者会被重复为 (2, 3)。如果对应维度既不等也不为 1，操作失败，抛出 ValueError，提示 operands could not be broadcast together。\n\n\n\n\nbroadcasting\n\n\n\n结果形状：广播后的结果形状取每个维度的最大值，缺失维度视为 1。\n\n例如：数组 A 形状 (2, 3)，数组 B 形状 (3,)：B 被视为 (1, 3)，然后广播为 (2, 3)，操作可进行。数组 C 形状 (2, 3)，数组 D 形状 (2, 2)：比较最后维度 3 和 2，不兼容，抛出错误。这些规则确保操作的正确性，但需要大家理解形状匹配的逻辑。\n以下是几个具体例子，展示广播机制的应用：\n\nimport numpy as np\narr = np.array([1, 2, 3])\nscalar = 2\nresult = arr + scalar\nprint(result)\n\n[3 4 5]\n\n\n这里，标量 2 被广播为 [2, 2, 2]，与 [1, 2, 3] 逐元素相加。效率高，因为不实际创建新数组。\n\narr1 = np.array([[1, 2, 3],\n                 [4, 5, 6]])\narr2 = np.array([10, 20, 30])\nresult = arr1 + arr2\nprint(result)\n\n[[11 22 33]\n [14 25 36]]\n\n\narr1 形状 (2, 3)，arr2 形状 (3,)。arr2 被视为 (1, 3)，然后广播为 (2, 3)，即 [[10, 20, 30], [10, 20, 30]]。结果为逐元素相加，符合预期。\n在图像处理中，假设有形状 (2, 2, 3) 的图像数组（高度、宽度、通道），与形状 (3,) 的缩放向量：\n\nimage = np.array([[[0.8, 2.9, 3.9],\n                   [52.4, 23.6, 36.5]],\n                  [[55.2, 31.7, 23.9],\n                   [14.4, 11.0, 4.9]]])\nscale = np.array([3, 3, 8])\nscaled_image = image * scale\n\nscale 被广播为 (2, 2, 3)，每个像素的通道值分别乘以 [3, 3, 8]。这种操作在数据标准化或特征缩放中非常常见。\n\n\n常见陷阱与优化建议\n尽管广播强大，但有以下常见问题需要注意：\n\n形状不匹配：如果数组形状无法广播，NumPy 会抛出 ValueError。建议先打印形状（如 print(arr.shape)）确认。\n意外广播：一维数组与二维数组相加，可能按行或按列广播，需明确意图。例如，np.array([1,2]) + np.array([[3,4],[5,6]]) 需要注意广播方向。\n\n\na= np.array([1,2])\nb= np.array([[3,4],[5,6]])\na + b\na[:, np.newaxis] + b\n\narray([[4, 5],\n       [7, 8]])\n\n\n\n性能问题：对于大型数组，广播可能创建临时数组，增加内存使用。在内存受限情况下，考虑使用循环或显式重塑。\n\n\n优化建议：\n\n使用 np.newaxis 或 reshape 显式控制维度，例如 arr[:, np.newaxis] 将一维数组转为列向量。\n检查数组形状，确保符合预期。\n对于超大型数据集，评估广播是否会导致内存瓶颈，可能需要替代方案。"
  },
  {
    "objectID": "blog/2025/04/01/pe0002/index.html",
    "href": "blog/2025/04/01/pe0002/index.html",
    "title": "PE0002: Even Fibonacci Numbers",
    "section": "",
    "text": "题目\nEach new term in the Fibonacci sequence is generated by adding the previous two terms. By starting with 1 and 2, the first 10 terms will be:\n\n1, 2, 3, 5, 8, 13, 21, 34, 55, 89, …\n\nBy considering the terms in the Fibonacci sequence whose values do not exceed four million, find the sum of the even-valued terms.\n\n\n问题描述\n题目要求：找出斐波那契数列中不超过四百万的所有偶数项，并计算它们的和。 斐波那契数列以1和2开始，后续项由前两项相加生成，例如：1, 2, 3, 5, 8, 13, 21, 34… 目标：求所有偶数项（如2、8、34等）的总和。\n\n\n解答\n\n方法一：直接遍历法（新手友好）\n思路：逐个生成斐波那契数，检查是否为偶数，累加符合条件的项。\n\ndef sum_of_even_fibonacci(limit) -&gt; int:\n    \"\"\"Returns the sum of even Fibonacci numbers not exceeding the limit.\"\"\"\n    a, b = 1, 2\n    total = 0\n    while a &lt;= limit:\n        if a % 2 ==0:\n            total += a\n        a, b= b, a+b\n    return total\n\nsum_of_even_fibonacci(4_000_000)\n\n4613732\n\n\n优点：逻辑简单，适合理解基础循环和条件判断。 缺点：需检查每个数是否为偶数，效率较低（时间复杂度 O(n)）。\n\n\n方法二：生成器 + NumPy（效率升级）\n思路：用生成器无限生成斐波那契数，结合NumPy数组快速筛选偶数。\n\n# solution using a more efficient method\ndef fibonacci():\n    \"\"\"Generator for Fibonacci numbers\"\"\"\n    a= b= 1\n    while True:\n        yield b\n        a, b= b, a+b\n\nseries= []\nfor num in fibonacci():\n    if num &gt; 4_000_000:\n        break\n    series.append(num)\n\n# series\n\nimport numpy as np\narr= np.array(series)\narr[arr%2==0].sum()  # sum of even Fibonacci numbers\n\nnp.int64(4613732)\n\n\n优点：生成器减少内存占用，NumPy向量化操作加速计算。 缺点：仍需生成全部数列，大范围数据仍不够高效。\n\n\n方法三：数学优化法\n核心发现：斐波那契数列中，每第三项为偶数（如2, 8, 34, 144…）。 利用此规律，只需计算这些特定项的和，跳过无关项，效率飙升。\n\ndef SumThirdValue(limit) -&gt; int:\n    \"\"\"\n    Every third Fibonacci number is even.\n    \"\"\"\n    x= y= 1\n    sum= 0\n    while x&lt;= limit:\n        sum+= (x+y)\n        x, y= x+2*y, 2*x+3*y\n    return sum\n\nSumThirdValue(4_000_000)\n\n4613732\n\n\n优点：时间复杂度降至 O(log n)，四千万甚至十亿级数据也能瞬间完成。\n\n\n\n答案\n答案为4613732。"
  },
  {
    "objectID": "blog/2025/04/04/dnanexus_instance/index.html",
    "href": "blog/2025/04/04/dnanexus_instance/index.html",
    "title": "如何科学选择DNAnexus平台的计算实例",
    "section": "",
    "text": "引言\n在生物信息学领域，DNAnexus 平台以其强大的数据管理和分析能力不得不成为许多研究者的首选工具。\nDNAnexus平台为用户提供了丰富的云计算资源，适用于从轻量级数据处理到高性能基因组分析的各种场景。实例类型作为决定计算资源的核心配置，直接影响作业的运行效率和成本。选择不当可能导致作业失败、运行时间过长或资源浪费。因此，了解实例类型的特性并根据需求进行选择，是每个DNAnexus用户都需要掌握的技能。要高效完成分析任务，选择合适的实例类型（Instance Type）至关重要。\n这里，我们将提供一份实用的实例选择指南，帮助大家在性能与成本之间找到最佳平衡。\n\n\n实例类型概述\nDNAnexus平台上的实例类型是虚拟机配置的集合，包含内存（Memory）、存储（Storage）和核心数（Cores）等关键参数。实例名称通常遵循特定规则，例如 mem1_ssd1_v2_x16，其含义如下：\n\nmem：内存级别（mem1、mem2、mem3、mem4等，数字越大内存越多）\nssd/hdd：存储类型（SSD为固态硬盘，速度快，适合高频读写；HDD为机械硬盘，成本低适合冷数据）\nv2：实例版本（通常为升级版本）\nx16：核心数（例如16核），并行计算能力直接影响任务速度\n\n此外，部分实例还包含GPU支持（如 gpu1 或 gpu4），适用于深度学习模型训练任务。\n\n\n如何科学选择实例类型\n选择实例类型时，需要综合考虑任务需求和预算。以下是几个关键点值得大家注意：\n\n1. 数据大小\n大数据集需要更大的存储空间。如果处理的是TB级基因组数据，选择存储容量高的实例。\n提示：HDD实例通常存储容量更大但速度较慢，适合存储密集型任务；SSD实例则更适合需要快速读写的场景。\n\n\n2. 计算密集度\n计算密集型任务（如基因组组装或变异检测）需要更多核心。选择高核心数的实例。\n提示：核心数越多，处理速度越快，但成本也随之上升。\n\n\n3. 内存需求\n内存密集型工具（如某些比对或组装软件）需要充足的内存支持。优先选择高内存实例，避免因内存不足导致作业失败。\n提示：日志中若提示 Out of Memory，说明当前内存不足。\n\n\n4. 成本效益\n在满足性能的前提下控制成本。对于中小型任务，避免过度配置，选择刚好满足需求的实例类型。\n\n\n5. 调试日志的经验\n调试日志是优化实例选择的利器。我们建议：\n\n监控资源使用：检查CPU、内存和存储的实际使用率。如果CPU长期满载，说明计算资源不足，可增加核心数；若内存接近上限，日志中出现 Killed 或 Out of Memory 时，需提升内存；若存储接近上限，日志中若出现 Low scratch storage space，则需增加存储空间。\n识别瓶颈：日志显示I/O等待时间长时、大量磁盘读写等待时间，优先选择SSD实例，提升I/O性能。\n迭代优化：通过日志分析实际资源使用情况，避免选择过于强大的实例。例如，若任务只用了50%内存，可尝试更低配置。\n\n\n\n\n实例类型列表\n我们整理了一份 DNAnexus 平台的部分实例类型及其规格和价格，大家公众号后台回复instance即可获取表格链接。\n\n\n\nDNAnexus"
  },
  {
    "objectID": "blog/2025/04/16/llm_translator/index.html",
    "href": "blog/2025/04/16/llm_translator/index.html",
    "title": "大模型翻译个人博客post",
    "section": "",
    "text": "背景\n前一段时间，大家想必已经在小红书上感受到了大模型双语翻译的魅力，应该来讲，信达雅的程度已经相当高了。对于一些个人技术博客网站而言，依靠大模型中英文翻译的能力，我们已经可以实现快速发布中英文双语版本的博客文章。\n受到 Rico00121/hugo-translator 项目的启发（地址：https://github.com/Rico00121/hugo-translator），我们利用 python 和 deepseek 对自己依靠 quarto 构建的个人网站上的文章进行了全部的自动翻译，下面是翻译的效果：\n\n\n\n中英本双语版本对比\n\n\n\n\n实现\n\nAPI 接入\n首先是接入硅基流动上可选的模型，我们没有使用 deepseek 官网的 api：\n\n# load the env file\n    load_dotenv('Python/materials/.env', override= True)\n    if not os.path.exists('Python/materials/.env'):\n        print(f'The file .env does not exist.')\n        sys.exit(1)\n    if not os.getenv('OPENAI_API_KEY'):\n        print(f'The OPENAI_API_KEY is not set.')\n        sys.exit(1)\n    \n    client= None\n    if llm_type== 'openai':\n        client= openai.OpenAI(api_key= os.getenv('OPENAI_API_KEY'))\n        print('Using OpenAI to translate the post...')\n    else:\n        client= openai.OpenAI(\n            api_key= os.getenv('OPENAI_API_KEY'),\n            base_url= os.getenv('DEEPSEEK_API_URL')\n        )\n        print('Using DeepSeek to translate the post...')\n\n这里，提醒大家，注意通过 .env 文件管理 API 密钥，避免 hard code 造成泄露风险。关于硅基流动的 API 使用，大家可以参考他们的官方文档或者我们之前的这篇文章。\n\n\n读取 qmd 文件\n对于 qmd 文件，我们需要识别应用于我们网页的 metadata，对于这部分信息，我们翻译了 title 并回写，以保证翻译后生成的 _en.qmd 文件可直接用于静态网站生成，无需手动调整。代码如下：\n\n# Load the QMD file\nwith open(file_path, 'r', encoding='utf-8') as f:\n    post = frontmatter.load(f)\n\n# Get the title and content of the post\ntitle = post.metadata.get('title', 'Untitled')\ncontent = post.content\n\n# Translate the title and content\nen_title = translate_title(title, llm_type)\nen_content = translate_text(content, llm_type)\n\n# Prepare metadata for the translated post\nen_metadata = post.metadata.copy()\nen_metadata['title'] = en_title\n\n# Create a new post with translated content\nen_post = frontmatter.Post(content=en_content, **en_metadata)\n\n# Save the translated post to a new file\nen_file_path = file_path.replace('.qmd', '_en.qmd')\nwith open(en_file_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(frontmatter.dumps(en_post))\n\nprint(f'Successfully translated the post to {en_file_path}')\n\n\n\n翻译函数\n再次感谢 Rico00121/hugo-translator 项目，由于原项目在翻译时，对于 code 的支持并不好，我们在原有的基础上，修改了大模型的 prompt，增加了对代码的翻译支持：要求翻译时，保留代码的原有格式和内容，不要破坏 Markdown 格式。\n\ndef translate_text(text, llm_type):\n    \"\"\"\n    translate text to english using llm\n    \"\"\"\n    total_length= len(text)\n    translated_text= ''\n\n    print(f'Start translating the main text...')\n    # translate in chunks to show progress\n    chunk_size= 1000\n    for i in range(0, total_length, chunk_size):\n        chunk= text[i:i+chunk_size]\n        translated_text+= get_translation(llm_type,\n                                          [\n                                              {\n                                                'role': 'system', \n                                                'content': 'You are a professional translator. '\n                                                   'Translate the following Chinese text into English. '\n                                                   'Strictly preserve the original text format in qmd file, including line breaks, indentation, and any special characters for inserting codes. '\n                                                   'Do not add, remove, or modify any content. '\n                                                   'Only return the translated text without any additional explanation, comments, or extra content.'\n                                              },\n                                              {\n                                                'role': 'user', \n                                                'content': chunk\n                                              }\n                                          ]\n                                         )\n        progress= min((i+chunk_size)/total_length*100, 100)\n        print(f'Tranlation progress: {progress: .2f}%')\n    \n    return translated_text\n\n这里我们同样采取了分块翻译的方式，避免大模型的 token 长度限制。最终效果前面已经展示。最终输出的格式完美的英文 .qmd 文件，我们可以直接部署到 GitHub Pages。完整 python 代码已经放进了星球里。\n\n\n\nTakeaway\n基于这种方式，对于需要中英文双语的 API 文档、个人技术教程、博客论文等，我们可以实现快速的中英文双语版本的生成。对于一些需要定期更新的内容，我们可以通过定时任务，自动化的实现中英文双语版本的更新。"
  },
  {
    "objectID": "blog/2025/04/23/hair_loss/index.html",
    "href": "blog/2025/04/23/hair_loss/index.html",
    "title": "脱发就诊",
    "section": "",
    "text": "背景\n头发容易出油，头皮屑多，发际线后移。\n\n\n就诊\n2025年4月22日，去医院美容皮肤科就诊。\n皮肤镜检测诊断 119￥：\n\n\n\n皮肤镜检查报告单\n\n\n临床诊断：雄激素性脱发，脂溢性皮炎。\n\n\n治疗\n\n二硫化硒洗剂 1瓶 230g 80￥，治疗头皮屑和头皮脂溢性皮炎。\n\n用法：\n\n每周2次，一个疗程2-4周。\n先用洗发水洗头，冲洗干净后再用二硫化硒洗剂。\n二硫化硒洗剂取 5-10克，涂抹在湿发及头皮上，轻揉至出泡沫，待3-5分钟后用温水冲洗干净。必要时可重复一次。\n\n\n米诺地尔泡沫剂 2瓶 396￥。用法：每天两次。\n激光疗法（1200￥）+ 红光治疗(300￥)。\n口服非那雄胺片 1mg*84片 148.96￥。用法：每天一次，一次一片。"
  },
  {
    "objectID": "blog/2025/05/07/pe0004/index.html",
    "href": "blog/2025/05/07/pe0004/index.html",
    "title": "PE0004: Largest Palindrome Product",
    "section": "",
    "text": "题目\nA palindromic number reads the same both ways. The largest palindrome made from the product of two 2-digit numbers is 9009=91*99.\nFind the largest palindrome made from the product of two 3-digit numbers.\n\n\n问题描述\n题目要求：给定两个三位数，找到它们的乘积中最大的回文数。\n\n\n解答\n\n方法一：暴力遍历法（新手入门）\n思路：生成所有3位数的乘积，逐个检查是否为回文数，记录最大值。\n\ndef find_maximum():\n    ans= max(\n        i*j\n        for i in range(100, 1000)\n        for j in range(100, 1000)\n        if str(i*j) == str(i*j)[::-1]\n    )\n    return ans\n\nfind_maximum()\n\n优点：代码简单，逻辑直观。 缺点：计算次数高达810,000次，效率极低。\n\n\n方法二\n\ndef reverse(n):  \n    reversed = 0  \n    while n &gt; 0:  \n        reversed = reversed * 10 + n % 10  \n        n = n // 10  \n    return reversed  \n\ndef is_palindrome(n):  \n    return n == reverse(n)  \n\nlargestPalindrome = 0  \na = 100  \ndef palindrome():  \n    global largestPalindrome, a  \n    while a &lt; 1000:  \n        b = 100  \n        while b &lt; 1000:  \n            product = a * b  \n            if is_palindrome(product) and product &gt; largestPalindrome:  \n                largestPalindrome = product  \n            b += 1  \n        a += 1  \n    return largestPalindrome  \n\nreverse()：通过数学方法反转数字，避免字符串转换。\nglobal变量：全局记录最大值，但嵌套循环未优化，计算量仍为810,000次。\n\n\n方法三：减少重复计算\n优化点：内层循环从a开始，避免重复检查i*j和j*i。\n\nlargestPalindrome = 0  \na = 100  \ndef improved_palindrome():  \n    global largestPalindrome, a  \n    while a &lt; 1000:  \n        b = a  # 从a开始，避免重复  \n        while b &lt; 1000:  \n            product = a * b  \n            if is_palindrome(product) and product &gt; largestPalindrome:  \n                largestPalindrome = product  \n            b += 1  \n        a += 1  \n    return largestPalindrome  \n\n\n\n方法四：倒序搜索 + 提前终止\n优化点：从999开始倒序搜索，若乘积小于当前最大值则提前终止。\n\nlargestPalindrome = 0  \na = 100  \ndef optimized_palindrome():  \n    global largestPalindrome, a  \n    while a &lt; 1000:  \n        b = 999  # 从999开始  \n        while b &gt;= a:  \n            product = a * b  \n            if product &lt;= largestPalindrome:  \n                break  # 提前终止  \n            if is_palindrome(product):  \n                largestPalindrome = product  \n            b -= 1  \n        a += 1  \n    return largestPalindrome  \n\nbreak：当product &lt;= largestPalindrome时跳出循环，实际计算约5,000次。\n\n\n方法五：数学优化\n数学原理：若乘积为回文数，至少一个数为11的倍数（仅限6位回文数）\n\nlargestPalindrome = 0  \na = 999  \ndef further_optimized_palindrome():  \n    global largestPalindrome, a  \n    while a &gt;= 100:  \n        if a % 11 == 0:  \n            b = 999  \n            subtract = 1  \n        else:  \n            b = 990  # 最大11倍数  \n            subtract = 11  \n        while b &gt;= a:  \n            product = a * b  \n            if product &lt;= largestPalindrome:  \n                break  \n            if is_palindrome(product):  \n                largestPalindrome = product  \n            b -= subtract  \n        a -= 1  \n    return largestPalindrome  \n\nb = 990：当a非11倍数时，b仅取11的倍数，减少候选数。\nsubtract = 11：步长为11，跳过非11倍数。\n\n\n方法六：向量化计算\n代码逻辑：利用NumPy矩阵运算加速。\n\nimport numpy as np  \nreverse_vec = np.vectorize(reverse)  \n\ndef is_palindrome_vec(n):  \n    return n == reverse_vec(n)  \n\ndef max_palindrome_vec(n):  \n    num = n[is_palindrome_vec(n)]  \n    return np.max(num) if np.size(num) &gt; 0 else None  \n\ndef vectorized_palindrome(n):  \n    largestPalindrome = 0  \n    x = np.arange(n//10, n)  \n    for i in x:  \n        max_value = max_palindrome_vec(i * x)  \n        if max_value is not None and max_value &gt; largestPalindrome:  \n            largestPalindrome = max_value  \n    return largestPalindrome  \n\n\n\n\n答案\n答案为906609。"
  },
  {
    "objectID": "blog/2025/06/09/DNAnexus/index.html",
    "href": "blog/2025/06/09/DNAnexus/index.html",
    "title": "在DNAnexus平台进行生信分析的省钱攻略",
    "section": "",
    "text": "在 DNAnexus 平台上进行生信分析时，掌握一些省钱技巧和攻略，能够帮助科研人员和机构更高效地利用资源，降低成本。这里，我们将总结一些在 DNAnexus 平台上进行生信分析时，特别是在应用服务器方面的实用的省钱技巧和攻略，帮助最大限度地节省成本。\n\n\n\nUKBB\n\n\n\n1. 优化计算资源的使用\nDNAnexus 平台使用云计算资源，用户需要为所使用的计算资源付费。因此，优化计算资源的使用是节省成本的关键。以下是一些具体建议：\n\n选择合适的实例类型：DNAnexus 平台支持多种实例类型，用户应根据分析任务的需求选择最合适的实例类型。例如，对于 CPU 密集型任务，选择高 CPU 实例；对于内存密集型任务，选择高内存实例。避免使用过于强大或不足的实例，以平衡性能和成本。前面我们总结过如何科学选择DNAnexus平台的计算实例，大家可以参考。\n利用批量处理：对于大规模分析任务，用户可以利用 DNAnexus 平台的批量处理功能，同时运行多个任务，以提高效率并降低单位任务的成本。批量处理可以减少资源闲置时间，最大化利用计算资源。\n监控和调整资源配置：定期监控分析任务的性能，并根据需要调整资源配置。例如，如果某个任务的 CPU 利用率较低，可以考虑降低实例的 CPU 核心数以节省成本。DNAnexus 平台提供性能监控工具，用户可以利用这些工具来优化资源使用。\n\n\n\n2. 利用闲置资源和峰谷时段\n在某些情况下，用户可以利用平台上的闲置资源或在非高峰时段运行分析任务，以降低成本：\n\n规划峰谷时段：了解平台的资源使用高峰和低谷时段，尽量在低谷时段运行分析任务，以获取更低的资源价格。特别地，当我们使用 Swiss Army Knife (SAK) 进行大规模数据处理时，可以选择在资源使用较低的时段运行，设置 priority 为 low，以降低成本。相比于 High 优先级，Low 优先级的任务在资源紧张时可能会被延迟，但在资源充足时仍能正常运行，且能可观地节省成本。\n\n\n\n3. 有效管理数据存储\n存储大量基因组数据可能会产生高昂的费用。\n\n定期清理不再需要的数据：定期审查存储的数据，删除不再需要的临时文件、中间结果或旧版本数据，以避免不必要的存储费用。DNAnexus 平台提供数据管理工具，帮助用户组织和清理数据。\n优化数据格式和压缩：使用压缩或优化的数据格式可以减少存储空间。例如，使用 gzip 或 bzip2 压缩 FASTQ 文件，使用 CRAM 格式存储比对数据。DNAnexus 平台支持多种数据格式，用户可以选择最节省空间的格式。\n\n\n\n4. 利用平台特性提高效率\nDNAnexus 平台具有许多特性，如自动化工作流程、批处理等，用户可以利用这些特性来提高效率和降低成本：\n\n自动化工作流程：通过自动化重复性任务，用户可以减少手动干预，降低错误率，并节省时间和资源。DNAnexus 平台支持工作流程的自动化，用户可以使用平台提供的工具或 API 来实现自动化。\n利用 API 优化资源使用：DNAnexus 平台提供 API，用户可以通过编程方式自动化任务和优化资源使用。例如，自动启动和停止计算实例，自动上传和下载数据等。\n\n\n\n5. 监控与管理成本\n定期监控和审查使用情况，帮助用户识别和消除不必要的开支：\n\n利用平台提供的成本报告：DNAnexus 平台提供详细的使用和成本报告，用户可以定期查看这些报告，了解资源的消耗情况，并根据需要调整策略。\n设置预算和警报：用户可以设置预算限制和警报，当成本接近或超过预算时，平台会自动通知用户，以便及时采取措施。"
  },
  {
    "objectID": "blog/2025/06/18/2024JCR/index.html",
    "href": "blog/2025/06/18/2024JCR/index.html",
    "title": "2024年度期刊影响因子 JCR2024 公布",
    "section": "",
    "text": "刚刚，2024 年度期刊影响因子 JCR2024 已在 Web of Science 平台正式公布。目前我们已经快速抓取成 Excel，供大家免费下载使用。公众号后台直接回复 JCR2024 即可获取，注意不是在评论区评论。\n\n\n\nJCR2024\n\n\n\n关注的一些期刊\n神刊 CA 降至 232.4；医学四大刊全部下降，最高 Lancet 为 88.5；Nature 下降至 48.5，Cell 为 42.5。\n值得注意的是，期刊 JOURNAL OF HEPATOLOGY 由 26.8 突破至 33 分。"
  },
  {
    "objectID": "blog/2025/06/30/comm/index.html",
    "href": "blog/2025/06/30/comm/index.html",
    "title": "换行符不一致导致 comm 结果异常",
    "section": "",
    "text": "问题背景：换行符\n在处理文本文件时，换行符可能导致由于文件格式不同而出现意想不到的问题。尤其是在跨平台开发中，Windows 和 Unix/Linux 系统使用的换行符不同，这可能会导致一些命令行工具（如 comm、diff 等）无法正确处理文件内容。\n比如，comm 命令明明应该返回正确的比较结果，却因为换行符不一致而失灵。这种情况在开发中并不少见，尤其是当你从不同来源获取文件时。\n通过检查文件格式，我们发现：\n#| eval: false\nfile aa_sorted.txt\nfile bb_sorted.txt\naa_sorted.txt：ASCII 文本，带有 CRLF 换行符（Windows 格式，每行以 结尾）。\nbb_sorted.txt：ASCII 文本，带有 LF 换行符（Unix/Linux 格式，每行以 结尾）。\n由于 comm 命令要求比较的行完全一致，换行符的差异会导致它无法正确识别相同的行。\n\n\n解决方法：统一换行符格式\n这个问题并不复杂，只需将文件格式统一即可。以下是具体步骤：\n\n1. 将 Windows 格式转换为 Unix 格式\n使用 dos2unix 工具可以将带有 CRLF 换行符的文件转换为 LF 格式。运行以下命令：\n#| eval: false\ndos2unix aa_sorted.txt\n这条命令会将 aa_sorted.txt 的换行符从 CRLF 转换为 LF，与 bb_sorted.txt 的格式保持一致。\n\n\n2. 重新运行 comm 命令\n在确保两个文件的换行符一致后，重新运行你的 comm 命令：\n#| eval: false\ncomm -23 aa_sorted.txt bb_sorted.txt | wc -l\n现在，comm 能正确比较文件内容，输出预期的结果。\n\n\n\nTakeaway\n换行符不一致是许多文本处理工具（如 comm、diff 等）结果异常的常见原因。通过使用 dos2unix 工具统一文件格式，这个问题可以轻松解决。以下是几点实用建议：\n\n检查文件格式：在比较文件前，使用 file 命令检查换行符类型。\n保持一致性：在团队协作或跨平台开发时，建议统一使用 LF 换行符（Linux 标准）。\n善用工具：dos2unix 和 unix2dos 是处理换行符问题的得力助手。"
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitæ",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "news/index.html",
    "href": "news/index.html",
    "title": "What I am doing now",
    "section": "",
    "text": "Feb 6, 2025: 📝🎉 My article, Characteristics of Human Papillomavirus Prevalence and Infection Patterns Among Women Aged 35–65 in Fujian Province, China: A Nine‐Year Retrospective Observational Study, has been accepted for publication in Journal of Medical Virology.\nJan 30, 2025: 📝🎉 My article, Identifying Data-Driven Clinical Subgroups for Cervical Cancer Prevention With Machine Learning: Population-Based, External, and Diagnostic Validation Study, has been accepted for publication in JMIR Public Health and Surveillance.\nJan 14, 2025: 📝 Our article, Development, Validation, and Clinical Application of a Machine Learning Model for Risk Stratification and Management in Cervical Cancer Screening Based on Full-Genotyping hrHPV Test (SMART-HPV): A Modelling Study, with me as the second co-first author, has been accepted for publication in The Lancet Regional Health - Western Pacific.\nNov 11, 2024: ✈️ 前往英国爱丁堡参加国际学术会议 The IPVC 2024 Conference，并张贴展示三篇论文海报。\nNov 8, 2024: 🎉 获得 2024 学年中山大学博士研究生国家奖学金。\nSep 23, 2024: 🎉 获得 2024 学年中山大学博士研究生校长奖学金（特等）。\nSep 10, 2024: 💻 第一个 R 包 lulab.utils 上线 R-universe 与 CRAN。\nMay 11, 2024: 🚀✨🎉 网站由 R Markdown 更新为 Quarto 支持。Hello World！"
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "This page contains the source code, links, and slides for various workshops, talks, lectures, and presentations I’ve given.\n\n\n\n\n\n\nCopyright\n\n\n\nExcept where noted, all content is © Zhen Lu and licensed under Creative Commons."
  }
]